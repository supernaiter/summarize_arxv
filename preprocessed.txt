1 INTRODUCTION A Silent Speech Interface SSI defined device enabling speech processing absence exploitable audio signal – example speech recognition obtained exclusively video images mouth electromyographic sensors EMA glued tongue Classic applications targeted SSIs include 1 Voicereplacement persons lost ability vocalize illness accident yet retain ability articulate 2 Speech communication environments silence either necessary desired responding cellphone meetings public places without disturbing others avoiding interference call centers conferences classrooms private communications police military business personnel The SSI concept first identified outgrowth speech production research tandem proliferation use cellular telephones 2010 special issue Speech Communication 1 SSIs based seven different nonacoustic sensor types presented 1 MHz range medical ultrasound US video imaging tongue lips 2 Surface electromyography sEMG sensors applied face neck 3 Electromagnetic articulography EMA sensors attached tongue lips jaw 4 Vibration sensors placed head neck 5 Nonaudible murmur microphones NAM placed neck 6 Electroencephalography EEG electrodes 7 Cortical implants “ thoughtdriven ” SSI Despite activity SSIs today remain part specialized laboratory instruments The performance automatic speech recognition ASR system often characterized Word Error Rate WER expressed percentage total number words appearing corpus To date SSI ASR system able achieve WER parity stateoftheart acoustic ASR Indeed number practical issues make SSI ASR systems considerably involved implement acoustic counterparts 1 Sensor handling While acoustic ASR may amount routine microphone protocol SSIs ’ nonacoustic sensors often rather specialized expensive require physical contact minimum careful placement respect speech biosignalproducing organs This introduces problems invasiveness nonportability nonrepeatability sensor placement bringing added complexity SSI experiments 2 Interference An SSI principle silent certain SSI modalities – vibration sensors radar low frequency airborne ultrasound example – actually associated signals propagate beyond area utilization SSI The possibility interference interception may limit adoption modalities outside laboratory 3 Feature extraction While easily calculated Mel Frequency Cepstral Coefficients MFCC acoustic ASR features choice decades feature selection specialized sensors SSIs remains open question particularly since many SSI modalities – ultrasound imaging EEG example – much higher intrinsic dimensionality simple acoustic signal Furthermore identification stable phonetic signatures acoustic data today mature field existence salient landmarks speech biosignals – arising imaging modalities electromyography example – less evident Medical US operating MHz frequency range propagate outside body It well established 53 documented 54 technique speech production speech pathology research whose first use context SSIs discussed 55 US also relatively noninvasive modality requiring transducer placed speaker ’ chin coupled small video camera front mouth capture lip movement These sensors easily accommodated lightweight acquisition helmet thus minimizing sensor placement issues US tongue imaging added lip video thus many ways attractive modality building practical SSI 12 The Silent Speech Challenge benchmark In 2010 US lip video SSI trained wellknown TIMIT corpus achieved aid language model LM single speaker WER 174 842 “ correct ” word rate independent test corpus 52 representing promising early SSI result benchmark continuous speech recognition task Subsequently raw image data 52 original tongue ultrasound lip videos made available online socalled Silent Speech Challenge SSC archive 56 The purpose archive provide stable data set newly developed feature extraction speech recognition techniques applied The SSC data serve basis experiments reported article Although 174 WER SSI trained monospeaker TIMIT corpus “ promising ” must remembered standard acoustic ASR obtain similar superior scores training full multispeaker acoustic TIMIT corpus much challenging task Further advances thus still necessary order truly put Silent Speech Recognition SSR par acoustic ASR In past several years improvements acoustic speech recognition using Deep Neural NetworkHidden Markov Model DNNHMM systems rather traditional Gaussian Mixture ModelHMM GMMHMM become common In approach deep learning strategy used improve estimation emission probabilities HMM used speech decoding It natural ask extent DNNHMM approach improve SSR performance well Despite SSI implementation challenges outlined earlier applications deep learning techniques SSR indeed begun appear In 57 example tests reported phonetic feature discrimination EMGbased SSI without LM small experimentspecific speech corpus In 58 deep learning EMA based SSI explored giving SSR phone error rates PER lower WER 36 MochaTIMIT corpus used training testing development specific bigram LM In 59 DNNHMM applied SSC benchmark data albeit 38 WER study comparing efficacy different feature extraction methods The present article reports first application DNNHMM approach SSC recognition benchmark using input features decoding strategy reported 52 thus allowing direct comparison performances The SSR results obtained significantly improved compared archive giving best scenario 64 WER 941 “ correct ” word recognition rate nearly threefold improvement benchmark value In contrast 5758 furthermore LM used 52 also employed developed completely independent speech corpus In adition results second less taskspecific LM included present article Finally tests reduced dimensionality feature vectors well completely new input features created raw SSC archive data also reported All new features added SSC archive future use researchers In remainder article details SSC data acquisition system description available archive data first summarized Section 2 Section 3 describes feature extraction strategy developed present study full details DNNHMM based speech recognition procedure appear Section 4 The results summarized Section 5 conclusions perspectives future work outlined final section 2 SSC DATA ACQUISITION AND ARCHIVE RESOURCES The SSC data acquisition system consisted acquisition helmet holding 128 element 48 MHz US probe tongue imaging black white infraredilluminated video camera capture lips The 320×240 pixel tongue images 640×480 pixel lip images created system acquired synchronized manner 60 frames per second fps using Ultraspeech multisensory acquisition system 60 The SSC training corpus consists US lip video data single native English speaker pronouncing 2342 utterances 47 lists 50 sentences TIMIT corpus nonverbalized punctuation manner The speech recorded silently ie without vocalization therefore audio track The test set comprised one hundred short sentences selected WSJ0 5000word corpus 61 read speaker The data available web address indicated 56 The archive initially contained raw ultrasound lip images training test sets original features used well reducedlength feature vectors new features created present article see section 3 appended Speech recognition Challenge data carried standard GMMHMM scheme made use LM also included archive Further details appear section 4 3 FEATURE EXTRACTION 31 Introduction As mentioned earlier speech recognition nonacoustic sensor data faces problem discovering effective feature recognition strategy US lip video SSIs although attractive many ways share drawback Being based images intrinsic input dimensionality may order 1 million pixels Some means dimensionreducing feature extraction thus critical The following discussion centered tongue features Lip features much easier handle overall coherence extracted way tongue features 32 Contour extraction approach Tongue contour extraction tempting choice reducing dimensionality retains visual interpretability features In ultrasound imaging tongue airtissue boundary upper surface tongue produces bright continuous contour referred sidelooking scan sagittal contour Image processing tools automatically extracting characterizing contour make ultrasound imaging powerful tool study speech production 53 54 Unfortunately despite extensive literature techniques extracting tongue contours ultrasound data see 6264