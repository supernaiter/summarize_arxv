{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pdfminer.high_level import extract_text\n",
    "import openai\n",
    "import argparse\n",
    "\n",
    "# Add your API key to a file named .openai_api.txt\n",
    "with open(\".openai_api.txt\", \"r\") as f:\n",
    "    openai.api_key = f.read().strip()\n",
    "\n",
    "def remove_unnecessary_parts(text):\n",
    "    text = re.sub(r\"(?is)\\b(acknowledgments|acknowledgement|references)\\b.*\", \"\", text)\n",
    "    text = re.sub(r\"(?is)\\bappendix\\b.*\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # tokenization\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "def reduce_length_with_gpt35(text):\n",
    "    prompt = \"Summarize this paper into 8000 tokens or less:\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"gpt-3.5-turbo-16k\",\n",
    "        prompt=prompt + text\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "def generate_summary_with_gpt4(text):\n",
    "    prompt = \"この論文の要約を生成してください：\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "                model='gpt-4',\n",
    "                messages=[\n",
    "                    {'role': 'system', 'content': prompt},\n",
    "                    {'role': 'user', 'content': text}\n",
    "                ],\n",
    "                temperature=0.25,\n",
    "            )\n",
    "    summary = response['choices'][0]['message']['content']\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25994"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read text from target.text\n",
    "def read_text_from_file(file_name):\n",
    "    with open(file_name, \"r\") as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "text = read_text_from_file(\"target.txt\")\n",
    "\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. INTRODUCTION\\nA Silent Speech Interface, or SSI, is defined as a device enabling speech processing in the absence of an exploitable audio signal – for example, speech recognition obtained exclusively from video images of the mouth, or from electromyographic sensors (EMA) glued to the tongue. Classic applications targeted by SSIs include: 1) Voice-replacement for persons who have lost the ability to vocalize through illness or an accident, yet who retain the ability to articulate; 2) Speech communication in environments where silence is either necessary or desired: responding to cellphone in meetings or public places without disturbing others; avoiding interference in call centers, conferences and classrooms; private communications by police, military, or business personnel.\\nThe SSI concept was first identified as an outgrowth of speech production research, in tandem with the proliferation of the use of cellular telephones, in 2010 in a special issue of Speech Communication [1], where SSIs based on seven different non-acoustic sensor types were presented: 1) MHz range medical ultrasound (US) + video imaging of tongue and lips; 2) Surface electromyography, sEMG, sensors applied to the face and neck; 3) Electromagnetic articulography EMA sensors attached to tongue, lips, jaw; 4) Vibration sensors placed on the head and neck; 5) Non-audible murmur microphones, NAM, placed on the neck; 6) Electro-encephalography, EEG, electrodes; 7) Cortical implants for a “thought-driven” SSI.\\nDespite this activity, SSIs today remain for the most part specialized laboratory instruments. The performance of any automatic speech recognition (ASR) system is most often characterized by a Word Error Rate, or WER, expressed as a percentage of the total number of words appearing in a corpus. To date, no SSI ASR system has been able to achieve WER parity with state-of-the-art acoustic ASR. Indeed, a number of practical issues make SSI ASR systems considerably more involved to implement than their acoustic counterparts: 1. Sensor handling. While in acoustic ASR this may amount to no more than routine microphone protocol, SSIs’ non-acoustic sensors are often rather specialized (and expensive), and require physical contact with, or at a minimum careful placement with respect to, the speech biosignalproducing organs. This introduces problems of invasiveness; non-portability; and non-repeatability of sensor placement, bringing added complexity to SSI experiments.\\n2. Interference. An SSI should in principle be silent, but certain SSI modalities – vibration sensors, radar, and low frequency air-borne ultrasound, for example – are actually associated with signals that can propagate beyond the area of utilization of the SSI. The possibility of interference or interception may limit the adoption of these modalities outside the laboratory.\\n3. Feature extraction. While easily calculated Mel Frequency Cepstral Coefficients, MFCC, have been the acoustic ASR features of choice for decades, feature selection for the specialized sensors of SSIs remains an open question, particularly since many SSI modalities – ultrasound imaging, or EEG, for example – are of much higher intrinsic dimensionality than a simple acoustic signal. Furthermore, while the identification of stable phonetic signatures in acoustic data is today a mature field, the existence of salient landmarks in speech biosignals – arising from imaging modalities or electromyography, for example – is less evident.\\nMedical US operating in the MHz frequency range does not propagate outside the body. It is a well established [53] and documented [54] technique in speech production and speech pathology research, whose first use in the context of SSIs was discussed in [55]. US is a also relatively non-invasive modality, requiring only a transducer placed under the speaker’s chin, coupled with a small video camera in front of the mouth to capture lip movement. These sensors can be easily accommodated in a lightweight acquisition helmet, thus minimizing sensor placement issues. US tongue imaging, with added lip video, is thus in many ways an attractive modality for building a practical SSI.\\n1.2. The Silent Speech Challenge benchmark\\nIn 2010, an US + lip video SSI trained on the well-known TIMIT corpus achieved, with the aid of a language model (LM), a single speaker WER of 17.4% (84.2% “correct” word rate) on an independent test corpus [52], representing a promising early SSI result on a benchmark continuous speech recognition task. Subsequently, the raw image data of [52], that is, the original tongue ultrasound and lip videos, were made available online as the so-called Silent Speech Challenge, or SSC archive [56]. The purpose of the archive is to provide a stable data set to which newly developed feature extraction and speech recognition techniques can be applied. The SSC data will serve as the basis of all the experiments reported in this article.\\nAlthough a 17.4% WER for an SSI trained on a mono-speaker TIMIT corpus is “promising”, it must be remembered that standard acoustic ASR can obtain similar or superior scores after training on the full multi-speaker acoustic TIMIT corpus, a much more challenging task. Further advances are thus still necessary in order to truly put Silent Speech Recognition, SSR, on a par with acoustic ASR.\\nIn the past several years, improvements in acoustic speech recognition using Deep Neural NetworkHidden Markov Model (DNN-HMM) systems, rather than the traditional Gaussian Mixture ModelHMM (GMM-HMM), have become common. In this approach, a deep learning strategy is used to improve estimation of the emission probabilities of the HMM used for speech decoding. It is natural to ask to what extent a DNN-HMM approach can improve SSR performance as well. Despite the SSI implementation challenges outlined earlier, applications of deep learning techniques to SSR have indeed begun to appear. In [57], for example, tests are reported of phonetic feature discrimination for an EMGbased SSI, without a LM, on a small, experiment-specific speech corpus. In [58], deep learning on an EMA based SSI is explored, giving SSR phone error rates, PER, (which will be lower than WER) of\\n36%, when the Mocha-TIMIT corpus is used for training, testing, and the development of a specific bigram LM. In [59], a DNN-HMM is applied to the SSC benchmark data, albeit with a 38% WER, in a study comparing the efficacy of different feature extraction methods.\\nThe present article reports the first application of the DNN-HMM approach to the SSC recognition benchmark using the same input features and decoding strategy as those reported in [52], thus allowing a direct comparison of performances. The SSR results obtained here are significantly improved compared to the archive, giving, in the best scenario, a 6.4% WER (94.1% “correct” word recognition rate), or a nearly threefold improvement over the benchmark value. In contrast to [57,58], furthermore, the LM used in [52], also employed here, was developed on a completely independent speech corpus. In adition, results with a second, less task-specific LM are included in the present article. Finally, tests of reduced dimensionality feature vectors, as well as completely new input features created from raw SSC archive data, are also reported here. All new features have been added to the SSC archive for future use by other researchers.\\nIn the remainder of the article, the details of the SSC data acquisition system and a description of the available archive data are first summarized, in Section 2. Section 3 then describes the feature extraction strategy developed for the present study; while full details of the DNN-HMM based speech recognition procedure appear in Section 4. The results are summarized in Section 5, and some conclusions and perspectives for future work outlined in the final section.\\n2. SSC DATA ACQUISITION AND ARCHIVE RESOURCES\\nThe SSC data acquisition system consisted of an acquisition helmet holding a 128 element, 4-8 MHz US probe for tongue imaging, and a black and white, infrared-illuminated video camera to capture the lips. The 320×240 pixel tongue images and 640×480 pixel lip images created by the system were acquired in a synchronized manner at 60 frames per second (fps) using the Ultraspeech multisensory acquisition system [60].\\nThe SSC training corpus consists of US and lip video data from a single native English speaker pronouncing the 2342 utterances (47 lists of 50 sentences) of the TIMIT corpus, in the non-verbalized punctuation manner. The speech was recorded silently, i.e., without any vocalization; there is therefore no audio track. The test set is comprised of one hundred short sentences selected from the WSJ0 5000word corpus [61] read by the same speaker. The data are available at the web address indicated in [56]. The archive initially contained only the raw ultrasound and lip images of the training and test sets; the original features used, as well as the reduced-length feature vectors and new features created for the present article (see section 3), have now been appended to it. Speech recognition for the Challenge data was carried out in a standard GMM-HMM scheme and made use of a LM, which is also included in the archive. Further details appear in section 4.\\n3. FEATURE EXTRACTION\\n3.1. Introduction\\nAs mentioned earlier, speech recognition from non-acoustic sensor data faces the problem of discovering an effective feature recognition strategy, and US + lip video SSIs, although attractive in many ways, share this drawback. Being based on images, their intrinsic input dimensionality may be of the order of 1 million pixels. Some means of dimension-reducing feature extraction is thus critical. (The following discussion is centered on tongue features. Lip features, which are much easier to handle, will for overall coherence be extracted in the same way as tongue features.)\\n3.2. Contour extraction approach\\nTongue contour extraction is a tempting choice for reducing dimensionality that retains visual interpretability of the features. In ultrasound imaging of the tongue, the air-tissue boundary at the upper surface of the tongue produces a bright, continuous contour, referred to in a side-looking scan as the sagittal contour. Image processing tools for automatically extracting and characterizing this contour make ultrasound imaging a powerful tool for the study of speech production [53], [54]. Unfortunately, despite extensive literature on techniques for extracting tongue contours from ultrasound data (see [62][64] and '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove unnecessary parts\n",
    "text = remove_unnecessary_parts(text)\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1641\n",
      "1112\n"
     ]
    }
   ],
   "source": [
    "# Remove unnecessary parts\n",
    "#text = remove_unnecessary_parts(text)\n",
    "\n",
    "# check the length of the text\n",
    "print(len(text.split()))\n",
    "\n",
    "# Preprocess text\n",
    "text = preprocess_text(text)\n",
    "\n",
    "# check the length of the text\n",
    "print(len(text.split()))\n",
    "\n",
    "# write the preprocessed text to a file\n",
    "with open(\"preprocessed.txt\", \"w\") as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qa_maker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
