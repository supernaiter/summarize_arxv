{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./papers/2303.01639.pdf ...\n",
      "Completed processing ./papers/2303.01639.pdf. Result saved to ./papers/2303.01639.pdf_result.json\n",
      "Processing ./papers/3569474.pdf ...\n",
      "Completed processing ./papers/3569474.pdf. Result saved to ./papers/3569474.pdf_result.json\n",
      "Processing ./papers/3544548.3580801.pdf ...\n"
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac@gmail.com/My Drive/2023/summarize_arxv/playground2.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 117>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac%40gmail.com/My%20Drive/2023/summarize_arxv/playground2.ipynb#W0sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCompleted processing \u001b[39m\u001b[39m{\u001b[39;00mpdf_filepath\u001b[39m}\u001b[39;00m\u001b[39m. Result saved to \u001b[39m\u001b[39m{\u001b[39;00mpdf_filepath\u001b[39m}\u001b[39;00m\u001b[39m_result.json\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac%40gmail.com/My%20Drive/2023/summarize_arxv/playground2.ipynb#W0sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac%40gmail.com/My%20Drive/2023/summarize_arxv/playground2.ipynb#W0sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m     main()\n",
      "\u001b[1;32m/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac@gmail.com/My Drive/2023/summarize_arxv/playground2.ipynb Cell 1\u001b[0m in \u001b[0;36mmain\u001b[0;34m(dir)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac%40gmail.com/My%20Drive/2023/summarize_arxv/playground2.ipynb#W0sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39m# If the text is too long, reduce it with GPT-3.5\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac%40gmail.com/My%20Drive/2023/summarize_arxv/playground2.ipynb#W0sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(text\u001b[39m.\u001b[39msplit()) \u001b[39m>\u001b[39m \u001b[39m8000\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac%40gmail.com/My%20Drive/2023/summarize_arxv/playground2.ipynb#W0sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m     text \u001b[39m=\u001b[39m reduce_length_with_gpt35(text)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac%40gmail.com/My%20Drive/2023/summarize_arxv/playground2.ipynb#W0sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m \u001b[39m# Generate a summary and metadata with GPT-4\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac%40gmail.com/My%20Drive/2023/summarize_arxv/playground2.ipynb#W0sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mAnalyze the following paper and provide the required information. Keep the metadata in the original language. Summarize each content item in Japanese, with each item summarized to a maximum of 180 characters.\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32m/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac@gmail.com/My Drive/2023/summarize_arxv/playground2.ipynb Cell 1\u001b[0m in \u001b[0;36mreduce_length_with_gpt35\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac%40gmail.com/My%20Drive/2023/summarize_arxv/playground2.ipynb#W0sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreduce_length_with_gpt35\u001b[39m(text):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac%40gmail.com/My%20Drive/2023/summarize_arxv/playground2.ipynb#W0sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSummarize this paper into 8000 tokens or less:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac%40gmail.com/My%20Drive/2023/summarize_arxv/playground2.ipynb#W0sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac%40gmail.com/My%20Drive/2023/summarize_arxv/playground2.ipynb#W0sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m         engine\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-3.5-turbo-16k\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac%40gmail.com/My%20Drive/2023/summarize_arxv/playground2.ipynb#W0sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m         prompt\u001b[39m=\u001b[39;49mprompt \u001b[39m+\u001b[39;49m text\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac%40gmail.com/My%20Drive/2023/summarize_arxv/playground2.ipynb#W0sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac%40gmail.com/My%20Drive/2023/summarize_arxv/playground2.ipynb#W0sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/miniforge3/envs/qa_maker/lib/python3.8/site-packages/openai/api_resources/completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/miniforge3/envs/qa_maker/lib/python3.8/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/miniforge3/envs/qa_maker/lib/python3.8/site-packages/openai/api_requestor.py:230\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    210\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    211\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    218\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    219\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    220\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    221\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    222\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    229\u001b[0m     )\n\u001b[0;32m--> 230\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    231\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/miniforge3/envs/qa_maker/lib/python3.8/site-packages/openai/api_requestor.py:624\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    617\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    618\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    619\u001b[0m         )\n\u001b[1;32m    620\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    621\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 624\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    625\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    626\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    627\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    628\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    629\u001b[0m         ),\n\u001b[1;32m    630\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    631\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/qa_maker/lib/python3.8/site-packages/openai/api_requestor.py:687\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    685\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    686\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 687\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    688\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    689\u001b[0m     )\n\u001b[1;32m    690\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?"
     ]
    }
   ],
   "source": [
    "def main(dir=\"./papers\"):\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pdfminer.high_level import extract_text\n",
    "import openai\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Add your API key to a file named .openai_api.txt\n",
    "with open(\".openai_api.txt\", \"r\") as f:\n",
    "    openai.api_key = f.read().strip()\n",
    "\n",
    "def remove_unnecessary_parts(text):\n",
    "    text = re.sub(r\"(?is)\\b(acknowledgments|acknowledgement|references)\\b.*\", \"\", text)\n",
    "    text = re.sub(r\"(?is)\\bappendix\\b.*\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # tokenization\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "def reduce_length_with_gpt35(text):\n",
    "\n",
    "    shortning_schema = {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"title\": {\"type\": \"string\"},\n",
    "            \"author\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "            \"journal/conference\": {\"type\": \"string\"},\n",
    "            \"year\": {\"type\": \"string\"},\n",
    "            \"abstract\": {\"type\": \"string\"},\n",
    "            \"CCS_concept\": {\"type\": \"string\"},\n",
    "            \"bib\": {\"type\": \"string\"},\n",
    "            \"keywords\": {\"type\": \"string\"},\n",
    "            \"introduction\": {\"type\": \"string\"},\n",
    "            \"related_work\": {\"type\": \"string\"},\n",
    "            \"body\": {\"type\": \"string\"},\n",
    "            \"discussion\": {\"type\": \"string\"},\n",
    "            \"result\": {\"type\": \"string\"},\n",
    "            \"future_work\": {\"type\": \"string\"},\n",
    "            \"conclusion\": {\"type\": \"string\"},\n",
    "            \"references\": {\"type\": \"string\"},\n",
    "            \"appendix\": {\"type\": \"string\"},\n",
    "            \"github\": {\"type\": \"string\"},\n",
    "            \"doi\": {\"type\": \"string\"}\n",
    "        },\n",
    "        \"required\": [\"title\", \"author\", \"abstract\", \"introduction\", \"related_work\", \"body\", \"discussion\", \"result\", \"future_work\", \"conclusion\", \"references\", \"appendix\", \"github\", \"doi\", \"keywords\"]\n",
    "    }\n",
    "    \"\"\"\n",
    "    prompt = \"Summarize this paper into 8000 tokens or less:\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"gpt-3.5-turbo-16k\",\n",
    "        prompt=prompt + text\n",
    "    )\n",
    "    \"\"\"\n",
    "    prompt = \"shorten this paper into 8000 tokens or less:\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4-0613\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\":f\"{prompt}\"},\n",
    "            {\"role\": \"user\", \"content\": \"Analyze the following paper and provide the required information.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"The paper is: [{text}]\"}\n",
    "        ],\n",
    "        functions=[{\"name\": \"shortning_paper\", \"parameters\": shortning_schema}],\n",
    "        function_call={\"name\": \"shortning_paper\"},\n",
    "        temperature=0,\n",
    "    )\n",
    "    # Convert the generated metadata to a JSON object\n",
    "    try:\n",
    "        json_result = json.loads(completion.choices[0].message.function_call.arguments)\n",
    "    except:\n",
    "        print(\"Error parsing JSON result.\")\n",
    "        print(completion.choices[0].message.function_call.arguments)\n",
    "\n",
    "    return json_result\n",
    "\n",
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "    \"title\": {\"type\": \"string\"},\n",
    "    \"author\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "    \"jornal/conference\": {\"type\": \"string\"},\n",
    "    \"year\": {\"type\": \"string\"},\n",
    "    \"abstract\": {\"type\": \"string\"},\n",
    "    \"CCS_concept\": {\"type\": \"string\"},\n",
    "    \"bib\": {\"type\": \"string\"},\n",
    "    \"metadata\": {\"type\": \"object\"},  # „Åì„Åì„Å´„Åï„Çâ„Å™„Çã„É°„Çø„Éá„Éº„Çø„ÅåÂÖ•„Çä„Åæ„Åô\n",
    "    \"keywords\": {\"type\": \"string\"},\n",
    "    \"problem\": {\"type\": \"string\"},\n",
    "    \"method\": {\"type\": \"string\"},\n",
    "    \"interaction\": {\"type\": \"string\"},\n",
    "    \"technical_contribution\": {\"type\": \"string\"},\n",
    "    \"result\": {\"type\": \"string\"},\n",
    "    \"github\": {\"type\": \"string\"},\n",
    "    \"doi\": {\"type\": \"string\"}\n",
    "    },\n",
    "    \"required\": [\"title\", \"metadata\", \"keywords\", \"problem\", \"method\", \"interaction\", \"technical_contribution\", \"result\", \"github\", \"doi\", \"texts\"]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pdfminer.high_level import extract_text\n",
    "import openai\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Add your API key to a file named .openai_api.txt\n",
    "with open(\".openai_api.txt\", \"r\") as f:\n",
    "    openai.api_key = f.read().strip()\n",
    "\n",
    "def remove_unnecessary_parts(text):\n",
    "    text = re.sub(r\"(?is)\\b(acknowledgments|acknowledgement|references)\\b.*\", \"\", text)\n",
    "    text = re.sub(r\"(?is)\\bappendix\\b.*\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # tokenization\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "def get_metadata_using_gpt35_json(text):\n",
    "\n",
    "    shortning_schema = {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"title\": {\"type\": \"string\"},\n",
    "            \"author\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "            \"journal/conference\": {\"type\": \"string\"},\n",
    "            \"year\": {\"type\": \"string\"},\n",
    "            \"abstract\": {\"type\": \"string\"},\n",
    "            \"CCS_concept\": {\"type\": \"string\"},\n",
    "            \"bib\": {\"type\": \"string\"},\n",
    "            \"keywords\": {\"type\": \"string\"},\n",
    "            \"references\": {\"type\": \"string\"},\n",
    "            \"appendix\": {\"type\": \"string\"},\n",
    "            \"github\": {\"type\": \"string\"},\n",
    "            \"doi\": {\"type\": \"string\"}\n",
    "        },\n",
    "        \"required\": [\"title\", \"author\", \"abstract\",\"appendix\", \"github\", \"doi\", \"keywords\"]\n",
    "    }\n",
    "    prompt = f\"Your task is to analyze the following academic paper and extract information according to a specified JSON format.\"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-16k\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\":f\"{prompt}\"},\n",
    "            {\"role\": \"user\", \"content\": f\"The paper is: [{text}]\"}\n",
    "        ],\n",
    "        functions=[{\"name\": \"shortning_paper\", \"parameters\": shortning_schema}],\n",
    "        function_call={\"name\": \"shortning_paper\"},\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "    # Convert the generated metadata to a JSON object\n",
    "    try:\n",
    "        json_result = json.loads(response.choices[0].message.function_call.arguments)\n",
    "        return json_result\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        #print(\"Error parsing JSON result.\")\n",
    "        print(response.choices[0].message.function_call.arguments)\n",
    "        return None\n",
    "    \n",
    "def reduce_length_with_gpt3(text):\n",
    "\n",
    "    prompt = \"Your task is to analyze the following academic paper and compress it within 8000 words, but over 4000 words.\"\n",
    "\n",
    "    print(text)\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-16k\",\n",
    "    max_tokens=6000,\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\":f\"{prompt}\"},\n",
    "            {\"role\": \"user\", \"content\": f\"The paper is: [{text}]\"}\n",
    "    ],\n",
    "    )\n",
    "\n",
    "    print(response)\n",
    "\n",
    "    #return response.choices[0].text.strip()\n",
    "    return response.choices[0].message.content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./papers/2303.01639.pdf ...\n",
      "4871\n",
      "WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions Jun Rekimoto The University Tokyo 731 Hongo Bunkyoku Tokyo Japan Sony Computer Science Laboratories Kyoto 131 Hontorocho Shimogyoku Kyotoshi Kyoto Japan rekimotoacmorg 3 2 0 2 r M 3 D S c 1 v 9 3 6 1 0 3 0 3 2 v X r Figure 1 WESPER realtime whispertonormal speech conversion mechanism consisting speechtounit STU en coder generates common speech units whispered normal utterances using selfsupervised pretraining unittospeech UTS decoder recovers speech speech units It achieves userindependent voice conversion real time ABSTRACT Recognizing whispered speech converting normal speech creates many possibilities speech interaction Because sound pressure whispered speech significantly lower normal speech used semisilent speech interaction public places without audible others Converting whispers normal speech also improves speech quality people speech hearing impairments However conventional speech con version techniques provide sufficient conversion quality require speakerdependent datasets consisting pairs whis pered normal speech utterances To address problems propose WESPER zeroshot realtime whispertonormal speech conversion mechanism based selfsupervised learning WESPER consists speechtounit STU encoder generates hidden speech units common whispered normal speech unittospeech UTS decoder reconstructs speech encoded speech units Unlike existing methods conversion userindependent require paired dataset whis pered normal speech The UTS decoder reconstruct speech target speaker ‚Äô voice speech units requires unlabeled target speaker ‚Äô speech data We confirmed quality speech converted whisper improved preserving natural prosody Additionally confirmed effectiveness proposed approach perform speech construction people speech hearing disabilities CCS CONCEPTS ‚Ä¢ Humancentered computing ‚Üí Soundbased input output Interface design prototyping Mobile devices ‚Ä¢ Computing method ologies ‚Üí Neural networks Permission make digital hard copies part work personal classroom use granted without fee provided copies made distributed profit commercial advantage copies bear notice full citation first page Copyrights components work owned others authors must honored Abstracting credit permitted To copy otherwise republish post servers redistribute lists requires prior specific permission andor fee Request permissions permissionsacmorg CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany ¬© 2023 Copyright held ownerauthors Publication rights licensed ACM ACM ISBN 97814503942152304 1500 httpsdoiorg10114535445483580706 KEYWORDS speech interaction whispered voice whispered voice conversion silent speech artificial intelligence neural networks selfsupervised learning ACM Reference Format Jun Rekimoto 2023 WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions In Proceedings 2023 CHI Conference Human Factors Computing Systems CHI ‚Äô 23 Energy PredictorPitch PredictorWhisper Hoarse Normal VoicesTransformer LayersMelSpectrogramReconstructed Normal VoiceCNN EncoderTransformer LayerTransformer LayerTransformer LayerTransformer LayerCommon speech unitsbetween whisper normal voices Modified FastSpeech2SpeechtoUnit STU EncoderUnittoSpeech UTS DecoderModified HuBERT Vocoder HiFiGANMelSpectrogram Decoder256one per 20msSpeaker independentincluding persons speech hearing disabilitiesCTC Speech RecogRecognized text CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany JRekimoto April 23‚Äì28 2023 Hamburg Germany ACM New York NY USA 13 pages httpsdoiorg10114535445483580706 1 INTRODUCTION Although voice interaction systems widely deployed typically easy use presence people Using voice commands public places may socially unaccept able risk confidential information leaked In addition speaking conference call uncomfortable people vicinity compromise confidentiality conversation To overcome problems various silent speech input tech niques developed 3 31 32 50 51 54 however methods require special sensors achieved high accu racy speech recognition remaining instead level recog nizing predefined commands Conversion unconditioned silent speech normal vocal utterances also achieved Additionally silent speech could employed capture utter ances people speech hearing impairments However owing abovementioned limitations existing methods meet requirements practical accessibility aids In contrast silent speech focus whispered speech Whis pers sufficiently low sound pressure ensure confidentiality whispering almost equivalent silent speech Whispering captured ordinary microphone require special sensor configuration People speech disorders still speak whisper hoarse voice although vocal organs may injured extracted thus voices might recognized To address issues propose WESPER realtime zeroshot whispertonormal voice conversion method based selfsupervised learning It designed perform speaker inde pendent conversion whispered speech normal speech There need peruser training paired datasets whispered normal utterances required The proposed architec ture consists speechtounit STU encoder pretrained normal whispered speeches unittospeech UTS decoder generates target voice speech units Figure 1 By pretraining unpaired whisper normal voices STU trained reduce difference normal whispered utterances output common speech unit The UTS decoder trained speech data specific speaker without accompanying text labels If person ‚Äô voice used conversion performed restore whispered voice person ‚Äô normal voice even another person ‚Äô voice Because encoder decoder operate nonautoregressive manner entire system operates realtime Therefore applied teleconferencing example conference participant speak whispered voice converted real time others listen speech played back normal voice Figure 2 shows melspectrogram examples whispertonormal voice conversion using proposed method More conversion results demonstrated accompanying video The contributions study summarized follows Figure 2 WESPER conversion result Left whispered speech Middle whispered speech converted WESPER Right Normal speech speaker tran scription ‚Ä¢ We propose realtime speakerindependent vocabularyfree whispertonormal speech conversion method trained unpaired whispers normal speech ‚Ä¢ The target voice learned voice samples specific speaker without text transcription ‚Ä¢ We experimentally confirmed improvement performance normal speakers dysarthric hearingimpaired speakers 2 RELATED WORK 21 Research Silent Whispered Speech Various studies silent speech developed methods rec ognize user ‚Äô silent utterances silent commands using various sensor configurations including lip reading EMG Electromyogra phy ultrasound 3 31 32 50 51 54 However systems require special dataset perform training increases need special sensors prevents technologies ing widely used Hence systems offer limited vocabulary less 100 commands available typically around 30 Al though silent speech recognition sometimes identified possible approach help people dysphonia methods interpret unrestricted free speech owing vocabulary limitation Whispered speech similar characteristics silent speech preserving social acceptability public spaces However widely adopted given ordinary mi crophones used SilentVoice 15 speech interaction technique uses gressive speech utterance made inhaling The amplitude ingressive speech low considered variation silent speech It requires microphonelike device placed close mouth training required users speak correctly ingressive speech A custom corpus ingressive speech text transcriptions also required Research whisper voice recognition 6 11 14 18 previ ously conducted The Alexa smart speaker supports whisper mode 23 In mode user whispers Alexa Alexa responds whisper 10 DualVoice 48 also proposed method per form endtoend recognition whispered voices based self supervised speech recognition system based wav2vec20 2 HuBERT 21 They also proposed interaction technique WhisperWESPER convertedWhisperNormalsecsecsecHz WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany model training data peruser dataset conversion normal voice whisper speech recognition silent speech ex3 31 32 50 51 54 special paireddataset dedicated sensors required Parotoron 4 paired whispernormal voice required DualVoice 48 SilentVoice 15 CycleGANVC 30 MSpeCNet 37 AGANW2SC 16 WESPER labeled unpaired whisper normal voice labeled silentvoice custom unpaired unlabeled whisper normal voice paired whispernormal voice paired whispernormal voice unpaired unlabeled whisper normal voice required required required required required required YES NO NO YES YES YES YES Table 1 Research silent whispered speech YES commands characters YES voice conversion YES YES NO NO NO YES voice conversion distinguish whispered normal utterances WESPER combined DualVoice selectively convert user ‚Äô whispered voice 22 Speech Conversion Normal speech conversion technologies developed convert one voiced utterance another 19 29 30 43 46 however conversion quality methods remains unsatisfactory applied whispertonormal voice conversion Recently several machine learningbased whispertonormal voice conversion techniques investigated 41 To com pare techniques important consider quality conversion required characteristics dataset used training A paired whispernormal voice dataset ie dataset containing whispered normal versions utter ance labeled whispered voice dataset ie dataset containing whispered utterances accompanied text labels require significant amount effort prepare Conversely unpaired unlabeled whisper voice dataset used ie dataset containing whisper utterances without corresponding normal versions labels effort required prepare dataset low Attentionguided generative adversarial network whisper normal speech conversion AGANW2SC GANbased whisper normal speech conversion 16 It converts whispered voice represented melspectrogram corresponding normal speech ‚Äô melspectrogram It based GAN attention map used conversion It requires paired whispernormal speech dataset MSpeCNet 37 autoencoderbased multi domain voice conversion supports whispertonormal conver sion It also requires paired whispernormal voice dataset Moreover whispered speech converted text using speech recognition generate normal speech using texttospeech methods 22 However approach requires labeled dataset whispered speech recognition prosodic information con tained whispered utterances lost intermediate text representation convey information Parrotron 4 speech conversion system designed im prove speech speakers dysplasia It based encoderdecoder model conforms texttospeech system Tacotron 58 however differs Tacotron input output data formatted melspectrograms It also quires paired source target speeches making construction required dataset relatively difficult Phonetic posteriorgrams PPGs 55 intermediate informa tion obtained automatic speech recognition PPGs used manytoone speaker conversion represent articulation spoken content speaker independently To knowledge effects PPGs whispered voice investigated Our method also uses intermediated speech units require textbased corpus case automatic speech recognition ASR PPGs In contrast proposed system requires samples un paired target source speech require accompa nying text transcriptions thus making datasets easy prepare independent target language The characteristics related technologies summarized Table 1 We also demonstrate whisper normal conversion exam ples including NMSEDiscoGAN 53 MSpeCNet 37 CycleGAN VC 30 AGANW2SC 16 WESPER httpswesperproj githubio 23 Self Supervised Representation Learning Speech Recently combining pretraining selfsupervised representa tion learning unlabeled speech data finetuning labeled speech data attracted attention These systems primarily CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany JRekimoto intended speech recognition applications also applied perform speaker language emotion recogni tion 44 59 In particular pretraining method hiddenunit BERT hu BERT 21 similar masked language model used bidirectional encoder representations transformers BERT 12 natural language processing It designed mask part input estimate corresponding expression features rest input With pretraining model learn acoustic properties input data characteristics speech For use ASR pretraining finetuning performed small amount audio data text transcriptions A projection layer connectionist temporal classification CTC layer 17 added generate text transcriptions audio waveforms As reported 2 59 selfsupervised ASR achieved speech recognition accuracy comparable conventional stateoftheart ASRs finetuning small amount labeled speech data Therefore architecture could suitable recogniz ing whispered voice recognition limited whispered speech corpora WESPER unique uses pretraining reduce dif ference latent vectors encoding whispered normal utterances 24 TextlessNLP Recently textfree speech processing speech conversion meth ods developed Through selfsupervised learning systems derive latent representations speech data accompanied text transcriptions TextlessNLP34 35 Au dioLM 5 use text transcriptions phoneme symbols speech processing systems use discrete units constructed selfsupervised learning Soft discrete unit another approach textless speech processing 57 Our proposed method also uses nondiscrete vectors latent speech representations obtained selfsupervised learning explicitly use text transcriptions phoneme symbols Our research unique demonstrate whispered normal speech represented similar speech units selfsupervised learning In addition method also designed work seamlessly speech generation system later stages 3 THE WESPER VOICE CONVERSION MODEL WESPER consists STU encoder UTS decoder The STU converts whispered normal speech common speech units The UTS converts common speech units melspectrograms reconstructed speech vocoder WESPER char acterized fact common speech units utterance similar although STU pretrained unpaired whispered normal speech trained paired set whispered normal utterances The details method used train model described Figure 3 Overview STU pretraining Unpaired whis pered normal speech used pretraining The trans former layer trained estimating discrete units masked input The projection layer transformer layers generates 256dimensional vectors one every 20 ms used common speech units 31 SpeechtoUnit STU Encoder The STU encoder takes audio waveform input outputs units It based HuBERT 21 selfsupervised neural network speech pretrains large amount unlabeled speech BERT 12like manner learns recover relevant parts partially masked speech features thereby acquires speech language model For purpose want generate speech units identical possible whispered normal speech To achieve pretrained STU mixture whispered normal speech utterances used normal English speech many speakers Librispeech 960h dataset 42 Librispeech speech data mechanically converted whispered voice us ing LPCbased audio conversion tool 60 Additionally used wTIMIT speech dataset normal whispered speech 36 speech length 58 h Figure 3 shows pretraining process STU detail Only unpaired whispered normal speech used pretraining The transformer layer trained estimating discrete units masked input Similar HuBERT discrete target units first generated kmeans clustering input speech data first stage kmeans clustering outputs transformer intermediate layer In experiment used 100 discrete units The projection layer transformer layers generates sequence 256dimensional vectors one per 20 ms used common speech units In STU 12 transformer layers placed front CNN feature extractor Figure 1 After pretraining comparing output layer confirmed 1 difference Transformer Layer 11Transformer Layer 12ProjectionLEN 768LEN 256Label Embedding256 KLEN KLENCrossEntropy LossCosinesimilarityTo Unit Speech CNN EncoderWhisper Normal voicesunpairedTransformer Layer 1MaskingAcoustic unit discoveryeg KMeans MFCC HuBERTintermediate layerCommon Speech UnitK Discrete unitsK100Predict hidden units masked locations25620ms WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany In addition finetune STU transcriptattached wTIMIT Librispeech datasets adding CTC layer 17 48 therefore STU could used ASR recognize whis pered normal speeches 32 UTS Decoder The UTS decoder takes speech units generated STU input reconstructs target normal speech It based nonautoregressive texttospeech TTS system FastSpeech2 49 While original FastSpeech2 includes embedding layer takes text phoneme tokens transforms sequence vector embeddings eliminate layer UTS takes speech units direct input The original FastSpeech2 also includes duration estimator estimates duration phoneme token length regulator adjusts number internal vectors according estimated duration These parts also eliminated purpose STU generates speech units constant rate When learning original FastSpeech2 necessary pro vide duration phoneme corpus ground truth external tool Montreal Forced Aligner 38 Because limitation FastSpeech2 ‚Äô learning languagedependent Conversely UTS require duration estimation UTS languageindependent The output UTS melspectrogram FastSpeech2 A vocoder HiFiGAN 33 converts actual speech waveform In regular TTS system target speech corresponding text labels needed training By contrast proposed UTS requires target speech text labels The target speech passed STU obtain sequence speech units associated speech waveform used train UTS Figure 6 In experiment UTS trained using speech data single speaker LJSpeech 27 data speakers taken narration data 4 SYSTEM CONFIGURATION We designed WESPER model using PyTorch framework The STU based HuBERT UTS based modified implementation FastSpeech2 PyTorch implementation 8 We used Librispeech wTIMIT include normal whispered speech pretraining With dual NVIDIA R6000 pretraining took 48 h UTS training took 26 h target voice The total processing time required perform conversion approximately 120th actual speech duration single NVIDIA R6000 110th Apple M1 Max CPU The actual quality conversion demonstrated attached video We designed two types interfaces The first operated pushtotalk style user first speaks whispered normal voice pressing button Thereafter ton released speech waveform recorded time sent speechconversion neural networks result immediately played back The detected nonaudio period input speech automatically converted speech segments without additional user input Figure 4 Comparison whisperednormal voice differ ences The comparison made using normal speech speech whispering transformations Above An STU pretrained normal whispered speeches produced fewer differences pretrained normal speech As transformer layer deepens difference tween two decreases Middle There difference melspectrogram decreased speech units Bottom UMAP 39 visualizations differences tween whisper normal voices melspectrogram speechunit ‚Äô space speech unit values whispered normal voices de creased increasing layer depth 2 difference decreased STU pretrained whispered normal speech utterances compared pretraining normal speech Figure 4 Figure 4 bottom also shows UMAP 39 vi sualization whispered normal speech melspectrogram space common speech unit space The corresponding periods two speeches connected lines As shown figure differences reduced common speech unit space except long distances Although speech feature values whispered normal voices different assume addressed self supervised pretraining utterances similar linguistic standpoints represented similar units We speculate learning method extract common pronunciations normal whispered utterances similar selfsupervised pretraining methods extract common pronunciations utterances different speakers ASR system CNN Encoder012341112Output transformer intermediate layersSTU pretrained normal voicesSTUpretrained whispered normal voices121086420Mean vector differenceWhisper Normal voicesTransformer Layer 1Transformer Layer 2Transformer Layer 3Transformer Layer 4Transformer Layer 11Transformer Layer 12DifferencereductionNormalWhisperdiffMelspectrogramSpeech UnitsMelspectrogramSpeech unit CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany JRekimoto Figure 5 Comparison FastSpeech2 49 UTS decoders FastSpeech2 needs predict duration phoneme whereas UTS accepts common speech unit duration This eliminates duration predictor length regulator LR figure Phoneme embedding also eliminated common speech unit consist discrete tokens Figure 6 STU training 1 UTS training 2 The UTS learns decode speech units target speech using wave data target voice frozen STU No text la beled dataset required Notably WESPER trained labeled corpus pretrained normal whispered voices Figure 7 shows current WESPER speech input configurations We tested headset b directional microphone four ar rayed MEMS Microelectromechanical systems microphones 52 c mobile phone microphone popguard avoid whis pering pop noise soundproofing material reduce ambient noise 5 EVALUATION The WESPER mechanism allows whispertonormal conversion independent input speaker Here evaluate conversion Figure 7 WESPER speech input device headset b ar ray directional microphone c cell phone microphone popprotection soundproofing material quality three aspects considering whispertonormal conver sion voice reconstruction people speech hearing disorders 51 Quality WhispertoNormal Conversion To evaluate quality converted speech recruited 50 genderbalanced participants online using Prolific crowdsourc ing system 25 18 fluent English Each participant listened four sets normal speech whispered speech WESPERconverted whispered speech 12 voices total webbased user interface rated utterances 5point Mean Opinion Score MOS questionnaires examples voices included attached video We used LJSpeechtrained WESPER voice target voice transcription sentence voices avoid differ ences impressions based sentence content The results presented Figure 8 Figure 8 shows sults MOS evaluation The WESPERconverted voice showed score normal whispered voices It con firmed WESPER conversion improved MOS original whispered speech ùëù 001 pairwise ttest Cohen ‚Äô effect size 054 Figure 8 b shows responses question ‚Äú Is voice hoarse normal ‚Äù clear improvement voices converted WESPER ùëù 001 Figure 8 c shows responses question ‚Äú Is voice using consistent artic ulation standard intonation prosody ‚Äù Here WESPER whisper showed almost equal results Notably WESPER affect naturalness prosody Considering speech con verted WESPER generated unittospeech module vocoder result indicates WESPER preserve natural prosody original speech Multiple Stimuli Hidden Reference Anchor MUSHRA eval uation In addition MOS test evaluated speech quality us ing Multiple Stimuli Hidden Reference Anchor MUSHRA MUSHRA method evaluating perceived quality audio defined ITUR Recommendation BS15343 56 MUSHRA uses anchor audio audios evaluator expected Energy PredictorPitch PredictorTransformer LayersMelSpectrogramb UnittoSpeech UTS DecoderMelSpectrogram DecoderEnergy PredictorPitch PredictorTransformer LayersMelSpectrogramMelSpectrogram DecoderPhoneme EmbeddingDuration PredictorLRCommon Speech Unita FastSpeech2PhonemePositional EncodingPositional EncodingSpeech Unit Unit Speechtarget voiceno text labelslossPredicted melspectrogramTarget melspectrogramSpeech unitMelspectrogramconversionpitch energyextractionPredictedPitch energylossfrozentrainingSpeech Unit Acoustic unit discovery systemeg KMeans MFCCHuBERTintermediate layerNormal voicesWhisper voicestrainingSelfsupervised learning masked token prediction1unpairedSTU trainingUTS training2abcpop shieldsoundproof material WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany Figure 8 Quality whispertonormal conversion Mean Opinion Scores MOS normal whispered WESPER converted whispered voices b Hoarsenormal voice rating c natural prosody rating se standard error ùëù 001 ttest ns significant model train finetune test WER CER BLEU Google trained Google wTIMITN wTIMITW WESPERwTIMITW HuBERT base wTIMITN Librispeech Librispeech wTIMITW librispeech wTIMITNW wTIMITW 1155 4470 2668 2106 3306 1375 466 2838 1270 817 1545 547 076 034 052 054 038 070 wTIMITN wTIMIT 36 normal voice Google Google Cloud SpeechtoText 24 wTIMITW wTIMIT whisper voice WESPER‚Ä¢ WESPER converted ‚Ä¢ HuBERT HuBERT base model pretrained Librispeech 42 wTIMITNW Table 2 Whispered voice recognition accuracy Google Cloud SpeechtoText 24 reference ASR The recognition rate whispered voice normal ASR high however result converting whispered voice normal voice using WESPER recognized recognition rate improved Notably WESPER trained labeled data pretrained unlabeled unpaired normal whispered voices assign ratings 0 100 audio comparing anchor audio reference For rating participants play voice many times like Because presence anchor hidden reference audio MUSHRA considered reliable MOS We recruited 50 genderbalanced Englishspeaking participants age 18 via Internet using Prolific crowdsourcing system 25 We used javascriptbased MUSHRA testing tool 28 online evaluation Figure 9 system available https githubcomrkmtmushraJSprolific We used whisper normal voice samples previous MOS test The participants ‚Äô responses collected Internet The results presented Figure 10 It revalidates results MOS evaluation ùëù 001 pairwise ttest effect size 057 According evaluations draw following con clusions ‚Ä¢ WESPER convert whispered voices normal voices ‚Ä¢ Speechdata converted WESPER better MOS whispered source voice Figure 9 An example MUSHRA evaluation web inter faces nsabc CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany JRekimoto Figure 10 Speech quality evaluation whispered WESPER converted voices MUSHRA MUltiple Stimuli Hidden Reference Anchor ùëù 001 ttest ‚Ä¢ WESPER preserved natural prosody source whispered voice 52 Speech Recognition Accuracy There two possible ways use WESPER speech recognizer including speech recognition WESPER recognition speech converted WESPER speech recognizers In former method WESPER model based HuBERT pre trained whispered normal speech finetuned using whispered corpus text inferred whispered voice In latter case whispered speech converted WESPER con trol speechenabled device If whispered speech converted normal speech existing speechenabled devices used immediately without need modify whispered speech Using existing speech recognizer google cloud speechto text 24 reference measured speech recognition accu racy normal speech whispered speech whispered speech converted WESPER The wTIMIT corpus used measurements wTIMIT TIMITcompliant transcription containing normal whispered speech labels This corpus evaluated recogni tion accuracy whispered speech converted WESPER using recognition accuracy normal whispered speech baseline The results summarized Table 2 terms word error rate WER character error rate CER well bilingual eval uation understudy BLEU As shown table recognition accuracy high whispered speech directly rec ognized WER4470 accuracy improved WER2668 conversion WESPER When tested wTIMITW HuBERTbase pretrained Librispeech wTIMITNW shows better results google cloud speechtotext HuBERT WER3306 Google WER4470 It speculated effect pretraining mixture whis pered normal speech without finetuning whispered voice may contribute accuracy ASR Figure 11 Speech quality evaluation people speech disorders ranked 5point MOS ùëù 001 ùëù 005 S1S5 speakers VFP Vocal Fold Polyps SD Spasmodic Dys phonia Figure 12 Speech quality evaluation people vocal disabilities ranked MUSHRA ùëù 001 S1S5 speakers VFP Vocal Fold Polyps SD Spasmodic Dysphonia Notably WESPER trained labeled corpus It pre trained whispered normal speeches improves speech recognition accuracy Therefore speech recognition via WESPER conversion require preparation corpus whispered speech specific unspecified speakers 53 Evaluations Speech Reconstruction People Speech Disorders An important goal WESPER reconstruction atypical speech people speech disorders hearing impairments This makes speech understandable people familiar individual speech patterns Dysphonia involuntary hoarse breathy strained sound ing voice low volume pitch There various causes including spasms polyps vocal tract causes If vocal cords removed throat cancer causes voice becomes extremely difficult produce For hearing impair ment even vocal cords affected control vocal cords becomes difficult resulting dysphonia Electrolarynx used mechanically vibrate throat vocalization ns WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany fluent German addition English reference sentence used German The results presented Figures 11 12 13 14 Figure 11 shows results terms MOS For SD VFP WESPER converted voices higher MOS scores ùëù 001 effect size066 MUSHRA scores ùëù 001 effect size079 Figure 13 shows responses question ‚Äú Is voice hoarse normal ‚Äù clear improvement WESPERconverted voices compared original VFP SD voices ùëù 001 Figure 14 shows answers question ‚Äú Does voice use consistent articulation standard intonation prosody ‚Äù Here WESPER converted original voices showed nearly equal scores although WESPERconverted voices showed slightly better scores It assumed WESPER affect prosody original speech According evaluations make following con clusions ‚Ä¢ WESPERconverted voices people VFP SD speech disabilities showed better quality This suggests WESPER improve quality speech people con ditions terms intelligibility people unfamiliar individual speech patterns ‚Ä¢ Similarly WESPER improve naturalness original VFP SD speech ‚Ä¢ WESPER could also preserve natural prosody source speech Notably test performed German sentences Although WESPER pretraining performed English speech German speech prosody source speech preserved MOSs improved Therefore result may demonstrate languageindependence ability WESPER model The attached video shows example whisper normal conversion Japanese Because wav2vec 20 base model STU also shown languageindependent pretraining perfor mance 9 languageindependent ability WESPER could feature worth evaluating future 54 Speech Reconstruction Evaluation People Hearing Impairment Finally evaluated effect speech reconstruction WES PER people hearing impairments People hearing loss hear speech others therefore tend difficulty speaking way easily understood general speakers However vocal organs mal speech different characteristics people dysphasia We used ‚Äú corpus deaf speech acoustic speech production research ‚Äù 40 Utterances five speakers one hearing speaker four hearing impaired used We recruited 50 evaluation participants using Prolific 25 evenly balanced terms gender fluent English speak ers age 18 We asked participants perform 5point ranking utterance terms MOS hoarsenormal natural prosody The results presented Figures 15 16 17 18 These results indicate MOS MUSHRA rating scores Figure 13 HoarseNormal assessments utterances peo ple speech disorders ùëù 001 Figure 14 Speech prosody consistency utterances peo ple speech disorders ns significant people vocal cord damage sound produced artificial pitch conversion deterministic large devia tion normal vocalization The communication deficit caused dysphonia serious problem elimination voice conversion technology great social value To investigate quality improvement WESPER voice conversion evaluated speech utterances people two types speech disorders described Vocal Fold Polyps refer VFPs VFPs among common benign lesions larynx affecting quality voice production Spasmodic Dysphonia refer SD This also called laryngeal dystonia SD another common neurological disorder affects voice speech It lifelong condition causes spasms muscles produce voice We used Saarbruecken Voice Database SVD corpus 1 45 sample utterances people VFP SD The SVD commonly used corpus voices people speech disorders It contains recordings vowel utterances recording Ger man reference sentence ‚Äú Guten Morgen wie geht es Ihnen ‚Äù ‚Äú Good morning ‚Äù This sentence used evaluation Fifty participants recruited evaluation using Prolific 25 The recruited participants evenly balanced terms gender age 18 The participants nsnsnsnsnsns CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany JRekimoto Figure 15 Speech quality evaluation people hear ing impairment Speech quality ranked 5point MOS S1 ‚Äô S4 ‚Äô speakers ùëù 001 ùëù 005 Figure 18 Speech natural prosody consistency people hearing impairment ùëù 005 The results experiments summarized Table 3 6 DISCUSSIONS Combination WESPER UserDependent Finetuning The sults evaluation experiments indicated degree improvement speech people hearing impairments less speech people dysphasia There fore assume former significant prosodic varia tions Although primary purpose study achieve speakerindependent speech conversion pretraining alone plan conduct additional experiments investigate whether conversion performance improved applying smallscale finetuning speaker Even case believe speech pairs necessary text transcription required A pair whispered hoarse speech utterances normal speech converted com mon speech unit STU used instead text transcription case ASR finetuning Audio Input Device Suitable Whispered Speech As demonstrated study whispered hoarse voices converted normal voices In practice selection appropriate audio input device would important We currently testing proposed approach normal headsets directional array microphone designed smart speakers obtained good results For wearable devices nonaudible murmur NAM mi crophone detects skin vibrations could used inaudible utterances 20 Combining noise reduction techniques 7 61 another important future direction Philips Dyson also developing masks provide pow ered respiratory ventilation protect air pollution infectious diseases 13 26 A microphone placed inside masks pick whispered voice would give effect almost equivalent silent speech HumanAI Integration This research concerns machinelearning techniques converting whispered speech unspecified speakers normal speech In practice however established users noticed similar whispered utterances easily converted normal speech whereas others difficult They tried speak relying machine learning user Figure 16 Speech quality evaluation people hear ing impairment Speech quality ranked MUSHRA S1 ‚Äô S4 ‚Äô speakers ùëù 005 Figure 17 HoarseNormal assessments utterances peo ple hearing impairment ùëù 001 ùëù 005 higher converted WESPER ùëù 005 effect size 045 MOS ùëù 005 effect size 021 MUSHRA degree improvement less speakers voice disorders In addition prosody ratings significantly lower speech hearing impaired compared speech hearing speakers Therefore results indicate people hearing loss may difficulty controlling prosody speaking nsnsnsns WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany Speaker Type Normal Whisper WESPERWhisper MOS MUSHRA Q1 416 9001 415 290 5133 327 397 6364 379 VFP SD S1 WESPERS1 S2 WESPERS2 S3 WESPERS3 S4 WESPERS4 S5 WESPERS4 Speakers speech disorders 1913 3572 3666 5753 4219 6274 2751 4445 4986 6553 3520 5330 272 342 308 396 320 412 224 288 354 394 293 364 All WESPERAll 154 378 220 416 200 436 120 336 268 390 193 389 Q2 399 331 343 300 324 340 376 326 360 236 220 354 364 312 329 Normal S1 ‚Äô WESPERS1 ‚Äô S2 ‚Äô WESPERS2 ‚Äô S3 ‚Äô WESPERS3 ‚Äô S4 ‚Äô WESPERS4 ‚Äô All WESPERAll Speakers hearing impaired 9175 4305 4747 2076 2005 2830 3507 3586 4123 3207 3621 368 230 261 161 193 203 239 238 306 208 250 Mean Opinion Score MUltiple Stimuli Hidden Reference Anchor ‚Äò Is voice hoarse normal ‚Äô 375 251 297 184 255 222 293 259 335 212 236 376 229 238 174 206 219 239 226 262 228 294 ‚Äò Is voice use consistent articulation standard intonation prosody ‚Äô MOS MUSHRA Q1 Q2 WESPER‚Ä¢ WESPER converted ‚Ä¢ VFP SD S1S5 S1 ‚Äô S4 ‚Äô Vocal Fold Polyps speakers Spasmodic Dysphonia speakers speaker IDs Table 3 Summary Voice Conversion Quality Evaluation MUSHRA 100point score others5point score side This interaction suggests machine learning unilaterally extend human capabilities synergistic effects achieved learning human side well This seen actual example humanAI integration 47 vocalization 7 CONCLUSION In study proposed WESPER mechanism real time conversion whispered speech normal speech We con firmed common speech unit obtained selfsupervised learning even acoustic features whispered mal speech different Speech reproduction learned speech data arbitrary target speakers without using text labels There need train user need parallel data whispered normal speech From speech units WESPER reconstruct utterances target speaker requires unlabeled speech data target speakers We con firmed quality converted speech improved prosody speech preserved Additionally reported evaluation results reconstructing speech utterances people speech disorders hearing impairments\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"Title: WESPER: Real-time Whisper-to-Normal Voice Conversion for Whisper-based Speech Interactions\\n\\nAbstract: \\nRecognizing whispered speech and converting it into normal speech can enable many possibilities for speech interactions. Whispered speech, due to its lower sound pressure compared to normal speech, can be used in semi-silent speech interactions in public places without audible disturbance to others. Converting whispers into normal speech also improves speech quality for people with speech hearing impairments. However, existing speech conversion techniques require speaker-dependent datasets consisting of pairs of whispered and normal speech utterances. To address these limitations, this paper proposes WESPER, a zero-shot real-time whisper-to-normal speech conversion mechanism based on self-supervised learning. WESPER consists of a speech-to-unit (STU) encoder that generates hidden speech units common to whispered and normal speech, and a unit-to-speech (UTS) decoder that reconstructs speech from the encoded speech units. Unlike existing methods, WESPER does not require paired datasets and can perform conversion for any user. Experimental results demonstrate the improved speech quality and preservation of natural prosody achieved by WESPER. Additionally, evaluations of speech reconstruction for individuals with speech disorders and hearing impairments show promising results.\\n\\n1. Introduction\\nVoice interaction systems are widely used but can be socially unacceptable or compromise confidentiality in public places. Silent speech input techniques have been developed but often require special sensors and are limited in vocabulary. Whispered speech, which preserves social acceptability in public spaces, has not been widely adopted due to the use of regular microphones. This paper proposes WESPER, a real-time whisper-to-normal voice conversion method based on self-supervised learning. WESPER aims to provide speaker-independent conversion of whispered speech to normal speech without the need for paired datasets.\\n\\n2. Related Work\\nThis section reviews research on silent and whispered speech recognition, as well as normal speech conversion techniques. It discusses various methods developed for silent and whispered speech recognition, including sensor-based approaches and generative adversarial networks. It also explores recent advances in self-supervised learning for speech representation and text-free speech processing methods.\\n\\n3. The WESPER Voice Conversion Model\\nThis section presents the architecture of the WESPER model, which consists of a speech-to-unit (STU) encoder and a unit-to-speech (UTS) decoder. The STU encoder pretrains on unpaired whispered and normal speech to generate hidden speech units. The UTS decoder reconstructs speech from the encoded speech units. The model is trained using a combination of labeled and unlabeled datasets.\\n\\n4. System Configuration\\nThe paper describes the system configuration of WESPER, including the use of the PyTorch framework and the datasets used for training and evaluation. It provides details on the training process, including the pretraining of the STU encoder and the finetuning of the UTS decoder.\\n\\n5. Evaluation\\nThis section presents the evaluation results of WESPER in terms of speech quality and speech reconstruction for individuals with speech disorders and hearing impairments. The results demonstrate the improved quality of converted speech and the preservation of natural prosody. It also compares the recognition accuracy of whispered speech before and after conversion using WESPER.\\n\\n6. Discussions\\nThe paper discusses the potential combination of WESPER with user-dependent finetuning to improve conversion performance. It also explores the use of different audio input devices suitable for whispered speech recognition. The concept of human-AI integration is also discussed, highlighting the synergistic effects that can be achieved by combining machine learning with human capabilities.\\n\\n7. Conclusion\\nThe paper concludes by summarizing the contributions of WESPER in enabling real-time whisper-to-normal voice conversion without speaker dependence. It highlights the potential applications of WESPER in speech interactions and its effectiveness in speech reconstruction for individuals with speech disorders and hearing impairments.\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1691127173,\n",
      "  \"id\": \"chatcmpl-7jhifaP98bNAkfevnwy3OTmP9SELz\",\n",
      "  \"model\": \"gpt-3.5-turbo-16k-0613\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 776,\n",
      "    \"prompt_tokens\": 7048,\n",
      "    \"total_tokens\": 7824\n",
      "  }\n",
      "}\n",
      "Title: WESPER: Real-time Whisper-to-Normal Voice Conversion for Whisper-based Speech Interactions\n",
      "\n",
      "Abstract: \n",
      "Recognizing whispered speech and converting it into normal speech can enable many possibilities for speech interactions. Whispered speech, due to its lower sound pressure compared to normal speech, can be used in semi-silent speech interactions in public places without audible disturbance to others. Converting whispers into normal speech also improves speech quality for people with speech hearing impairments. However, existing speech conversion techniques require speaker-dependent datasets consisting of pairs of whispered and normal speech utterances. To address these limitations, this paper proposes WESPER, a zero-shot real-time whisper-to-normal speech conversion mechanism based on self-supervised learning. WESPER consists of a speech-to-unit (STU) encoder that generates hidden speech units common to whispered and normal speech, and a unit-to-speech (UTS) decoder that reconstructs speech from the encoded speech units. Unlike existing methods, WESPER does not require paired datasets and can perform conversion for any user. Experimental results demonstrate the improved speech quality and preservation of natural prosody achieved by WESPER. Additionally, evaluations of speech reconstruction for individuals with speech disorders and hearing impairments show promising results.\n",
      "\n",
      "1. Introduction\n",
      "Voice interaction systems are widely used but can be socially unacceptable or compromise confidentiality in public places. Silent speech input techniques have been developed but often require special sensors and are limited in vocabulary. Whispered speech, which preserves social acceptability in public spaces, has not been widely adopted due to the use of regular microphones. This paper proposes WESPER, a real-time whisper-to-normal voice conversion method based on self-supervised learning. WESPER aims to provide speaker-independent conversion of whispered speech to normal speech without the need for paired datasets.\n",
      "\n",
      "2. Related Work\n",
      "This section reviews research on silent and whispered speech recognition, as well as normal speech conversion techniques. It discusses various methods developed for silent and whispered speech recognition, including sensor-based approaches and generative adversarial networks. It also explores recent advances in self-supervised learning for speech representation and text-free speech processing methods.\n",
      "\n",
      "3. The WESPER Voice Conversion Model\n",
      "This section presents the architecture of the WESPER model, which consists of a speech-to-unit (STU) encoder and a unit-to-speech (UTS) decoder. The STU encoder pretrains on unpaired whispered and normal speech to generate hidden speech units. The UTS decoder reconstructs speech from the encoded speech units. The model is trained using a combination of labeled and unlabeled datasets.\n",
      "\n",
      "4. System Configuration\n",
      "The paper describes the system configuration of WESPER, including the use of the PyTorch framework and the datasets used for training and evaluation. It provides details on the training process, including the pretraining of the STU encoder and the finetuning of the UTS decoder.\n",
      "\n",
      "5. Evaluation\n",
      "This section presents the evaluation results of WESPER in terms of speech quality and speech reconstruction for individuals with speech disorders and hearing impairments. The results demonstrate the improved quality of converted speech and the preservation of natural prosody. It also compares the recognition accuracy of whispered speech before and after conversion using WESPER.\n",
      "\n",
      "6. Discussions\n",
      "The paper discusses the potential combination of WESPER with user-dependent finetuning to improve conversion performance. It also explores the use of different audio input devices suitable for whispered speech recognition. The concept of human-AI integration is also discussed, highlighting the synergistic effects that can be achieved by combining machine learning with human capabilities.\n",
      "\n",
      "7. Conclusion\n",
      "The paper concludes by summarizing the contributions of WESPER in enabling real-time whisper-to-normal voice conversion without speaker dependence. It highlights the potential applications of WESPER in speech interactions and its effectiveness in speech reconstruction for individuals with speech disorders and hearing impairments.\n",
      "592\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "\n",
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "    \"title\": {\"type\": \"string\"},\n",
    "    \"author\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "    \"jornal/conference\": {\"type\": \"string\"},\n",
    "    \"year\": {\"type\": \"string\"},\n",
    "    \"abstract\": {\"type\": \"string\"},\n",
    "    \"CCS_concept\": {\"type\": \"string\"},\n",
    "    \"bib\": {\"type\": \"string\"},\n",
    "    \"metadata\": {\"type\": \"object\"},  # „Åì„Åì„Å´„Åï„Çâ„Å™„Çã„É°„Çø„Éá„Éº„Çø„ÅåÂÖ•„Çä„Åæ„Åô\n",
    "    \"keywords\": {\"type\": \"string\"},\n",
    "    \"problem\": {\"type\": \"string\"},\n",
    "    \"method\": {\"type\": \"string\"},\n",
    "    \"interaction\": {\"type\": \"string\"},\n",
    "    \"technical_contribution\": {\"type\": \"string\"},\n",
    "    \"result\": {\"type\": \"string\"},\n",
    "    \"github\": {\"type\": \"string\"},\n",
    "    \"doi\": {\"type\": \"string\"}\n",
    "    },\n",
    "    \"required\": [\"title\", \"metadata\", \"keywords\", \"problem\", \"method\", \"interaction\", \"technical_contribution\", \"result\", \"github\", \"doi\", \"texts\"]\n",
    "}\n",
    "\n",
    "dir=\"./papers\"\n",
    "\n",
    "paths = [os.path.join(dir, f) for f in os.listdir(dir) if f.endswith(\".pdf\")]\n",
    "\n",
    "for pdf_filepath in paths:\n",
    "    print(f\"Processing {pdf_filepath} ...\")\n",
    "\n",
    "    # Extract text from the PDF file\n",
    "    text = extract_text(pdf_filepath)\n",
    "\n",
    "    # Remove unnecessary parts\n",
    "    text = remove_unnecessary_parts(text)\n",
    "\n",
    "    # Preprocess text\n",
    "    text = preprocess_text(text)\n",
    "\n",
    "    print(len(text.split()))\n",
    "\n",
    "    # If the text is too long, reduce it with GPT-3.5\n",
    "    gpt3_json = reduce_length_with_gpt3(text)\n",
    "\n",
    "\n",
    "    print(gpt3_json)\n",
    "\n",
    "    print(len(gpt3_json.split()))\n",
    "\n",
    "    # write json\n",
    "    #with open('gpt3.json', 'w') as f:\n",
    "    #    json.dump(gpt3_json, f, indent=4)\n",
    "    \n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./papers/2303.01639.pdf ...\n",
      "Completed processing ./papers/2303.01639.pdf. Result saved to ./papers/2303.01639.pdf_result.json\n",
      "Processing ./papers/3569474.pdf ...\n",
      "Completed processing ./papers/3569474.pdf. Result saved to ./papers/3569474.pdf_result.json\n",
      "Processing ./papers/3544548.3580801.pdf ...\n",
      "Completed processing ./papers/3544548.3580801.pdf. Result saved to ./papers/3544548.3580801.pdf_result.json\n",
      "Processing ./papers/3544549.3585876.pdf ...\n",
      "Completed processing ./papers/3544549.3585876.pdf. Result saved to ./papers/3544549.3585876.pdf_result.json\n",
      "Processing ./papers/3581641.3584072.pdf ...\n",
      "Completed processing ./papers/3581641.3584072.pdf. Result saved to ./papers/3581641.3584072.pdf_result.json\n",
      "Processing ./papers/3544548.3581264.pdf ...\n",
      "Completed processing ./papers/3544548.3581264.pdf. Result saved to ./papers/3544548.3581264.pdf_result.json\n",
      "Processing ./papers/3491102.3517686.pdf ...\n",
      "Completed processing ./papers/3491102.3517686.pdf. Result saved to ./papers/3491102.3517686.pdf_result.json\n",
      "Processing ./papers/3544548.3581055.pdf ...\n",
      "Completed processing ./papers/3544548.3581055.pdf. Result saved to ./papers/3544548.3581055.pdf_result.json\n",
      "Processing ./papers/3544548.3580770.pdf ...\n",
      "Completed processing ./papers/3544548.3580770.pdf. Result saved to ./papers/3544548.3580770.pdf_result.json\n",
      "Processing ./papers/3544548.3581269.pdf ...\n",
      "Completed processing ./papers/3544548.3581269.pdf. Result saved to ./papers/3544548.3581269.pdf_result.json\n",
      "Processing ./papers/3544548.3581279.pdf ...\n",
      "Completed processing ./papers/3544548.3581279.pdf. Result saved to ./papers/3544548.3581279.pdf_result.json\n",
      "Processing ./papers/3569491.pdf ...\n",
      "Completed processing ./papers/3569491.pdf. Result saved to ./papers/3569491.pdf_result.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "\n",
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "    \"title\": {\"type\": \"string\"},\n",
    "    \"author\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "    \"jornal/conference\": {\"type\": \"string\"},\n",
    "    \"year\": {\"type\": \"string\"},\n",
    "    \"abstract\": {\"type\": \"string\"},\n",
    "    \"CCS_concept\": {\"type\": \"string\"},\n",
    "    \"bib\": {\"type\": \"string\"},\n",
    "    \"metadata\": {\"type\": \"object\"},  # „Åì„Åì„Å´„Åï„Çâ„Å™„Çã„É°„Çø„Éá„Éº„Çø„ÅåÂÖ•„Çä„Åæ„Åô\n",
    "    \"keywords\": {\"type\": \"string\"},\n",
    "    \"problem\": {\"type\": \"string\"},\n",
    "    \"method\": {\"type\": \"string\"},\n",
    "    \"interaction\": {\"type\": \"string\"},\n",
    "    \"technical_contribution\": {\"type\": \"string\"},\n",
    "    \"result\": {\"type\": \"string\"},\n",
    "    \"github\": {\"type\": \"string\"},\n",
    "    \"doi\": {\"type\": \"string\"}\n",
    "    },\n",
    "    \"required\": [\"title\", \"metadata\", \"keywords\", \"problem\", \"method\", \"interaction\", \"technical_contribution\", \"result\", \"github\", \"doi\", \"texts\"]\n",
    "}\n",
    "\n",
    "\n",
    "def main(dir=\"./papers\"):\n",
    "    # Get all PDF files in the directory\n",
    "    paths = [os.path.join(dir, f) for f in os.listdir(dir) if f.endswith(\".pdf\")]\n",
    "\n",
    "    for pdf_filepath in paths:\n",
    "        print(f\"Processing {pdf_filepath} ...\")\n",
    "        \n",
    "        # Extract text from the PDF file\n",
    "        text = extract_text(pdf_filepath)\n",
    "\n",
    "        # Remove unnecessary parts\n",
    "        text = remove_unnecessary_parts(text)\n",
    "\n",
    "        # Preprocess text\n",
    "        text = preprocess_text(text)\n",
    "\n",
    "        # If the text is too long, reduce it with GPT-3.5\n",
    "        #if len(text.split()) > 8000:\n",
    "        #    text = reduce_length_with_gpt35(text)\n",
    "        \n",
    "        # Generate a summary and metadata with GPT-4\n",
    "        prompt = \"Analyze the following paper and provide the required information. Keep the metadata in the original language. Summarize each content item in Japanese, with each item summarized to a maximum of 180 characters.\"\n",
    "        completion = openai.ChatCompletion.create(\n",
    "            #model=\"gpt-4-0613\",\n",
    "            model = \"gpt-3.5-turbo-16k\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\":f\"{prompt}\"},\n",
    "                {\"role\": \"user\", \"content\": \"Analyze the following paper and provide the required information.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"The paper is: [{text}]\"}\n",
    "            ],\n",
    "            functions=[{\"name\": \"analyze_paper\", \"parameters\": schema}],\n",
    "            function_call={\"name\": \"analyze_paper\"},\n",
    "            temperature=0,\n",
    "        )\n",
    "\n",
    "        # Convert the generated metadata to a JSON object\n",
    "        try:\n",
    "            json_result = json.loads(completion.choices[0].message.function_call.arguments)\n",
    "        except:\n",
    "            print(\"Error parsing JSON result.\")\n",
    "            print(completion.choices[0].message.function_call.arguments)\n",
    "\n",
    "        # Add the original text as a field in the JSON object\n",
    "        json_result[\"text\"] = text\n",
    "\n",
    "        # Save the JSON object to a file\n",
    "        with open(f\"{pdf_filepath}_result.json\", \"w\") as f:\n",
    "            json.dump(json_result, f)\n",
    "\n",
    "        print(f\"Completed processing {pdf_filepath}. Result saved to {pdf_filepath}_result.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qa_maker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
