{"title": "WESPER: Zeroshot Realtime Whisper-to-Normal Voice Conversion for Whisper-based Speech Interactions", "author": ["Jun Rekimoto"], "jornal/conference": "CHI \u2019 23", "year": "2023", "abstract": "Recognizing whispered speech and converting it to normal speech creates many possibilities for speech interaction. Because the sound pressure of whispered speech is significantly lower than normal speech, it can be used for semi-silent speech interaction in public places without being audible to others. Converting whispers to normal speech also improves speech quality for people with speech and hearing impairments. However, conventional speech conversion techniques do not provide sufficient conversion quality and require speaker-dependent datasets consisting of pairs of whispered and normal speech utterances. To address these problems, we propose WESPER, a zero-shot, real-time whisper-to-normal speech conversion mechanism based on self-supervised learning. WESPER consists of a speech-to-unit (STU) encoder that generates hidden speech units common to whispered and normal speech, and a unit-to-speech (UTS) decoder that reconstructs speech from the encoded speech units. Unlike existing methods, our conversion is user-independent and does not require a paired dataset of whispered and normal speech.", "keywords": "speech interaction, whispered voice, whispered voice conversion, silent speech, artificial intelligence, neural networks, self-supervised learning", "problem": "Recognizing whispered speech and converting it to normal speech is a challenge due to the significantly lower sound pressure of whispered speech. Conventional speech conversion techniques do not provide sufficient conversion quality and require speaker-dependent datasets consisting of pairs of whispered and normal speech utterances.", "method": "The proposed solution, WESPER, is a zero-shot, real-time whisper-to-normal speech conversion mechanism based on self-supervised learning. It consists of a speech-to-unit (STU) encoder that generates hidden speech units common to whispered and normal speech, and a unit-to-speech (UTS) decoder that reconstructs speech from the encoded speech units.", "interaction": "The system operates in real-time and can be applied to teleconferencing, for example, where a conference participant can speak in a whispered voice that is converted in real time for others to listen to as speech played back in a normal voice.", "technical_contribution": "The technical contribution of this paper is the development of WESPER, a zero-shot, real-time whisper-to-normal speech conversion mechanism based on self-supervised learning. Unlike existing methods, the conversion is user-independent and does not require a paired dataset of whispered and normal speech.", "result": "The quality of speech converted from whisper improved while preserving natural prosody. The effectiveness of the proposed approach was confirmed through its performance in speech reconstruction for people with speech and hearing disabilities.", "github": "https://wesperproj.github.io", "doi": "https://doi.org/10.1145/3544548.3580706", "text": "WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions Jun Rekimoto The University Tokyo 731 Hongo Bunkyoku Tokyo Japan Sony Computer Science Laboratories Kyoto 131 Hontorocho Shimogyoku Kyotoshi Kyoto Japan rekimotoacmorg 3 2 0 2 r M 3 D S c 1 v 9 3 6 1 0 3 0 3 2 v X r Figure 1 WESPER realtime whispertonormal speech conversion mechanism consisting speechtounit STU en coder generates common speech units whispered normal utterances using selfsupervised pretraining unittospeech UTS decoder recovers speech speech units It achieves userindependent voice conversion real time ABSTRACT Recognizing whispered speech converting normal speech creates many possibilities speech interaction Because sound pressure whispered speech significantly lower normal speech used semisilent speech interaction public places without audible others Converting whispers normal speech also improves speech quality people speech hearing impairments However conventional speech con version techniques provide sufficient conversion quality require speakerdependent datasets consisting pairs whis pered normal speech utterances To address problems propose WESPER zeroshot realtime whispertonormal speech conversion mechanism based selfsupervised learning WESPER consists speechtounit STU encoder generates hidden speech units common whispered normal speech unittospeech UTS decoder reconstructs speech encoded speech units Unlike existing methods conversion userindependent require paired dataset whis pered normal speech The UTS decoder reconstruct speech target speaker \u2019 voice speech units requires unlabeled target speaker \u2019 speech data We confirmed quality speech converted whisper improved preserving natural prosody Additionally confirmed effectiveness proposed approach perform speech construction people speech hearing disabilities CCS CONCEPTS \u2022 Humancentered computing \u2192 Soundbased input output Interface design prototyping Mobile devices \u2022 Computing method ologies \u2192 Neural networks Permission make digital hard copies part work personal classroom use granted without fee provided copies made distributed profit commercial advantage copies bear notice full citation first page Copyrights components work owned others authors must honored Abstracting credit permitted To copy otherwise republish post servers redistribute lists requires prior specific permission andor fee Request permissions permissionsacmorg CHI \u2019 23 April 23\u201328 2023 Hamburg Germany \u00a9 2023 Copyright held ownerauthors Publication rights licensed ACM ACM ISBN 97814503942152304 1500 httpsdoiorg10114535445483580706 KEYWORDS speech interaction whispered voice whispered voice conversion silent speech artificial intelligence neural networks selfsupervised learning ACM Reference Format Jun Rekimoto 2023 WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions In Proceedings 2023 CHI Conference Human Factors Computing Systems CHI \u2019 23 Energy PredictorPitch PredictorWhisper Hoarse Normal VoicesTransformer LayersMelSpectrogramReconstructed Normal VoiceCNN EncoderTransformer LayerTransformer LayerTransformer LayerTransformer LayerCommon speech unitsbetween whisper normal voices Modified FastSpeech2SpeechtoUnit STU EncoderUnittoSpeech UTS DecoderModified HuBERT Vocoder HiFiGANMelSpectrogram Decoder256one per 20msSpeaker independentincluding persons speech hearing disabilitiesCTC Speech RecogRecognized text CHI \u2019 23 April 23\u201328 2023 Hamburg Germany JRekimoto April 23\u201328 2023 Hamburg Germany ACM New York NY USA 13 pages httpsdoiorg10114535445483580706 1 INTRODUCTION Although voice interaction systems widely deployed typically easy use presence people Using voice commands public places may socially unaccept able risk confidential information leaked In addition speaking conference call uncomfortable people vicinity compromise confidentiality conversation To overcome problems various silent speech input tech niques developed 3 31 32 50 51 54 however methods require special sensors achieved high accu racy speech recognition remaining instead level recog nizing predefined commands Conversion unconditioned silent speech normal vocal utterances also achieved Additionally silent speech could employed capture utter ances people speech hearing impairments However owing abovementioned limitations existing methods meet requirements practical accessibility aids In contrast silent speech focus whispered speech Whis pers sufficiently low sound pressure ensure confidentiality whispering almost equivalent silent speech Whispering captured ordinary microphone require special sensor configuration People speech disorders still speak whisper hoarse voice although vocal organs may injured extracted thus voices might recognized To address issues propose WESPER realtime zeroshot whispertonormal voice conversion method based selfsupervised learning It designed perform speaker inde pendent conversion whispered speech normal speech There need peruser training paired datasets whispered normal utterances required The proposed architec ture consists speechtounit STU encoder pretrained normal whispered speeches unittospeech UTS decoder generates target voice speech units Figure 1 By pretraining unpaired whisper normal voices STU trained reduce difference normal whispered utterances output common speech unit The UTS decoder trained speech data specific speaker without accompanying text labels If person \u2019 voice used conversion performed restore whispered voice person \u2019 normal voice even another person \u2019 voice Because encoder decoder operate nonautoregressive manner entire system operates realtime Therefore applied teleconferencing example conference participant speak whispered voice converted real time others listen speech played back normal voice Figure 2 shows melspectrogram examples whispertonormal voice conversion using proposed method More conversion results demonstrated accompanying video The contributions study summarized follows Figure 2 WESPER conversion result Left whispered speech Middle whispered speech converted WESPER Right Normal speech speaker tran scription \u2022 We propose realtime speakerindependent vocabularyfree whispertonormal speech conversion method trained unpaired whispers normal speech \u2022 The target voice learned voice samples specific speaker without text transcription \u2022 We experimentally confirmed improvement performance normal speakers dysarthric hearingimpaired speakers 2 RELATED WORK 21 Research Silent Whispered Speech Various studies silent speech developed methods rec ognize user \u2019 silent utterances silent commands using various sensor configurations including lip reading EMG Electromyogra phy ultrasound 3 31 32 50 51 54 However systems require special dataset perform training increases need special sensors prevents technologies ing widely used Hence systems offer limited vocabulary less 100 commands available typically around 30 Al though silent speech recognition sometimes identified possible approach help people dysphonia methods interpret unrestricted free speech owing vocabulary limitation Whispered speech similar characteristics silent speech preserving social acceptability public spaces However widely adopted given ordinary mi crophones used SilentVoice 15 speech interaction technique uses gressive speech utterance made inhaling The amplitude ingressive speech low considered variation silent speech It requires microphonelike device placed close mouth training required users speak correctly ingressive speech A custom corpus ingressive speech text transcriptions also required Research whisper voice recognition 6 11 14 18 previ ously conducted The Alexa smart speaker supports whisper mode 23 In mode user whispers Alexa Alexa responds whisper 10 DualVoice 48 also proposed method per form endtoend recognition whispered voices based self supervised speech recognition system based wav2vec20 2 HuBERT 21 They also proposed interaction technique WhisperWESPER convertedWhisperNormalsecsecsecHz WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions CHI \u2019 23 April 23\u201328 2023 Hamburg Germany model training data peruser dataset conversion normal voice whisper speech recognition silent speech ex3 31 32 50 51 54 special paireddataset dedicated sensors required Parotoron 4 paired whispernormal voice required DualVoice 48 SilentVoice 15 CycleGANVC 30 MSpeCNet 37 AGANW2SC 16 WESPER labeled unpaired whisper normal voice labeled silentvoice custom unpaired unlabeled whisper normal voice paired whispernormal voice paired whispernormal voice unpaired unlabeled whisper normal voice required required required required required required YES NO NO YES YES YES YES Table 1 Research silent whispered speech YES commands characters YES voice conversion YES YES NO NO NO YES voice conversion distinguish whispered normal utterances WESPER combined DualVoice selectively convert user \u2019 whispered voice 22 Speech Conversion Normal speech conversion technologies developed convert one voiced utterance another 19 29 30 43 46 however conversion quality methods remains unsatisfactory applied whispertonormal voice conversion Recently several machine learningbased whispertonormal voice conversion techniques investigated 41 To com pare techniques important consider quality conversion required characteristics dataset used training A paired whispernormal voice dataset ie dataset containing whispered normal versions utter ance labeled whispered voice dataset ie dataset containing whispered utterances accompanied text labels require significant amount effort prepare Conversely unpaired unlabeled whisper voice dataset used ie dataset containing whisper utterances without corresponding normal versions labels effort required prepare dataset low Attentionguided generative adversarial network whisper normal speech conversion AGANW2SC GANbased whisper normal speech conversion 16 It converts whispered voice represented melspectrogram corresponding normal speech \u2019 melspectrogram It based GAN attention map used conversion It requires paired whispernormal speech dataset MSpeCNet 37 autoencoderbased multi domain voice conversion supports whispertonormal conver sion It also requires paired whispernormal voice dataset Moreover whispered speech converted text using speech recognition generate normal speech using texttospeech methods 22 However approach requires labeled dataset whispered speech recognition prosodic information con tained whispered utterances lost intermediate text representation convey information Parrotron 4 speech conversion system designed im prove speech speakers dysplasia It based encoderdecoder model conforms texttospeech system Tacotron 58 however differs Tacotron input output data formatted melspectrograms It also quires paired source target speeches making construction required dataset relatively difficult Phonetic posteriorgrams PPGs 55 intermediate informa tion obtained automatic speech recognition PPGs used manytoone speaker conversion represent articulation spoken content speaker independently To knowledge effects PPGs whispered voice investigated Our method also uses intermediated speech units require textbased corpus case automatic speech recognition ASR PPGs In contrast proposed system requires samples un paired target source speech require accompa nying text transcriptions thus making datasets easy prepare independent target language The characteristics related technologies summarized Table 1 We also demonstrate whisper normal conversion exam ples including NMSEDiscoGAN 53 MSpeCNet 37 CycleGAN VC 30 AGANW2SC 16 WESPER httpswesperproj githubio 23 Self Supervised Representation Learning Speech Recently combining pretraining selfsupervised representa tion learning unlabeled speech data finetuning labeled speech data attracted attention These systems primarily CHI \u2019 23 April 23\u201328 2023 Hamburg Germany JRekimoto intended speech recognition applications also applied perform speaker language emotion recogni tion 44 59 In particular pretraining method hiddenunit BERT hu BERT 21 similar masked language model used bidirectional encoder representations transformers BERT 12 natural language processing It designed mask part input estimate corresponding expression features rest input With pretraining model learn acoustic properties input data characteristics speech For use ASR pretraining finetuning performed small amount audio data text transcriptions A projection layer connectionist temporal classification CTC layer 17 added generate text transcriptions audio waveforms As reported 2 59 selfsupervised ASR achieved speech recognition accuracy comparable conventional stateoftheart ASRs finetuning small amount labeled speech data Therefore architecture could suitable recogniz ing whispered voice recognition limited whispered speech corpora WESPER unique uses pretraining reduce dif ference latent vectors encoding whispered normal utterances 24 TextlessNLP Recently textfree speech processing speech conversion meth ods developed Through selfsupervised learning systems derive latent representations speech data accompanied text transcriptions TextlessNLP34 35 Au dioLM 5 use text transcriptions phoneme symbols speech processing systems use discrete units constructed selfsupervised learning Soft discrete unit another approach textless speech processing 57 Our proposed method also uses nondiscrete vectors latent speech representations obtained selfsupervised learning explicitly use text transcriptions phoneme symbols Our research unique demonstrate whispered normal speech represented similar speech units selfsupervised learning In addition method also designed work seamlessly speech generation system later stages 3 THE WESPER VOICE CONVERSION MODEL WESPER consists STU encoder UTS decoder The STU converts whispered normal speech common speech units The UTS converts common speech units melspectrograms reconstructed speech vocoder WESPER char acterized fact common speech units utterance similar although STU pretrained unpaired whispered normal speech trained paired set whispered normal utterances The details method used train model described Figure 3 Overview STU pretraining Unpaired whis pered normal speech used pretraining The trans former layer trained estimating discrete units masked input The projection layer transformer layers generates 256dimensional vectors one every 20 ms used common speech units 31 SpeechtoUnit STU Encoder The STU encoder takes audio waveform input outputs units It based HuBERT 21 selfsupervised neural network speech pretrains large amount unlabeled speech BERT 12like manner learns recover relevant parts partially masked speech features thereby acquires speech language model For purpose want generate speech units identical possible whispered normal speech To achieve pretrained STU mixture whispered normal speech utterances used normal English speech many speakers Librispeech 960h dataset 42 Librispeech speech data mechanically converted whispered voice us ing LPCbased audio conversion tool 60 Additionally used wTIMIT speech dataset normal whispered speech 36 speech length 58 h Figure 3 shows pretraining process STU detail Only unpaired whispered normal speech used pretraining The transformer layer trained estimating discrete units masked input Similar HuBERT discrete target units first generated kmeans clustering input speech data first stage kmeans clustering outputs transformer intermediate layer In experiment used 100 discrete units The projection layer transformer layers generates sequence 256dimensional vectors one per 20 ms used common speech units In STU 12 transformer layers placed front CNN feature extractor Figure 1 After pretraining comparing output layer confirmed 1 difference Transformer Layer 11Transformer Layer 12ProjectionLEN 768LEN 256Label Embedding256 KLEN KLENCrossEntropy LossCosinesimilarityTo Unit Speech CNN EncoderWhisper Normal voicesunpairedTransformer Layer 1MaskingAcoustic unit discoveryeg KMeans MFCC HuBERTintermediate layerCommon Speech UnitK Discrete unitsK100Predict hidden units masked locations25620ms WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions CHI \u2019 23 April 23\u201328 2023 Hamburg Germany In addition finetune STU transcriptattached wTIMIT Librispeech datasets adding CTC layer 17 48 therefore STU could used ASR recognize whis pered normal speeches 32 UTS Decoder The UTS decoder takes speech units generated STU input reconstructs target normal speech It based nonautoregressive texttospeech TTS system FastSpeech2 49 While original FastSpeech2 includes embedding layer takes text phoneme tokens transforms sequence vector embeddings eliminate layer UTS takes speech units direct input The original FastSpeech2 also includes duration estimator estimates duration phoneme token length regulator adjusts number internal vectors according estimated duration These parts also eliminated purpose STU generates speech units constant rate When learning original FastSpeech2 necessary pro vide duration phoneme corpus ground truth external tool Montreal Forced Aligner 38 Because limitation FastSpeech2 \u2019 learning languagedependent Conversely UTS require duration estimation UTS languageindependent The output UTS melspectrogram FastSpeech2 A vocoder HiFiGAN 33 converts actual speech waveform In regular TTS system target speech corresponding text labels needed training By contrast proposed UTS requires target speech text labels The target speech passed STU obtain sequence speech units associated speech waveform used train UTS Figure 6 In experiment UTS trained using speech data single speaker LJSpeech 27 data speakers taken narration data 4 SYSTEM CONFIGURATION We designed WESPER model using PyTorch framework The STU based HuBERT UTS based modified implementation FastSpeech2 PyTorch implementation 8 We used Librispeech wTIMIT include normal whispered speech pretraining With dual NVIDIA R6000 pretraining took 48 h UTS training took 26 h target voice The total processing time required perform conversion approximately 120th actual speech duration single NVIDIA R6000 110th Apple M1 Max CPU The actual quality conversion demonstrated attached video We designed two types interfaces The first operated pushtotalk style user first speaks whispered normal voice pressing button Thereafter ton released speech waveform recorded time sent speechconversion neural networks result immediately played back The detected nonaudio period input speech automatically converted speech segments without additional user input Figure 4 Comparison whisperednormal voice differ ences The comparison made using normal speech speech whispering transformations Above An STU pretrained normal whispered speeches produced fewer differences pretrained normal speech As transformer layer deepens difference tween two decreases Middle There difference melspectrogram decreased speech units Bottom UMAP 39 visualizations differences tween whisper normal voices melspectrogram speechunit \u2019 space speech unit values whispered normal voices de creased increasing layer depth 2 difference decreased STU pretrained whispered normal speech utterances compared pretraining normal speech Figure 4 Figure 4 bottom also shows UMAP 39 vi sualization whispered normal speech melspectrogram space common speech unit space The corresponding periods two speeches connected lines As shown figure differences reduced common speech unit space except long distances Although speech feature values whispered normal voices different assume addressed self supervised pretraining utterances similar linguistic standpoints represented similar units We speculate learning method extract common pronunciations normal whispered utterances similar selfsupervised pretraining methods extract common pronunciations utterances different speakers ASR system CNN Encoder012341112Output transformer intermediate layersSTU pretrained normal voicesSTUpretrained whispered normal voices121086420Mean vector differenceWhisper Normal voicesTransformer Layer 1Transformer Layer 2Transformer Layer 3Transformer Layer 4Transformer Layer 11Transformer Layer 12DifferencereductionNormalWhisperdiffMelspectrogramSpeech UnitsMelspectrogramSpeech unit CHI \u2019 23 April 23\u201328 2023 Hamburg Germany JRekimoto Figure 5 Comparison FastSpeech2 49 UTS decoders FastSpeech2 needs predict duration phoneme whereas UTS accepts common speech unit duration This eliminates duration predictor length regulator LR figure Phoneme embedding also eliminated common speech unit consist discrete tokens Figure 6 STU training 1 UTS training 2 The UTS learns decode speech units target speech using wave data target voice frozen STU No text la beled dataset required Notably WESPER trained labeled corpus pretrained normal whispered voices Figure 7 shows current WESPER speech input configurations We tested headset b directional microphone four ar rayed MEMS Microelectromechanical systems microphones 52 c mobile phone microphone popguard avoid whis pering pop noise soundproofing material reduce ambient noise 5 EVALUATION The WESPER mechanism allows whispertonormal conversion independent input speaker Here evaluate conversion Figure 7 WESPER speech input device headset b ar ray directional microphone c cell phone microphone popprotection soundproofing material quality three aspects considering whispertonormal conver sion voice reconstruction people speech hearing disorders 51 Quality WhispertoNormal Conversion To evaluate quality converted speech recruited 50 genderbalanced participants online using Prolific crowdsourc ing system 25 18 fluent English Each participant listened four sets normal speech whispered speech WESPERconverted whispered speech 12 voices total webbased user interface rated utterances 5point Mean Opinion Score MOS questionnaires examples voices included attached video We used LJSpeechtrained WESPER voice target voice transcription sentence voices avoid differ ences impressions based sentence content The results presented Figure 8 Figure 8 shows sults MOS evaluation The WESPERconverted voice showed score normal whispered voices It con firmed WESPER conversion improved MOS original whispered speech \ud835\udc5d 001 pairwise ttest Cohen \u2019 effect size 054 Figure 8 b shows responses question \u201c Is voice hoarse normal \u201d clear improvement voices converted WESPER \ud835\udc5d 001 Figure 8 c shows responses question \u201c Is voice using consistent artic ulation standard intonation prosody \u201d Here WESPER whisper showed almost equal results Notably WESPER affect naturalness prosody Considering speech con verted WESPER generated unittospeech module vocoder result indicates WESPER preserve natural prosody original speech Multiple Stimuli Hidden Reference Anchor MUSHRA eval uation In addition MOS test evaluated speech quality us ing Multiple Stimuli Hidden Reference Anchor MUSHRA MUSHRA method evaluating perceived quality audio defined ITUR Recommendation BS15343 56 MUSHRA uses anchor audio audios evaluator expected Energy PredictorPitch PredictorTransformer LayersMelSpectrogramb UnittoSpeech UTS DecoderMelSpectrogram DecoderEnergy PredictorPitch PredictorTransformer LayersMelSpectrogramMelSpectrogram DecoderPhoneme EmbeddingDuration PredictorLRCommon Speech Unita FastSpeech2PhonemePositional EncodingPositional EncodingSpeech Unit Unit Speechtarget voiceno text labelslossPredicted melspectrogramTarget melspectrogramSpeech unitMelspectrogramconversionpitch energyextractionPredictedPitch energylossfrozentrainingSpeech Unit Acoustic unit discovery systemeg KMeans MFCCHuBERTintermediate layerNormal voicesWhisper voicestrainingSelfsupervised learning masked token prediction1unpairedSTU trainingUTS training2abcpop shieldsoundproof material WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions CHI \u2019 23 April 23\u201328 2023 Hamburg Germany Figure 8 Quality whispertonormal conversion Mean Opinion Scores MOS normal whispered WESPER converted whispered voices b Hoarsenormal voice rating c natural prosody rating se standard error \ud835\udc5d 001 ttest ns significant model train finetune test WER CER BLEU Google trained Google wTIMITN wTIMITW WESPERwTIMITW HuBERT base wTIMITN Librispeech Librispeech wTIMITW librispeech wTIMITNW wTIMITW 1155 4470 2668 2106 3306 1375 466 2838 1270 817 1545 547 076 034 052 054 038 070 wTIMITN wTIMIT 36 normal voice Google Google Cloud SpeechtoText 24 wTIMITW wTIMIT whisper voice WESPER\u2022 WESPER converted \u2022 HuBERT HuBERT base model pretrained Librispeech 42 wTIMITNW Table 2 Whispered voice recognition accuracy Google Cloud SpeechtoText 24 reference ASR The recognition rate whispered voice normal ASR high however result converting whispered voice normal voice using WESPER recognized recognition rate improved Notably WESPER trained labeled data pretrained unlabeled unpaired normal whispered voices assign ratings 0 100 audio comparing anchor audio reference For rating participants play voice many times like Because presence anchor hidden reference audio MUSHRA considered reliable MOS We recruited 50 genderbalanced Englishspeaking participants age 18 via Internet using Prolific crowdsourcing system 25 We used javascriptbased MUSHRA testing tool 28 online evaluation Figure 9 system available https githubcomrkmtmushraJSprolific We used whisper normal voice samples previous MOS test The participants \u2019 responses collected Internet The results presented Figure 10 It revalidates results MOS evaluation \ud835\udc5d 001 pairwise ttest effect size 057 According evaluations draw following con clusions \u2022 WESPER convert whispered voices normal voices \u2022 Speechdata converted WESPER better MOS whispered source voice Figure 9 An example MUSHRA evaluation web inter faces nsabc CHI \u2019 23 April 23\u201328 2023 Hamburg Germany JRekimoto Figure 10 Speech quality evaluation whispered WESPER converted voices MUSHRA MUltiple Stimuli Hidden Reference Anchor \ud835\udc5d 001 ttest \u2022 WESPER preserved natural prosody source whispered voice 52 Speech Recognition Accuracy There two possible ways use WESPER speech recognizer including speech recognition WESPER recognition speech converted WESPER speech recognizers In former method WESPER model based HuBERT pre trained whispered normal speech finetuned using whispered corpus text inferred whispered voice In latter case whispered speech converted WESPER con trol speechenabled device If whispered speech converted normal speech existing speechenabled devices used immediately without need modify whispered speech Using existing speech recognizer google cloud speechto text 24 reference measured speech recognition accu racy normal speech whispered speech whispered speech converted WESPER The wTIMIT corpus used measurements wTIMIT TIMITcompliant transcription containing normal whispered speech labels This corpus evaluated recogni tion accuracy whispered speech converted WESPER using recognition accuracy normal whispered speech baseline The results summarized Table 2 terms word error rate WER character error rate CER well bilingual eval uation understudy BLEU As shown table recognition accuracy high whispered speech directly rec ognized WER4470 accuracy improved WER2668 conversion WESPER When tested wTIMITW HuBERTbase pretrained Librispeech wTIMITNW shows better results google cloud speechtotext HuBERT WER3306 Google WER4470 It speculated effect pretraining mixture whis pered normal speech without finetuning whispered voice may contribute accuracy ASR Figure 11 Speech quality evaluation people speech disorders ranked 5point MOS \ud835\udc5d 001 \ud835\udc5d 005 S1S5 speakers VFP Vocal Fold Polyps SD Spasmodic Dys phonia Figure 12 Speech quality evaluation people vocal disabilities ranked MUSHRA \ud835\udc5d 001 S1S5 speakers VFP Vocal Fold Polyps SD Spasmodic Dysphonia Notably WESPER trained labeled corpus It pre trained whispered normal speeches improves speech recognition accuracy Therefore speech recognition via WESPER conversion require preparation corpus whispered speech specific unspecified speakers 53 Evaluations Speech Reconstruction People Speech Disorders An important goal WESPER reconstruction atypical speech people speech disorders hearing impairments This makes speech understandable people familiar individual speech patterns Dysphonia involuntary hoarse breathy strained sound ing voice low volume pitch There various causes including spasms polyps vocal tract causes If vocal cords removed throat cancer causes voice becomes extremely difficult produce For hearing impair ment even vocal cords affected control vocal cords becomes difficult resulting dysphonia Electrolarynx used mechanically vibrate throat vocalization ns WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions CHI \u2019 23 April 23\u201328 2023 Hamburg Germany fluent German addition English reference sentence used German The results presented Figures 11 12 13 14 Figure 11 shows results terms MOS For SD VFP WESPER converted voices higher MOS scores \ud835\udc5d 001 effect size066 MUSHRA scores \ud835\udc5d 001 effect size079 Figure 13 shows responses question \u201c Is voice hoarse normal \u201d clear improvement WESPERconverted voices compared original VFP SD voices \ud835\udc5d 001 Figure 14 shows answers question \u201c Does voice use consistent articulation standard intonation prosody \u201d Here WESPER converted original voices showed nearly equal scores although WESPERconverted voices showed slightly better scores It assumed WESPER affect prosody original speech According evaluations make following con clusions \u2022 WESPERconverted voices people VFP SD speech disabilities showed better quality This suggests WESPER improve quality speech people con ditions terms intelligibility people unfamiliar individual speech patterns \u2022 Similarly WESPER improve naturalness original VFP SD speech \u2022 WESPER could also preserve natural prosody source speech Notably test performed German sentences Although WESPER pretraining performed English speech German speech prosody source speech preserved MOSs improved Therefore result may demonstrate languageindependence ability WESPER model The attached video shows example whisper normal conversion Japanese Because wav2vec 20 base model STU also shown languageindependent pretraining perfor mance 9 languageindependent ability WESPER could feature worth evaluating future 54 Speech Reconstruction Evaluation People Hearing Impairment Finally evaluated effect speech reconstruction WES PER people hearing impairments People hearing loss hear speech others therefore tend difficulty speaking way easily understood general speakers However vocal organs mal speech different characteristics people dysphasia We used \u201c corpus deaf speech acoustic speech production research \u201d 40 Utterances five speakers one hearing speaker four hearing impaired used We recruited 50 evaluation participants using Prolific 25 evenly balanced terms gender fluent English speak ers age 18 We asked participants perform 5point ranking utterance terms MOS hoarsenormal natural prosody The results presented Figures 15 16 17 18 These results indicate MOS MUSHRA rating scores Figure 13 HoarseNormal assessments utterances peo ple speech disorders \ud835\udc5d 001 Figure 14 Speech prosody consistency utterances peo ple speech disorders ns significant people vocal cord damage sound produced artificial pitch conversion deterministic large devia tion normal vocalization The communication deficit caused dysphonia serious problem elimination voice conversion technology great social value To investigate quality improvement WESPER voice conversion evaluated speech utterances people two types speech disorders described Vocal Fold Polyps refer VFPs VFPs among common benign lesions larynx affecting quality voice production Spasmodic Dysphonia refer SD This also called laryngeal dystonia SD another common neurological disorder affects voice speech It lifelong condition causes spasms muscles produce voice We used Saarbruecken Voice Database SVD corpus 1 45 sample utterances people VFP SD The SVD commonly used corpus voices people speech disorders It contains recordings vowel utterances recording Ger man reference sentence \u201c Guten Morgen wie geht es Ihnen \u201d \u201c Good morning \u201d This sentence used evaluation Fifty participants recruited evaluation using Prolific 25 The recruited participants evenly balanced terms gender age 18 The participants nsnsnsnsnsns CHI \u2019 23 April 23\u201328 2023 Hamburg Germany JRekimoto Figure 15 Speech quality evaluation people hear ing impairment Speech quality ranked 5point MOS S1 \u2019 S4 \u2019 speakers \ud835\udc5d 001 \ud835\udc5d 005 Figure 18 Speech natural prosody consistency people hearing impairment \ud835\udc5d 005 The results experiments summarized Table 3 6 DISCUSSIONS Combination WESPER UserDependent Finetuning The sults evaluation experiments indicated degree improvement speech people hearing impairments less speech people dysphasia There fore assume former significant prosodic varia tions Although primary purpose study achieve speakerindependent speech conversion pretraining alone plan conduct additional experiments investigate whether conversion performance improved applying smallscale finetuning speaker Even case believe speech pairs necessary text transcription required A pair whispered hoarse speech utterances normal speech converted com mon speech unit STU used instead text transcription case ASR finetuning Audio Input Device Suitable Whispered Speech As demonstrated study whispered hoarse voices converted normal voices In practice selection appropriate audio input device would important We currently testing proposed approach normal headsets directional array microphone designed smart speakers obtained good results For wearable devices nonaudible murmur NAM mi crophone detects skin vibrations could used inaudible utterances 20 Combining noise reduction techniques 7 61 another important future direction Philips Dyson also developing masks provide pow ered respiratory ventilation protect air pollution infectious diseases 13 26 A microphone placed inside masks pick whispered voice would give effect almost equivalent silent speech HumanAI Integration This research concerns machinelearning techniques converting whispered speech unspecified speakers normal speech In practice however established users noticed similar whispered utterances easily converted normal speech whereas others difficult They tried speak relying machine learning user Figure 16 Speech quality evaluation people hear ing impairment Speech quality ranked MUSHRA S1 \u2019 S4 \u2019 speakers \ud835\udc5d 005 Figure 17 HoarseNormal assessments utterances peo ple hearing impairment \ud835\udc5d 001 \ud835\udc5d 005 higher converted WESPER \ud835\udc5d 005 effect size 045 MOS \ud835\udc5d 005 effect size 021 MUSHRA degree improvement less speakers voice disorders In addition prosody ratings significantly lower speech hearing impaired compared speech hearing speakers Therefore results indicate people hearing loss may difficulty controlling prosody speaking nsnsnsns WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions CHI \u2019 23 April 23\u201328 2023 Hamburg Germany Speaker Type Normal Whisper WESPERWhisper MOS MUSHRA Q1 416 9001 415 290 5133 327 397 6364 379 VFP SD S1 WESPERS1 S2 WESPERS2 S3 WESPERS3 S4 WESPERS4 S5 WESPERS4 Speakers speech disorders 1913 3572 3666 5753 4219 6274 2751 4445 4986 6553 3520 5330 272 342 308 396 320 412 224 288 354 394 293 364 All WESPERAll 154 378 220 416 200 436 120 336 268 390 193 389 Q2 399 331 343 300 324 340 376 326 360 236 220 354 364 312 329 Normal S1 \u2019 WESPERS1 \u2019 S2 \u2019 WESPERS2 \u2019 S3 \u2019 WESPERS3 \u2019 S4 \u2019 WESPERS4 \u2019 All WESPERAll Speakers hearing impaired 9175 4305 4747 2076 2005 2830 3507 3586 4123 3207 3621 368 230 261 161 193 203 239 238 306 208 250 Mean Opinion Score MUltiple Stimuli Hidden Reference Anchor \u2018 Is voice hoarse normal \u2019 375 251 297 184 255 222 293 259 335 212 236 376 229 238 174 206 219 239 226 262 228 294 \u2018 Is voice use consistent articulation standard intonation prosody \u2019 MOS MUSHRA Q1 Q2 WESPER\u2022 WESPER converted \u2022 VFP SD S1S5 S1 \u2019 S4 \u2019 Vocal Fold Polyps speakers Spasmodic Dysphonia speakers speaker IDs Table 3 Summary Voice Conversion Quality Evaluation MUSHRA 100point score others5point score side This interaction suggests machine learning unilaterally extend human capabilities synergistic effects achieved learning human side well This seen actual example humanAI integration 47 vocalization 7 CONCLUSION In study proposed WESPER mechanism real time conversion whispered speech normal speech We con firmed common speech unit obtained selfsupervised learning even acoustic features whispered mal speech different Speech reproduction learned speech data arbitrary target speakers without using text labels There need train user need parallel data whispered normal speech From speech units WESPER reconstruct utterances target speaker requires unlabeled speech data target speakers We con firmed quality converted speech improved prosody speech preserved Additionally reported evaluation results reconstructing speech utterances people speech disorders hearing impairments"}