{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pdfminer.high_level import extract_text\n",
    "import openai\n",
    "import argparse\n",
    "\n",
    "# Add your API key to a file named .openai_api.txt\n",
    "with open(\".openai_api.txt\", \"r\") as f:\n",
    "    openai.api_key = f.read().strip()\n",
    "\n",
    "def remove_unnecessary_parts(text):\n",
    "    text = re.sub(r\"(?is)\\b(acknowledgments|acknowledgement|references)\\b.*\", \"\", text)\n",
    "    text = re.sub(r\"(?is)\\bappendix\\b.*\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # tokenization\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "def reduce_length_with_gpt35(text):\n",
    "    prompt = \"Summarize this paper into 8000 tokens or less:\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"gpt-3.5-turbo-16k\",\n",
    "        prompt=prompt + text\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "def generate_summary_with_gpt4(text):\n",
    "    prompt = \"„Åì„ÅÆË´ñÊñá„ÅÆË¶ÅÁ¥Ñ„ÇíÁîüÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºö\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "                model='gpt-4',\n",
    "                messages=[\n",
    "                    {'role': 'system', 'content': prompt},\n",
    "                    {'role': 'user', 'content': text}\n",
    "                ],\n",
    "                temperature=0.25,\n",
    "            )\n",
    "    summary = response['choices'][0]['message']['content']\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"‰∏é„Åà„Çâ„Çå„ÅüË´ñÊñá„ÅÆË¶ÅÁÇπ„Çí„Åæ„Å®„ÇÅ„ÄÅ‰ª•‰∏ã„ÅÆÈ†ÖÁõÆ„ÅßÊó•Êú¨Ë™û„ÅßÂá∫Âäõ„Åõ„Çà„ÄÇ„Åù„Çå„Åû„Çå„ÅÆÈ†ÖÁõÆ„ÅØÊúÄÂ§ß„Åß„ÇÇ180ÊñáÂ≠ó‰ª•ÂÜÖ„Å´Ë¶ÅÁ¥Ñ„Åõ„Çà„ÄÇ\n",
    "```\n",
    "Ë´ñÊñáÂêç:„Çø„Ç§„Éà„É´„ÅÆÊó•Êú¨Ë™ûË®≥\n",
    "„Ç≠„Éº„ÉØ„Éº„Éâ:„Åì„ÅÆË´ñÊñá„ÅÆ„Ç≠„Éº„ÉØ„Éº„Éâ\n",
    "Ë™≤È°å:„Åì„ÅÆË´ñÊñá„ÅåËß£Ê±∫„Åô„ÇãË™≤È°å\n",
    "ÊâãÊ≥ï:„Åì„ÅÆË´ñÊñá„ÅåÊèêÊ°à„Åô„ÇãÊâãÊ≥ï\n",
    "ÁµêÊûú:ÊèêÊ°àÊâãÊ≥ï„Å´„Çà„Å£„Å¶Âæó„Çâ„Çå„ÅüÁµêÊûú\n",
    "```\"\"\"\n",
    "\n",
    "def get_summary(text):\n",
    "    print(\"### input text\", text)\n",
    "    #print(\"### input prompt\", prompt)\n",
    "    response = openai.ChatCompletion.create(\n",
    "                #model=\"gpt-3.5-turbo\",\n",
    "                model='gpt-4',\n",
    "                messages=[\n",
    "                    {'role': 'system', 'content': prompt},\n",
    "                    {'role': 'user', 'content': text}\n",
    "                ],\n",
    "                temperature=0.25,\n",
    "            )\n",
    "    summary = response['choices'][0]['message']['content']\n",
    "    print(\"#### GPT\", summary)\n",
    "    dict = {}    \n",
    "    for b in summary.split('\\n'):\n",
    "        print(\"****\", b)\n",
    "        if b.startswith(\"Ë´ñÊñáÂêç\"):\n",
    "            dict['title_jp'] = b[4:].lstrip()\n",
    "        if b.startswith(\"„Ç≠„Éº„ÉØ„Éº„Éâ\"):\n",
    "            dict['keywords'] = b[6:].lstrip()\n",
    "        if b.startswith(\"Ë™≤È°å\"):\n",
    "            dict['problem'] = b[3:].lstrip()\n",
    "        if b.startswith(\"ÊâãÊ≥ï\"):\n",
    "            dict['method'] = b[3:].lstrip()\n",
    "        if b.startswith(\"ÁµêÊûú\"):\n",
    "            dict['result'] = b[3:].lstrip()\n",
    "    print(\"Dict by ChatGPT\", dict)\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WESPER: Zero-shot and Realtime Whisper to Normal Voice\\nConversion for Whisper-based Speech Interactions\\n\\nJun Rekimoto\\nThe University of Tokyo\\n7-3-1, Hongo, Bunkyo-ku, Tokyo, Japan\\nSony Computer Science Laboratories, Kyoto\\n13-1 Hontoro-cho, Shimogyo-ku, Kyoto-shi, Kyoto, Japan\\nrekimoto@acm.org\\n\\n3\\n2\\n0\\n2\\n\\nr\\na\\n\\nM\\n3\\n\\n]\\n\\nD\\nS\\n.\\ns\\nc\\n[\\n\\n1\\nv\\n9\\n3\\n6\\n1\\n0\\n.\\n3\\n0\\n3\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\nFigure 1: WESPER is a real-time whisper-to-normal speech conversion mechanism consisting of a speech-to-unit (STU) en-\\ncoder that generates common speech units for whispered and normal utterances using self-supervised pre-training, and a\\nunit-to-speech (UTS) decoder that recovers speech from the speech units. It achieves user-independent voice conversion in\\nreal time.\\n\\nABSTRACT\\nRecognizing whispered speech and converting it to normal speech\\ncreates many possibilities for speech interaction. Because the sound\\npressure of whispered speech is significantly lower than that of\\nnormal speech, it can be used as a semi-silent speech interaction in\\npublic places without being audible to others. Converting whispers\\nto normal speech also improves the speech quality for people with\\nspeech or hearing impairments. However, conventional speech con-\\nversion techniques do not provide sufficient conversion quality\\nor require speaker-dependent datasets consisting of pairs of whis-\\npered and normal speech utterances. To address these problems, we\\npropose WESPER, a zero-shot, real-time whisper-to-normal speech\\nconversion mechanism based on self-supervised learning. WESPER\\nconsists of a speech-to-unit (STU) encoder, which generates hidden\\nspeech units common to both whispered and normal speech, and a\\n\\nunit-to-speech (UTS) decoder, which reconstructs speech from the\\nencoded speech units. Unlike the existing methods, this conversion\\nis user-independent and does not require a paired dataset for whis-\\npered and normal speech. The UTS decoder can reconstruct speech\\nin any target speaker‚Äôs voice from speech units, and it requires\\nonly an unlabeled target speaker‚Äôs speech data. We confirmed that\\nthe quality of the speech converted from a whisper was improved\\nwhile preserving its natural prosody. Additionally, we confirmed\\nthe effectiveness of the proposed approach to perform speech re-\\nconstruction for people with speech or hearing disabilities.\\n\\nCCS CONCEPTS\\n‚Ä¢ Human-centered computing ‚Üí Sound-based input / output;\\nInterface design prototyping; Mobile devices; ‚Ä¢ Computing method-\\nologies ‚Üí Neural networks.\\n\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for components of this work owned by others than the\\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\\nand/or a fee. Request permissions from permissions@acm.org.\\nCHI ‚Äô23, April 23‚Äì28, 2023, Hamburg, Germany\\n¬© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\nACM ISBN 978-1-4503-9421-5/23/04. . . $15.00\\nhttps://doi.org/10.1145/3544548.3580706\\n\\nKEYWORDS\\nspeech interaction, whispered voice, whispered voice conversion,\\nsilent speech, artificial intelligence, neural networks, self-supervised\\nlearning\\n\\nACM Reference Format:\\nJun Rekimoto. 2023. WESPER: Zero-shot and Realtime Whisper to Normal\\nVoice Conversion for Whisper-based Speech Interactions. In Proceedings of\\nthe 2023 CHI Conference on Human Factors in Computing Systems (CHI ‚Äô23),\\n\\nEnergy PredictorPitch PredictorWhisper / Hoarse / Normal VoicesTransformer LayersMel-SpectrogramReconstructed Normal VoiceCNN EncoderTransformer LayerTransformer LayerTransformer LayerTransformer LayerCommon speech unitsbetween whisper and normal voices (Modified FastSpeech2)Speech-to-Unit (STU) EncoderUnit-to-Speech (UTS) Decoder(Modified HuBERT) Vocoder (HiFi-GAN)Mel-Spectrogram Decoder256one per 20msSpeaker independent(including  persons with speech or hearing disabilities)CTC & Speech Recog.Recognized text  \\n \\n \\n \\n \\n \\n\\x0cCHI ‚Äô23, April 23‚Äì28, 2023, Hamburg, Germany\\n\\nJ.Rekimoto\\n\\nApril 23‚Äì28, 2023, Hamburg, Germany. ACM, New York, NY, USA, 13 pages.\\nhttps://doi.org/10.1145/3544548.3580706\\n\\n1 INTRODUCTION\\nAlthough voice interaction systems have been widely deployed,\\nthey are typically not easy to use in the presence of other people.\\nUsing voice commands in public places may be socially unaccept-\\nable, and there is a risk of confidential information being leaked. In\\naddition, speaking during a conference call can be uncomfortable\\nfor people in the vicinity and can compromise the confidentiality\\nof a conversation.\\n\\nTo overcome these problems, various silent speech input tech-\\nniques have been developed [3, 31, 32, 50, 51, 54]; however, these\\nmethods require special sensors and have not achieved high accu-\\nracy in speech recognition, remaining instead at the level of recog-\\nnizing predefined commands. Conversion of unconditioned silent\\nspeech into normal vocal utterances has also not been achieved.\\n\\nAdditionally, silent speech could be employed to capture utter-\\nances from people with speech or hearing impairments. However,\\nowing to the above-mentioned limitations, existing methods cannot\\nmeet the requirements of practical accessibility aids.\\n\\nIn contrast to silent speech, we focus on whispered speech. Whis-\\npers are sufficiently low in sound pressure to ensure confidentiality,\\nand whispering is almost equivalent to silent speech. Whispering\\ncan be captured with an ordinary microphone and does not require\\nany special sensor configuration. People with speech disorders can\\nstill speak in a whisper or with a hoarse voice, although their vocal\\norgans may have been injured or extracted; thus, their voices might\\nbe recognized.\\n\\nTo address these issues, we propose WESPER, a real-time and\\nzero-shot whisper-to-normal voice conversion method based on\\nself-supervised learning. It is designed to perform speaker inde-\\npendent conversion of whispered speech to normal speech. There\\nis no need for per-user training, and paired datasets of whispered\\nand normal utterances are not required. The proposed architec-\\nture consists of a speech-to-unit (STU) encoder that is pre-trained\\nwith normal and whispered speeches and a unit-to-speech (UTS)\\ndecoder that generates a target voice from speech units (Figure 1).\\nBy pre-training with (unpaired) whisper and normal voices, the\\nSTU can be trained to reduce the difference between normal and\\nwhispered utterances and output a common speech unit.\\n\\nThe UTS decoder can be trained from the speech data of a specific\\nspeaker (without accompanying text labels). If the person‚Äôs voice is\\nused, a conversion can be performed to restore the whispered voice\\nto the person‚Äôs normal voice, or even to another person‚Äôs voice.\\n\\nBecause the encoder and decoder operate in a non-autoregressive\\nmanner, the entire system operates in real-time. Therefore, when\\napplied to teleconferencing, for example, a conference participant\\ncan speak in a whispered voice, which is then converted in real\\ntime, and others can listen to the speech played back in a normal\\nvoice.\\n\\nFigure 2 shows mel-spectrogram examples of whisper-to-normal\\nvoice conversion using the proposed method. More conversion\\nresults are demonstrated in the accompanying video.\\n\\nThe contributions of this study can be summarized as follows.\\n\\nFigure 2: WESPER conversion result: Left: whispered speech;\\nMiddle: whispered speech converted by WESPER; Right:\\nNormal speech by the same speaker, with the same tran-\\nscription\\n\\n‚Ä¢ We propose a real-time, speaker-independent, vocabulary-free\\nwhisper-to-normal speech conversion method that can be trained\\nonly on unpaired whispers and normal speech.\\n\\n‚Ä¢ The target voice can be learned by voice samples of a specific\\n\\nspeaker without text transcription.\\n\\n‚Ä¢ We experimentally confirmed an improvement in performance\\nfor normal speakers and for dysarthric or hearing-impaired\\nspeakers.\\n\\n2 RELATED WORK\\n2.1 Research on Silent and Whispered Speech\\nVarious studies on silent speech have developed methods to rec-\\nognize a user‚Äôs silent utterances or silent commands using various\\nsensor configurations, including lip reading, EMG (Electromyogra-\\nphy), and ultrasound [3, 31, 32, 50, 51, 54]. However, these systems\\nrequire a special dataset to perform training, which increases the\\nneed for special sensors and prevents these technologies from be-\\ning widely used. Hence, these systems offer limited vocabulary\\n(less than 100 commands are available, typically around 30). Al-\\nthough silent speech recognition has sometimes been identified as\\na possible approach to help people with dysphonia, these methods\\ncannot interpret unrestricted free speech owing to the vocabulary\\nlimitation.\\n\\nWhispered speech has some similar characteristics to silent\\nspeech, such as preserving social acceptability in public spaces.\\nHowever, it can be more widely adopted, given that ordinary mi-\\ncrophones can be used.\\n\\nSilentVoice [15] is a speech interaction technique that uses in-\\ngressive speech, an utterance made while inhaling. The amplitude\\nof ingressive speech is low, and it can be considered a variation of\\nsilent speech. It requires a microphone-like device placed very close\\nto the mouth, and training is required for users to speak correctly\\nwith ingressive speech. A custom corpus of ingressive speech with\\ntext transcriptions is also required.\\n\\nResearch on whisper voice recognition [6, 11, 14, 18] has previ-\\nously been conducted. The Alexa smart speaker supports a whisper\\nmode [23]. In this mode, when a user whispers to Alexa, Alexa\\nresponds in a whisper [10].\\n\\nDualVoice [48] has also been proposed as a method to per-\\nform end-to-end recognition of whispered voices based on a self-\\nsupervised speech recognition system based on wav2vec2.0 [2]\\nand HuBERT [21]. They also proposed an interaction technique to\\n\\nWhisperWESPER convertedWhisperNormalsecsecsecHz\\x0cWESPER: Zero-shot and Realtime Whisper to Normal Voice Conversion for Whisper-based Speech Interactions\\n\\nCHI ‚Äô23, April 23‚Äì28, 2023, Hamburg, Germany\\n\\nmodel\\n\\ntraining data\\n\\nper-user dataset\\n\\nconversion to\\nnormal voice\\n\\n(whisper)\\nspeech recognition\\n\\nsilent speech\\nex.[3, 31, 32, 50, 51, 54]\\n\\nspecial paired-dataset for\\ndedicated sensors\\n\\nrequired\\n\\nParotoron [4]\\n\\npaired whisper-normal voice\\n\\nrequired\\n\\nDualVoice [48]\\n\\nSilentVoice [15]\\n\\nCycleGAN-VC [30]\\n\\nMSpeC-Net [37]\\n\\nAGAN-W2SC [16]\\n\\nWESPER (ours)\\n\\nlabeled, unpaired\\nwhisper and normal voice\\nlabeled\\nsilentvoice (custom)\\nunpaired, unlabeled\\nwhisper and normal voice\\npaired\\nwhisper-normal voice\\npaired\\nwhisper-normal voice\\nunpaired and unlabeled\\nwhisper and normal voice\\n\\nrequired\\n\\nrequired\\n\\nnot required\\n\\nrequired\\n\\nrequired\\n\\nnot required\\n\\nboth\\n\\nYES\\n\\nNO\\n\\nNO\\n\\nYES\\n\\nYES\\n\\nYES\\n\\nYES\\n\\nTable 1: Research on silent and whispered speech\\n\\nYES\\n(commands, or characters)\\nYES\\n(through voice conversion)\\n\\nYES\\n\\nYES\\n\\nNO\\n\\nNO\\n\\nNO\\n\\nYES\\n(through voice conversion)\\n\\ndistinguish between whispered and normal utterances. WESPER\\ncan be combined with DualVoice to selectively convert a user‚Äôs\\nwhispered voice.\\n\\n2.2 Speech Conversion\\nNormal speech conversion technologies have been developed to\\nconvert one voiced utterance to another [19, 29, 30, 43, 46]; however,\\nthe conversion quality of these methods remains unsatisfactory\\nwhen applied to whisper-to-normal voice conversion.\\n\\nRecently, several machine learning-based whisper-to-normal\\nvoice conversion techniques have been investigated [41]. To com-\\npare these techniques, it is important to consider the quality of\\nconversion and the required characteristics of the dataset used\\nfor training. A paired whisper-normal voice dataset (i.e. a dataset\\ncontaining both whispered and normal versions of the same utter-\\nance) or a labeled whispered voice dataset (i.e. a dataset containing\\nwhispered utterances accompanied by text labels) will require a\\nsignificant amount of effort to prepare. Conversely, if an un-paired\\nand un-labeled whisper voice dataset is used (i.e. a dataset containing\\nonly whisper utterances without corresponding normal versions\\nor labels), the effort required to prepare the dataset is low.\\n\\nAttention-guided generative adversarial network for whisper to\\nnormal speech conversion (AGAN-W2SC) is a GAN-based whisper\\nto normal speech conversion [16]. It converts a whispered voice\\nrepresented as mel-spectrogram into the corresponding normal\\nspeech‚Äôs mel-spectrogram. It is based on GAN, and the attention-\\nmap is used for the conversion. It requires a paired whisper-normal\\nspeech dataset. MSpeC-Net [37] is an autoencoder-based multi-\\ndomain voice conversion and supports whisper-to-normal conver-\\nsion. It also requires a paired whisper-normal voice dataset.\\n\\nMoreover, whispered speech can be converted to text using\\nspeech recognition and generate normal speech using text-to-speech\\nmethods [22]. However, this approach requires a labeled dataset\\n\\nfor whispered speech recognition, and prosodic information con-\\ntained in whispered utterances is lost because the intermediate text\\nrepresentation does not convey such information.\\n\\nParrotron [4] is a speech conversion system designed to im-\\nprove the speech of speakers with dysplasia. It is based on an\\nencoder-decoder model that conforms to the text-to-speech system,\\nTacotron [58]; however, it differs from Tacotron in that both the\\ninput and output data are formatted as mel-spectrograms. It also re-\\nquires paired source and target speeches, making the construction\\nof the required dataset relatively difficult.\\n\\nPhonetic posteriorgrams (PPGs) [55] are intermediate informa-\\ntion obtained from automatic speech recognition. PPGs can be\\nused for many-to-one speaker conversion because they represent\\nthe articulation of spoken content speaker independently. To our\\nknowledge, the effects of PPGs on whispered voice have not been\\ninvestigated. Our method also uses intermediated speech units, but\\nit does not require text-based corpus as in the case of automatic\\nspeech recognition (ASR) and PPGs.\\n\\nIn contrast, our proposed system requires only samples of un-\\npaired target and source speech, and it does not require accompa-\\nnying text transcriptions, thus making datasets easy to prepare and\\nindependent of the target language.\\n\\nThe characteristics of related technologies are summarized in\\nTable 1. We also demonstrate whisper to normal conversion exam-\\nples including NMSE-DiscoGAN [53], MSpeC-Net [37], CycleGAN-\\nVC [30], AGAN-W2SC [16], and WESPER at https://wesperproj.\\ngithub.io/.\\n\\n2.3 Self Supervised Representation Learning\\n\\nfor Speech\\n\\nRecently, combining pre-training with self-supervised representa-\\ntion learning on unlabeled speech data and fine-tuning on labeled\\nspeech data has attracted attention. These systems are primarily\\n\\n\\x0cCHI ‚Äô23, April 23‚Äì28, 2023, Hamburg, Germany\\n\\nJ.Rekimoto\\n\\nintended for speech recognition applications, but they have also\\nbeen applied to perform speaker, language, and emotion recogni-\\ntion [44, 59].\\n\\nIn particular, the pre-training method of hidden-unit BERT (hu-\\nBERT) [21] is similar to that of the masked language model used in\\nbidirectional encoder representations from transformers (BERT) [12]\\nin natural language processing. It is designed to mask part of the\\ninput and estimate the corresponding expression features from the\\nrest of the input. With this pre-training, the model can learn the\\nacoustic properties of the input data and the characteristics of the\\nspeech.\\n\\nFor use as ASR, after pre-training, fine-tuning is performed on\\nonly a small amount of the audio data with text transcriptions. A\\nprojection layer and a connectionist temporal classification (CTC)\\nlayer [17] has been added to generate text transcriptions from audio\\nwaveforms.\\n\\nAs reported in [2, 59], self-supervised ASR achieved a speech\\nrecognition accuracy comparable to conventional state-of-the-art\\nASRs with fine-tuning only on a small amount of labeled speech\\ndata. Therefore, this architecture could be suitable for recogniz-\\ning whispered voice recognition with limited whispered speech\\ncorpora.\\n\\nWESPER is unique in that it uses pre-training to reduce the dif-\\nference between the latent vectors encoding whispered and normal\\nutterances.\\n\\n2.4 Textless-NLP\\nRecently, text-free speech processing and speech conversion meth-\\nods have been developed. Through self-supervised learning, these\\nsystems derive latent representations from speech data that is not\\naccompanied by text transcriptions. Textless-NLP[34, 35] and Au-\\ndioLM [5] do not use text transcriptions or phoneme symbols in\\nspeech processing systems; they use discrete units constructed by\\nself-supervised learning. Soft discrete unit is another approach for\\ntextless speech processing [57].\\n\\nOur proposed method also uses non-discrete vectors as latent\\nspeech representations obtained from self-supervised learning and\\ndoes not explicitly use text transcriptions or phoneme symbols.\\nOur research is unique because we demonstrate that whispered and\\nnormal speech can be represented by similar speech units through\\nself-supervised learning. In addition, our method is also designed\\nto work seamlessly with a speech generation system in the later\\nstages.\\n\\n3 THE WESPER VOICE CONVERSION MODEL\\nWESPER consists of an STU encoder and a UTS decoder. The STU\\nconverts whispered or normal speech into common speech units.\\nThe UTS converts common speech units into mel-spectrograms\\nthat can be reconstructed as speech by a vocoder. WESPER is char-\\nacterized by the fact that the common speech units of the same\\nutterance can be similar, although the STU is only pretrained with\\n(un-paired) whispered and normal speech, and it is not trained with\\na paired set of whispered and normal utterances. The details of\\neach and the method used to train the model are described below.\\n\\nFigure 3: Overview of STU pre-training. Unpaired whis-\\npered or normal speech is used for pretraining. The trans-\\nformer layer is trained by estimating discrete units from the\\nmasked input. The projection layer after the transformer\\nlayers generates 256-dimensional vectors (one every 20 ms),\\nwhich are used as common speech units.\\n\\n3.1 Speech-to-Unit (STU) Encoder\\nThe STU encoder takes audio waveform as the input and outputs\\nunits. It is based on HuBERT [21], a self-supervised neural network\\nfor speech, which pre-trains a large amount of unlabeled speech in a\\nBERT [12]-like manner; it learns to recover the relevant parts from\\npartially masked speech features and thereby acquires a speech-\\nlanguage model.\\n\\nFor our purpose, we want to generate speech units that are as\\nidentical as possible between whispered and normal speech. To\\nachieve this, we pre-trained an STU with a mixture of whispered\\nand normal speech utterances; we used normal English speech with\\nmany speakers from the Librispeech 960h dataset [42]. Librispeech\\nspeech data were mechanically converted to a whispered voice us-\\ning an LPC-based audio conversion tool [60]. Additionally, we used\\nthe wTIMIT speech dataset of normal and whispered speech [36]\\nwith a speech length of 58 h.\\n\\nFigure 3 shows the pre-training process of the STU in detail. Only\\nthe unpaired whispered or normal speech is used for pre-training.\\nThe transformer layer is trained by estimating discrete units from\\nthe masked input. Similar to HuBERT, the discrete target units are\\nfirst generated by k-means clustering of the input speech data in\\nthe first stage, and then by k-means clustering of the outputs of\\nthe transformer intermediate layer. In our experiment, we used 100\\ndiscrete units. The projection layer after the transformer layers\\ngenerates a sequence of 256-dimensional vectors (one per 20 ms),\\nwhich are used as common speech units.\\n\\nIn the STU, 12 transformer layers are placed in front of the\\nCNN feature extractor (Figure 1). After pre-training, by comparing\\nthe output from each layer, we confirmed that (1) the difference\\n\\nTransformer Layer 11Transformer Layer 12Projection[LEN, 768][LEN, 256]Label Embedding[256, K][LEN, K][LEN]Cross-Entropy LossCosine-similarity(To Unit to Speech) CNN EncoderWhisper / Normal voices(un-paired)Transformer Layer 1MaskingAcoustic unit discovery(e.g., K-Means on MFCC or HuBERTintermediate layer)Common Speech UnitK Discrete units(K=100)Predict hidden units at masked locations25620ms\\x0cWESPER: Zero-shot and Realtime Whisper to Normal Voice Conversion for Whisper-based Speech Interactions\\n\\nCHI ‚Äô23, April 23‚Äì28, 2023, Hamburg, Germany\\n\\nIn addition, we can fine-tune the STU with (transcript-attached)\\nwTIMIT and Librispeech datasets by adding a CTC layer [17] as\\nin [48]; therefore, STU could be used as an ASR to recognize whis-\\npered and normal speeches.\\n\\n3.2 UTS Decoder\\nThe UTS decoder takes the speech units generated by the STU as the\\ninput and reconstructs the target (normal) speech. It is based on the\\nnon-autoregressive text-to-speech (TTS) system FastSpeech2 [49].\\nWhile the original FastSpeech2 includes an embedding layer that\\ntakes text or phoneme tokens and transforms them into a sequence\\nof vector embeddings, we can eliminate this layer because UTS\\ntakes speech units as direct input. The original FastSpeech2 also\\nincludes a duration estimator that estimates the duration of each\\nphoneme token and a length regulator that adjusts the number of\\ninternal vectors according to the estimated duration. These parts\\ncan also be eliminated for our purpose because the STU generates\\nspeech units at a constant rate.\\n\\nWhen learning the original FastSpeech2, it was necessary to pro-\\nvide the duration of each phoneme in the corpus as a ground truth\\nby an external tool such as Montreal Forced Aligner [38]. Because\\nof this limitation, FastSpeech2‚Äôs learning is language-dependent.\\nConversely, UTS does not require duration estimation, and UTS\\ncan be language-independent.\\n\\nThe output of UTS is a mel-spectrogram, as in FastSpeech2.\\nA vocoder (HiFi-GAN [33]) converts this into an actual speech\\nwaveform.\\n\\nIn a regular TTS system, the target speech and the corresponding\\ntext labels are needed for training. By contrast, the proposed UTS\\nrequires only the target speech and no text labels. The target speech\\nis passed through the STU to obtain a sequence of speech units\\nassociated with the speech waveform and used to train the UTS\\n(Figure 6). In our experiment, the UTS was trained using speech\\ndata from a single speaker of LJSpeech [27], and data from other\\nspeakers taken from narration data.\\n\\n4 SYSTEM CONFIGURATION\\nWe designed the WESPER model using the PyTorch framework.\\nThe STU is based on HuBERT, and the UTS is based on a modified\\nimplementation of FastSpeech2 PyTorch implementation by [8]. We\\nused Librispeech and wTIMIT (both of which include normal and\\nwhispered speech) for pre-training. With a dual NVIDIA R6000,\\npre-training took 48 h. UTS training took 26 h for each target voice.\\nThe total processing time required to perform the conversion\\nwas approximately 1/20th of the actual speech duration on a single\\nNVIDIA R6000 and 1/10th on an Apple M1 Max CPU. The actual\\nquality of the conversion is demonstrated in the attached video.\\n\\nWe designed two types of interfaces. The first operated in a\\npush-to-talk style, where the user first speaks in a whispered or\\nnormal voice while pressing a button. Thereafter, when the but-\\nton is released, the speech waveform recorded during that time is\\nsent to the speech-conversion neural networks, and the result is\\nimmediately played back. The other detected a non-audio period of\\ninput speech and automatically converted speech segments without\\nadditional user input.\\n\\nFigure 4: Comparison of whispered/normal voice differ-\\nences: The comparison is made using normal speech and\\nspeech with whispering transformations. (Above) An STU\\npre-trained with normal and whispered speeches produced\\nfewer differences than that pre-trained with normal speech\\nonly. As the transformer layer deepens, the difference be-\\ntween the two decreases. (Middle) There was a difference\\nin the mel-spectrogram, which decreased with the speech\\nunits. (Bottom) UMAP [39] visualizations of differences be-\\ntween whisper and normal voices in mel-spectrogram and\\nspeech-unit‚Äôs space.\\n\\nin speech unit values between whispered and normal voices de-\\ncreased with increasing layer depth; (2) the difference decreased\\nwhen the STU was pre-trained with both whispered and normal\\nspeech utterances compared to pre-training with normal speech\\nonly (Figure 4). Figure 4 (bottom) also shows the UMAP [39] vi-\\nsualization of whispered and normal speech in mel-spectrogram\\nspace and common speech unit space. The corresponding periods\\nof two speeches are connected by lines. As shown in the figure, the\\ndifferences are reduced in the common speech unit space (except\\nfor a few long distances).\\n\\nAlthough the speech feature values of whispered and normal\\nvoices are different, we assume this is addressed through the self-\\nsupervised pre-training so that utterances with similar linguistic\\nstandpoints are represented with similar units. We speculate that\\nour learning method can extract common pronunciations from both\\nnormal and whispered utterances, similar to how self-supervised\\npre-training methods can extract common pronunciations from the\\nutterances of different speakers in an ASR system.\\n\\nCNN Encoder012341112Output from transformer intermediate layersSTU pretrained with normal voicesSTUpretrained with whispered and normal voices121086420Mean vector differenceWhisper / Normal voicesTransformer Layer 1Transformer Layer 2Transformer Layer 3Transformer Layer 4Transformer Layer 11Transformer Layer 12DifferencereductionNormalWhisperdiffMel-spectrogramSpeech UnitsMel-spectrogramSpeech unit\\x0cCHI ‚Äô23, April 23‚Äì28, 2023, Hamburg, Germany\\n\\nJ.Rekimoto\\n\\nFigure 5: Comparison of FastSpeech2 [49] and UTS decoders:\\nFastSpeech2 needs to predict the duration of each phoneme,\\nwhereas UTS accepts a common speech unit with the same\\nduration. This eliminates the duration predictor and the\\nlength regulator (LR in the figure). Phoneme embedding can\\nalso be eliminated because the common speech unit does not\\nconsist of discrete tokens.\\n\\nFigure 6: STU training (1) and UTS training (2): The UTS\\nlearns to decode speech units to the target speech using only\\nwave data of the target voice with (frozen) STU. No text la-\\nbeled dataset is required. Notably, WESPER was not trained\\non a labeled corpus, but only pre-trained on normal and\\nwhispered voices.\\n\\nFigure 7 shows the current WESPER speech input configurations.\\nWe tested with (a) a headset, (b) directional microphone with four ar-\\nrayed MEMS (Microelectromechanical systems) microphones [52],\\nand (c) mobile phone microphone with pop-guard to avoid whis-\\npering pop noise and soundproofing material to reduce ambient\\nnoise.\\n\\n5 EVALUATION\\nThe WESPER mechanism allows whisper-to-normal conversion\\nindependent of the input speaker. Here, we evaluate the conversion\\n\\nFigure 7: WESPER speech input device: (a) headset, (b) ar-\\nray (directional) microphone, and (c) cell phone microphone\\nwith pop-protection and soundproofing material.\\n\\nquality in three aspects, considering whisper-to-normal conver-\\nsion and voice reconstruction for people with speech and hearing\\ndisorders.\\n\\n5.1 Quality of Whisper-to-Normal Conversion\\nTo evaluate the quality of the converted speech, we recruited 50\\ngender-balanced participants online using the Prolific crowdsourc-\\ning system [25], all of whom were over 18 and fluent in English.\\nEach participant listened to four sets of normal speech, whispered\\nspeech, and WESPER-converted whispered speech (12 voices in\\ntotal) with a web-based user interface and rated the utterances\\non a 5-point Mean Opinion Score (MOS) and other questionnaires\\n(examples of voices are included in an attached video).\\n\\nWe used the LJSpeech-trained WESPER voice as the target voice\\nand the same transcription sentence for all voices to avoid differ-\\nences in impressions based on sentence content.\\n\\nThe results are presented in Figure 8. Figure 8 (a) shows the re-\\nsults of the MOS evaluation. The WESPER-converted voice showed\\na score between that of normal and whispered voices. It was con-\\nfirmed that WESPER conversion improved the MOS of the original\\nwhispered speech (ùëù < 0.01 by pairwise t-test, Cohen‚Äôs d effect\\nsize = 0.54). Figure 8 (b) shows the responses to the question ‚ÄúIs\\nthis voice hoarse or normal?‚Äù and there is a clear improvement in\\nthe voices converted with WESPER (ùëù < 0.01). Figure 8 (c) shows\\nthe responses to the question ‚ÄúIs the voice using consistent artic-\\nulation, standard intonation, and prosody?‚Äù Here, WESPER and\\nwhisper showed almost equal results. Notably, WESPER did not\\naffect the naturalness of the prosody. Considering the speech con-\\nverted by WESPER is generated by the unit-to-speech module and\\nthe vocoder, this result indicates that WESPER can preserve the\\nnatural prosody of the original speech.\\n\\nMultiple Stimuli with Hidden Reference and Anchor (MUSHRA) eval-\\nuation. In addition to the MOS test, we evaluated speech quality us-\\ning Multiple Stimuli with Hidden Reference and Anchor (MUSHRA).\\nMUSHRA is a method for evaluating the perceived quality of audio\\nas defined by ITU-R Recommendation BS.1534-3 [56]. MUSHRA\\nuses anchor audio and other audios, and an evaluator is expected\\n\\nEnergy PredictorPitch PredictorTransformer LayersMel-Spectrogram(b) Unit-to-Speech (UTS) DecoderMel-Spectrogram DecoderEnergy PredictorPitch PredictorTransformer LayersMel-SpectrogramMel-Spectrogram DecoderPhoneme EmbeddingDuration PredictorLRCommon Speech Unit(a) FastSpeech2PhonemePositional EncodingPositional EncodingSpeech to Unit Unit to Speechtarget voice(no text labels)lossPredicted mel-spectrogramTarget mel-spectrogramSpeech unitMel-spectrogramconversionpitch & energyextractionPredictedPitch & energyloss(frozen)(training)Speech to Unit Acoustic unit discovery system(e.g., K-Means on MFCC/HuBERTintermediate layer)Normal voicesWhisper voices(training)Self-supervised learning (masked token prediction)1(un-paired)STU trainingUTS training2abcpop shieldsoundproof material\\x0cWESPER: Zero-shot and Realtime Whisper to Normal Voice Conversion for Whisper-based Speech Interactions\\n\\nCHI ‚Äô23, April 23‚Äì28, 2023, Hamburg, Germany\\n\\nFigure 8: Quality of whisper-to-normal conversion: (a) Mean Opinion Scores (MOS) of normal, whispered, and WESPER-\\nconverted whispered voices. (b) Hoarse-normal voice rating, and (c) natural prosody rating (s.e.: standard error, *: ùëù < 0.01 by\\nt-test, n.s.: not significant)\\n\\nmodel\\n\\ntrain (finetune)\\n\\ntest\\n\\nWER (%) CER (%) BLEU\\n\\nGoogle\\n\\n(trained by Google)\\n\\nwTIMIT(N)\\nwTIMIT(W)\\nWESPER[wTIMIT(W)]\\n\\nHuBERT\\nbase\\n\\nwTIMIT(N)\\nLibrispeech\\nLibrispeech\\nwTIMIT(W)\\nlibrispeech + wTIMIT(N,W) wTIMIT(W)\\n\\n11.55\\n44.70\\n26.68\\n\\n21.06\\n33.06\\n13.75\\n\\n4.66\\n28.38\\n12.70\\n\\n8.17\\n15.45\\n5.47\\n\\n0.76\\n0.34\\n0.52\\n\\n0.54\\n0.38\\n0.70\\n\\nwTIMIT(N): wTIMIT [36] normal voice\\n\\nGoogle: Google Cloud Speech-to-Text [24]\\n\\nwTIMIT(W): wTIMIT whisper voice\\nWESPER[‚Ä¢]: WESPER converted ‚Ä¢\\n\\nHuBERT: HuBERT base model, pretrained with Librispeech [42] + wTIMIT(N,W)\\n\\nTable 2: Whispered voice recognition accuracy of Google Cloud Speech-to-Text [24] as a reference ASR: The recognition rate\\nof whispered voice in normal ASR was not high; however, when the result of converting whispered voice to normal voice\\nusing WESPER was recognized, the recognition rate improved. Notably, WESPER was not trained on labeled data, but it was\\npre-trained on (un-labeled and unpaired) normal and whispered voices.\\n\\nto assign ratings (from 0 to 100) to the audio by comparing it to the\\nanchor audio as the reference. For each rating, participants can play\\neach voice as many times as they like. Because of the presence of\\nanchor and hidden reference audio, MUSHRA is considered more\\nreliable than MOS.\\n\\nWe recruited 50 gender-balanced, English-speaking participants\\nover the age of 18 via the Internet using the Prolific crowdsourcing\\nsystem [25]. We used a javascript-based MUSHRA testing tool [28]\\nfor online evaluation (Figure 9, the system is available from https:\\n//github.com/rkmt/mushraJS_prolific ). We used the same whisper-\\nnormal voice samples as in the previous MOS test. The participants‚Äô\\nresponses are collected over the Internet.\\n\\nThe results are presented in Figure 10. It revalidates the results\\nof the MOS evaluation (ùëù < 0.01 through pairwise t-test, effect size\\n= 0.57).\\n\\nAccording to these evaluations, we can draw the following con-\\n\\nclusions:\\n\\n‚Ä¢ WESPER can convert whispered voices to normal voices.\\n‚Ä¢ Speechdata converted with WESPER had a better MOS than the\\n\\nwhispered source voice.\\n\\nFigure 9: An example of MUSHRA evaluation web inter-\\nfaces.\\n\\n**n.s.(a)(b)(c)\\x0cCHI ‚Äô23, April 23‚Äì28, 2023, Hamburg, Germany\\n\\nJ.Rekimoto\\n\\nFigure 10: Speech quality evaluation between whispered and\\nWESPER converted voices by MUSHRA (MUltiple Stimuli\\nwith Hidden Reference and Anchor). (*: ùëù < 0.01, t-test)\\n\\n‚Ä¢ WESPER preserved the natural prosody of the source whispered\\n\\nvoice.\\n\\n5.2 Speech Recognition Accuracy\\nThere are two possible ways to use WESPER as a speech recognizer,\\nincluding speech recognition with WESPER and recognition of\\nspeech converted with WESPER by other speech recognizers.\\n\\nIn the former method, a WESPER model (based on HuBERT) pre-\\ntrained with whispered and normal speech is fine-tuned using the\\nwhispered corpus, and text is inferred from the whispered voice. In\\nthe latter case, the whispered speech converted by WESPER can con-\\ntrol any other speech-enabled device. If the whispered speech can\\nbe converted to normal speech, then the existing speech-enabled\\ndevices can be used immediately without the need to modify them\\nfor whispered speech.\\n\\nUsing the existing speech recognizer (google cloud speech-to-\\ntext [24]) as a reference, we measured the speech recognition accu-\\nracy of normal speech, whispered speech, and whispered speech\\nconverted by WESPER.\\n\\nThe wTIMIT corpus was used for the measurements. wTIMIT\\nis a TIMIT-compliant transcription containing both normal and\\nwhispered speech with labels. This corpus evaluated the recogni-\\ntion accuracy of whispered speech converted by WESPER using\\nthe recognition accuracy of normal and whispered speech as the\\nbaseline.\\n\\nThe results are summarized in Table 2 in terms of word error\\nrate (WER) and character error rate (CER), as well as bilingual eval-\\nuation understudy (BLEU). As shown in the table, the recognition\\naccuracy was not high when whispered speech was directly rec-\\nognized (WER=44.70%), but the accuracy improved (WER=26.68%)\\nafter conversion with WESPER.\\n\\nWhen tested with wTIMIT(W), HuBERT-base pre-trained with\\nLibrispeech and wTIMIT(N,W) shows better results than the google\\ncloud speech-to-text (HuBERT: WER=33.06%, Google: WER=44.70%).\\nIt is speculated that the effect of pre-training with a mixture of whis-\\npered and normal speech (but without fine-tuning with whispered\\nvoice) may contribute to the accuracy of ASR.\\n\\nFigure 11: Speech quality evaluation for people with speech\\ndisorders ranked by 5-point MOS: (*: ùëù < 0.01, **: ùëù < 0.05,\\nS1-S5: speakers, VFP: Vocal Fold Polyps, SD: Spasmodic Dys-\\nphonia)\\n\\nFigure 12: Speech quality evaluation for people with vocal\\ndisabilities ranked by MUSHRA. (*: ùëù < 0.01, S1-S5: speakers,\\nVFP: Vocal Fold Polyps, SD: Spasmodic Dysphonia)\\n\\nNotably, WESPER is not trained with a labeled corpus. It is pre-\\ntrained with whispered and normal speeches, which improves the\\nspeech recognition accuracy.\\n\\nTherefore, speech recognition via WESPER conversion does not\\nrequire the preparation of a corpus of whispered speech for specific\\nor unspecified speakers.\\n\\n5.3 Evaluations on Speech Reconstruction for\\n\\nPeople with Speech Disorders\\n\\nAn important goal of WESPER is the reconstruction of atypical\\nspeech of people with speech disorders or hearing impairments.\\nThis makes their speech more understandable to people who are\\nnot familiar with their individual speech patterns.\\n\\nDysphonia is an involuntary hoarse, breathy, or strained sound-\\ning voice or low volume or pitch. There are various causes, including\\nspasms, polyps of the vocal tract, and other causes. If the vocal cords\\nhave been removed because of throat cancer or other causes, the\\nvoice becomes extremely difficult to produce. For hearing impair-\\nment, even if the vocal cords are not affected, the control of the\\nvocal cords becomes difficult, resulting in dysphonia. Electrolarynx\\nare used to mechanically vibrate the throat as a vocalization for\\n\\n*******n.s.******\\x0cWESPER: Zero-shot and Realtime Whisper to Normal Voice Conversion for Whisper-based Speech Interactions\\n\\nCHI ‚Äô23, April 23‚Äì28, 2023, Hamburg, Germany\\n\\nwere fluent in German in addition to English, as the reference\\nsentence used was in German.\\n\\nThe results are presented in Figures 11, 12, 13, and 14. Figure 11\\nshows the results in terms of MOS. For both SD and VFP, WESPER-\\nconverted voices had higher MOS scores (ùëù < 0.01, effect size=0.66)\\nand MUSHRA scores (ùëù < 0.01, effect size=0.79). Figure 13 shows\\nthe responses to the question ‚ÄúIs this voice hoarse or normal?‚Äù\\nand there is a clear improvement in the WESPER-converted voices\\ncompared to the original VFP and SD voices (ùëù < 0.01). Figure 14\\nshows the answers to the question ‚ÄúDoes the voice use consistent\\narticulation, standard intonation, and prosody?‚Äù. Here, WESPER-\\nconverted and original voices showed nearly equal scores, although\\nWESPER-converted voices showed slightly better scores. It can be\\nassumed that WESPER did not affect the prosody of the original\\nspeech.\\n\\nAccording to these evaluations, we can make the following con-\\n\\nclusions.\\n‚Ä¢ WESPER-converted voices of people with VFP and SD speech\\ndisabilities showed better quality. This suggests that WESPER\\ncan improve the quality of the speech of people with these con-\\nditions in terms of its intelligibility by people unfamiliar with\\ntheir individual speech patterns.\\n\\n‚Ä¢ Similarly, WESPER can improve the naturalness of original VFP\\n\\nand SD speech.\\n\\n‚Ä¢ WESPER could also preserve the natural prosody of the source\\n\\nspeech.\\n\\nNotably, this test was performed on German sentences. Although\\nthe WESPER pre-training was performed only on English speech\\nand not on German speech, the prosody of the source speech was\\npreserved and its MOSs were improved. Therefore, this result may\\ndemonstrate the language-independence ability of the WESPER\\nmodel. The attached video shows an example of whisper to normal\\nconversion in Japanese. Because wav2vec 2.0, the base model for\\nSTU, has also shown language-independent pre-training perfor-\\nmance [9], the language-independent ability of WESPER could be\\na feature worth evaluating in the future.\\n\\n5.4 Speech Reconstruction Evaluation for\\nPeople with Hearing Impairment\\n\\nFinally, we evaluated the effect of speech reconstruction by WES-\\nPER for people with hearing impairments. People with hearing loss\\ncannot hear their own speech and that of others; therefore, they\\ntend to have difficulty speaking in a way that is easily understood\\nby general speakers. However, because their vocal organs are nor-\\nmal, their speech has different characteristics from that of people\\nwith dysphasia.\\n\\nWe used the ‚Äúcorpus of deaf speech for acoustic and speech\\nproduction research‚Äù [40]. Utterances of five speakers (one was a\\nhearing speaker and the other four were hearing impaired) were\\nused. We recruited 50 evaluation participants using Prolific [25],\\nwho were evenly balanced in terms of gender, fluent English speak-\\ners, and over the age of 18. We asked the participants to perform a\\n5-point ranking of each utterance in terms of MOS, hoarse-normal,\\nand natural prosody.\\n\\nThe results are presented in Figures 15, 16, 17, and 18. These\\nresults indicate that MOS, MUSHRA, and other rating scores were\\n\\nFigure 13: Hoarse-Normal assessments of utterances by peo-\\nple with speech disorders (*: ùëù < 0.01).\\n\\nFigure 14: Speech prosody consistency of utterances by peo-\\nple with speech disorders. (n.s.: not significant)\\n\\npeople with vocal cord damage, but the sound produced is artificial,\\nthe pitch conversion is deterministic, and there is a large devia-\\ntion from normal vocalization. The communication deficit caused\\nby dysphonia is a serious problem, and its elimination by voice\\nconversion technology should be of great social value.\\n\\nTo investigate the quality of the improvement by WESPER voice\\nconversion, we evaluated the speech utterances of people with two\\ntypes of speech disorders, as described below.\\n\\nVocal Fold Polyps (we will refer as VFPs): VFPs are among the\\nmost common benign lesions of the larynx, affecting the quality\\nof voice production.\\n\\nSpasmodic Dysphonia (we will refer as SD): This is also called\\nlaryngeal dystonia, SD is another common neurological disorder\\nthat affects voice and speech. It is a lifelong condition that causes\\nspasms of the muscles that produce the voice.\\n\\nWe used the Saarbruecken Voice Database (SVD) corpus [1, 45]\\nto sample utterances from people with VFP and SD. The SVD is a\\ncommonly used corpus of voices of people with speech disorders. It\\ncontains recordings of vowel utterances and a recording of the Ger-\\nman reference sentence ‚ÄúGuten Morgen, wie geht es Ihnen?‚Äù (‚ÄúGood\\nmorning, how are you?‚Äù). This sentence was used for evaluation.\\nFifty participants were recruited for the evaluation using the\\nProlific [25]. The recruited participants were evenly balanced in\\nterms of gender and were all over the age of 18. The participants\\n\\n******n.s.n.s.n.s.n.s.n.s.n.s.\\x0cCHI ‚Äô23, April 23‚Äì28, 2023, Hamburg, Germany\\n\\nJ.Rekimoto\\n\\nFigure 15: Speech quality evaluation for people with hear-\\ning impairment: Speech quality was ranked by 5-point MOS.\\n(S1‚Äô-S4‚Äô :speakers, *: ùëù < 0.01, **: ùëù < 0.05)\\n\\nFigure 18: Speech natural prosody consistency by people\\nwith hearing impairment. (**: ùëù < 0.05)\\n\\nThe results of all experiments are summarized in Table 3.\\n\\n6 DISCUSSIONS\\nCombination of WESPER and User-Dependent Fine-tuning. The re-\\nsults of the evaluation experiments indicated that the degree of\\nimprovement in the speech of people with hearing impairments\\nwas less than that of the speech of people with dysphasia. There-\\nfore, we assume that the former has significant prosodic varia-\\ntions. Although the primary purpose of this study was to achieve\\nspeaker-independent speech conversion by pre-training alone, we\\nplan to conduct additional experiments to investigate whether the\\nconversion performance can be improved by applying small-scale\\nfine-tuning to each speaker.\\n\\nEven in this case, we believe that speech pairs are necessary,\\nbut text transcription is not required. A pair of whispered and\\nhoarse speech utterances and normal speech converted to a com-\\nmon speech unit by STU can be used instead of text transcription,\\nas in the case of ASR fine-tuning.\\n\\nAudio Input Device Suitable for Whispered Speech. As demonstrated\\nin this study, whispered or hoarse voices can be converted into\\nnormal voices. In practice, the selection of an appropriate audio\\ninput device would be important. We are currently testing our\\nproposed approach with normal headsets and a directional array\\nmicrophone (designed for smart speakers), and have obtained good\\nresults. For wearable devices, a non-audible murmur (NAM) mi-\\ncrophone that detects skin vibrations could be used for inaudible\\nutterances [20]. Combining with noise reduction techniques such\\nas [7, 61] is another important future direction.\\n\\nPhilips and Dyson are also developing masks that provide pow-\\nered respiratory ventilation to protect against air pollution and\\ninfectious diseases [13, 26]. A microphone can be placed inside\\nsuch masks to pick up a whispered voice, which would give an\\neffect almost equivalent to silent speech.\\n\\nHuman-AI Integration. This research concerns machine-learning\\ntechniques for converting the whispered speech of unspecified\\nspeakers into normal speech. In practice, however, we established\\nthat users noticed that some similar whispered utterances were\\neasily converted to normal speech, whereas others were difficult.\\nThey tried to speak by relying on machine learning on the user\\n\\nFigure 16: Speech quality evaluation for people with hear-\\ning impairment: Speech quality was ranked by MUSHRA.\\n(S1‚Äô-S4‚Äô: speakers, **: ùëù < 0.05)\\n\\nFigure 17: Hoarse-Normal assessments of utterances by peo-\\nple with hearing impairment. (*: ùëù < 0.01, **: ùëù < 0.05)\\n\\nhigher for those converted by WESPER (ùëù < 0.05, effect size = 0.45\\nfor MOS, ùëù < 0.05, effect size = 0.21 for MUSHRA), but the degree\\nof improvement was less than for the speakers with voice disorders.\\nIn addition, prosody ratings were significantly lower for the speech\\nof the hearing impaired compared to the speech of the hearing\\nspeakers. Therefore, these results indicate that people with hearing\\nloss may have difficulty controlling prosody while speaking.\\n\\n**********n.s.n.s.****************n.s.n.s.\\x0cWESPER: Zero-shot and Realtime Whisper to Normal Voice Conversion for Whisper-based Speech Interactions\\n\\nCHI ‚Äô23, April 23‚Äì28, 2023, Hamburg, Germany\\n\\nSpeaker Type\\n\\nNormal\\nWhisper\\nWESPER[Whisper]\\n\\nMOS MUSHRA Q1\\n4.16\\n90.01\\n4.15\\n2.90\\n51.33\\n3.27\\n3.97\\n63.64\\n3.79\\n\\nVFP\\n\\nSD\\n\\nS1\\nWESPER[S1]\\nS2\\nWESPER[S2]\\nS3\\nWESPER[S3]\\nS4\\nWESPER[S4]\\nS5\\nWESPER[S4]\\n\\nSpeakers with speech disorders\\n19.13\\n35.72\\n36.66\\n57.53\\n42.19\\n62.74\\n27.51\\n44.45\\n49.86\\n65.53\\n35.20\\n53.30\\n\\n2.72\\n3.42\\n3.08\\n3.96\\n3.20\\n4.12\\n2.24\\n2.88\\n3.54\\n3.94\\n2.93\\n3.64\\n\\nAll\\nWESPER[All]\\n\\n1.54\\n3.78\\n2.20\\n4.16\\n2.00\\n4.36\\n1.20\\n3.36\\n2.68\\n3.90\\n1.93\\n3.89\\n\\nQ2\\n3.99\\n3.31\\n3.43\\n\\n3.00\\n3.24\\n3.40\\n3.76\\n3.26\\n3.60\\n2.36\\n2.20\\n3.54\\n3.64\\n3.12\\n3.29\\n\\nNormal\\nS1‚Äô\\nWESPER[S1‚Äô]\\nS2‚Äô\\nWESPER[S2‚Äô]\\nS3‚Äô\\nWESPER[S3‚Äô]\\nS4‚Äô\\nWESPER[S4‚Äô]\\nAll\\nWESPER[All]\\n\\nSpeakers with hearing impaired\\n91.75\\n43.05\\n47.47\\n20.76\\n20.05\\n28.30\\n35.07\\n35.86\\n41.23\\n32.07\\n36.21\\n\\n3.68\\n2.30\\n2.61\\n1.61\\n1.93\\n2.03\\n2.39\\n2.38\\n3.06\\n2.08\\n2.50\\nMean Opinion Score\\nMUltiple Stimuli with Hidden Reference and Anchor\\n‚ÄòIs this voice hoarse or normal?‚Äô\\n\\n3.75\\n2.51\\n2.97\\n1.84\\n2.55\\n2.22\\n2.93\\n2.59\\n3.35\\n2.12\\n2.36\\n\\n3.76\\n2.29\\n2.38\\n1.74\\n2.06\\n2.19\\n2.39\\n2.26\\n2.62\\n2.28\\n2.94\\n\\n‚ÄòIs the voice use consistent articulation,\\nstandard intonation and prosody?‚Äô\\n\\nMOS\\nMUSHRA\\nQ1\\n\\nQ2\\n\\nWESPER[‚Ä¢] WESPER converted ‚Ä¢\\nVFP\\nSD\\nS1-S5, S1‚Äô-S4‚Äô\\n\\nVocal Fold Polyps speakers\\nSpasmodic Dysphonia speakers\\nspeaker IDs\\n\\nTable 3: Summary of Voice Conversion Quality Evaluation\\n(MUSHRA: 100-point score, others:5-point score)\\n\\nside. This interaction suggests that machine learning does not only\\nunilaterally extend human capabilities, but that further synergistic\\neffects can be achieved by learning on the human side as well. This\\ncan be seen as an actual example of human-AI integration [47]\\nthrough vocalization.\\n\\n7 CONCLUSION\\nIn this study, we proposed WESPER as a mechanism for the real-\\ntime conversion of whispered speech into normal speech. We con-\\nfirmed that a common speech unit can be obtained by self-supervised\\nlearning even when the acoustic features of whispered and nor-\\nmal speech are different. Speech reproduction can be learned from\\nspeech data of arbitrary target speakers only without using text\\n\\nlabels. There is no need to train for each user, nor is there a need\\nfor parallel data for whispered and normal speech. From speech\\nunits, WESPER can reconstruct utterances of any target speaker and\\nrequires only unlabeled speech data from target speakers. We con-\\nfirmed that the quality of the converted speech is improved and that\\nthe prosody of the speech is preserved. Additionally, we reported\\nan evaluation of the results of reconstructing speech utterances of\\npeople with speech disorders and hearing impairments.\\n\\nACKNOWLEDGMENTS\\nWe are grateful to the reviewers for their valuable comments. This\\nwork was supported by JST Moonshot R&D Grant Number JP-\\nMJMS2012, JST CREST Grant Number JPMJCR17A3, the commis-\\nsioned research by NICT Japan, and The University of Tokyo Hu-\\nman Augmentation Research Initiative.\\n\\nREFERENCES\\n[1] A. Al-Nasheri, G. Muhammad, M. Alsulaiman, and Z. Ali. 2017. Investigation of\\nVoice Pathology Detection and Classification on Different Frequency Regions\\nUsing Correlation Functions. Journal of Voice (2017). https://doi.org/10.1016/j.\\njvoice.2016.01.014\\n\\n[2] Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. 2020.\\nwav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representa-\\ntions. arXiv [cs.CL] (June 2020).\\n\\n[3] Abdelkareem Bedri, Himanshu Sahni, Pavleen Thukral, Thad Starner, David Byrd,\\nPeter Presti, Gabriel Reyes, Maysam Ghovanloo, and Zehua Guo. 2015. Toward\\nSilent-Speech Control of Consumer Wearables. Computer 48, 10 (2015), 54‚Äì62.\\nhttps://doi.org/10.1109/MC.2015.310\\n\\n[4] Fadi Biadsy, Ron J. Weiss, Pedro J. Moreno, Dimitri Kanevsky, and Ye Jia.\\n2019. Parrotron: An End-to-End Speech-to-Speech Conversion Model and\\nhttps:\\nits Applications to Hearing-Impaired Speech and Speech Separation.\\n//doi.org/10.48550/ARXIV.1904.04169\\n\\n[5] Zal√°n Borsos, Rapha√´l Marinier, Damien Vincent, Eugene Kharitonov, Olivier\\nPietquin, Matt Sharifi, Olivier Teboul, David Grangier, Marco Tagliasacchi, and\\nNeil Zeghidour. 2022. AudioLM: a Language Modeling Approach to Audio\\nGeneration. https://doi.org/10.48550/ARXIV.2209.03143\\n\\n[6] Heng-Jui Chang, Alexander H Liu, Hung-Yi Lee, and Lin-Shan Lee. 2020. End-to-\\nend Whispered Speech Recognition with Frequency-weighted Approaches and\\nPseudo Whisper Pre-training. (May 2020). arXiv:2005.01972 [cs.CL]\\n\\n[7] Ishan Chatterjee, Maruchi Kim, Vivek Jayaram, Shyamnath Gollakota, Ira Kemel-\\nmacher, Shwetak Patel, and Steven M. Seitz. 2022. ClearBuds. In Proceedings\\nof the 20th Annual International Conference on Mobile Systems, Applications and\\nServices. ACM. https://doi.org/10.1145/3498361.3538933\\n\\n[8] Chung-Ming Chien, Jheng-Hao Lin, Chien-yu Huang, Po-chun Hsu, and Hung-yi\\nLee. 2021.\\nInvestigating on Incorporating Pretrained and Learnable Speaker\\nRepresentations for Multi-Speaker Multi-Style Text-to-Speech. In ICASSP 2021\\n- 2021 IEEE International Conference on Acoustics, Speech and Signal Processing\\n(ICASSP). 8588‚Äì8592. https://doi.org/10.1109/ICASSP39728.2021.9413880\\n[9] Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, and\\nMichael Auli. 2020. Unsupervised Cross-lingual Representation Learning for\\nSpeech Recognition. https://doi.org/10.48550/ARXIV.2006.13979\\n\\n[10] Marius Cotescu, Thomas Drugman, Goeric Huybrechts, Jaime Lorenzo-Trueba,\\nand Alexis Moinet. 2019. Voice Conversion for Whispered Speech Synthesis.\\n(Dec. 2019). arXiv:1912.05289 [cs.SD]\\n\\n[11] B. Denby, T. Schultz, K. Honda, T. Hueber, J. M. Gilbert, and J. S. Brumberg.\\n2010. Silent Speech Interfaces. Speech Commun. 52, 4 (April 2010), 270‚Äì287.\\nhttps://doi.org/10.1016/j.specom.2009.08.002\\n\\n[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT:\\nPre-training of Deep Bidirectional Transformers for Language Understanding.\\nhttps://doi.org/10.48550/ARXIV.1810.04805\\n\\n[13] Dyson. 2022. dyson zone: Air-purifying headphones with active noise cancelling.\\n\\nhttps://www.dyson.co.uk/en.\\n\\n[14] Joo Freitas, Antnio Teixeira, Miguel Sales Dias, and Samuel Silva. 2016. An\\nIntroduction to Silent Speech Interfaces (1st ed.). Springer Publishing Company,\\nIncorporated.\\n\\n[15] Masaaki Fukumoto. 2018. SilentVoice: Unnoticeable Voice Input by Ingressive\\nSpeech. In Proceedings of the 31st Annual ACM Symposium on User Interface Soft-\\nware and Technology (Berlin, Germany) (UIST ‚Äô18). Association for Computing Ma-\\nchinery, New York, NY, USA, 237‚Äì246. https://doi.org/10.1145/3242587.3242603\\n[16] Teng Gao, Jian Zhou, Huabin Wang, Liang Tao, and Hon Keung Kwan. 2021.\\nAttention-Guided Generative Adversarial Network for Whisper to Normal Speech\\n\\n\\x0cCHI ‚Äô23, April 23‚Äì28, 2023, Hamburg, Germany\\n\\nJ.Rekimoto\\n\\nConversion. https://doi.org/10.48550/ARXIV.2111.01342\\n\\n[17] Alex Graves, Santiago Fern√°ndez, Faustino Gomez, and J√ºrgen Schmidhuber.\\n2006. Connectionist Temporal Classification: Labelling Unsegmented Sequence\\nData with Recurrent Neural Networks. In Proceedings of the 23rd International\\nConference on Machine Learning (Pittsburgh, Pennsylvania, USA) (ICML ‚Äô06).\\nAssociation for Computing Machinery, New York, NY, USA, 369‚Äì376. https:\\n//doi.org/10.1145/1143844.1143891\\n\\n[18] Dorde T. Grozdic and Slobodan T. Jovicic. 2017. Whispered Speech Recognition\\nUsing Deep Denoising Autoencoder and Inverse Filtering.\\nIEEE/ACM Trans.\\nAudio, Speech and Lang. Proc. 25, 12 (dec 2017), 2313‚Äì2322. https://doi.org/10.\\n1109/TASLP.2017.2738559\\n\\n[19] Tomoki Hayashi, Wen-Chin Huang, Kazuhiro Kobayashi, and Tomoki Toda.\\nhttps:\\n\\n2021. Non-autoregressive sequence-to-sequence voice conversion.\\n//doi.org/10.48550/ARXIV.2104.06793\\n\\n[20] Panikos Heracleous, Yoshitaka Nakajima, Hiroshi Saruwatari, and Kiyohiro\\nShikano. 2005. A Tissue-Conductive Acoustic Sensor Applied in Speech Recogni-\\ntion for Privacy. In Proceedings of the 2005 Joint Conference on Smart Objects and\\nAmbient Intelligence: Innovative Context-Aware Services: Usages and Technologies\\n(Grenoble, France) (sOc-EUSAI ‚Äô05). Association for Computing Machinery, New\\nYork, NY, USA, 93‚Äì97. https://doi.org/10.1145/1107548.1107577\\n\\n[21] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia,\\nRuslan Salakhutdinov, and Abdelrahman Mohamed. 2021. HuBERT: Self-\\nSupervised Speech Representation Learning by Masked Prediction of Hidden\\nUnits. IEEE/ACM Trans. Audio, Speech and Lang. Proc. 29 (jan 2021), 3451‚Äì3460.\\nhttps://doi.org/10.1109/TASLP.2021.3122291\\n\\n[22] Wen-Chin Huang, Tomoki Hayashi, Shinji Watanabe, and Tomoki Toda. 2020.\\nThe Sequence-to-Sequence Baseline for the Voice Conversion Challenge 2020:\\nCascading ASR and TTS. https://doi.org/10.48550/ARXIV.2010.02434\\n\\n[23] Amazon.com Inc. 2018. How Alexa keeps getting smarter.\\naboutamazon.com/devices/how-alexa-keeps-getting-smarter\\n\\nhttps://www.\\n\\n[24] Google Inc. 2020. Google Cloud Speech-to-Text. https://cloud.google.com/speech-\\n\\nto-text.\\n\\n[25] Prolific inc. 2014. Prolific. https://www.prolific.co\\n[26] Philips Inc. 2021. Fresh Air Mask Series 6000. https://www.philips.com.sg/c-\\n\\np/ACM066_01/fresh-air-mask-series-6000.\\n\\n[27] Keith Ito and Linda Johnson. 2017. The LJ Speech Dataset. https://keithito.com/LJ-\\n\\nSpeech-Dataset/.\\n\\n[28] jfsantos. 2019. mushraJS. https://github.com/jfsantos/mushraJS\\n[29] Hirokazu Kameoka, Takuhiro Kaneko, Kou Tanaka, and Nobukatsu Hojo. 2018.\\nStarGAN-VC: Non-parallel many-to-many voice conversion with star generative\\nadversarial networks. https://doi.org/10.48550/ARXIV.1806.02169\\n\\n[30] Takuhiro Kaneko and Hirokazu Kameoka. 2017. Parallel-Data-Free Voice Con-\\nversion Using Cycle-Consistent Adversarial Networks. https://doi.org/10.48550/\\nARXIV.1711.11293\\n\\n[31] Arnav Kapur, Shreyas Kapur, and Pattie Maes. 2018. AlterEgo: A Personalized\\nWearable Silent Speech Interface. In 23rd International Conference on Intelligent\\nUser Interfaces (Tokyo, Japan) (IUI ‚Äô18). ACM, 43‚Äì53. https://doi.org/10.1145/\\n3172944.3172977\\n\\n[32] Naoki Kimura, Michinari Kono, and Jun Rekimoto. 2019. SottoVoce: An Ultra-\\nsound Imaging-Based Silent Speech Interaction Using Deep Neural Networks. In\\nProceedings of the 2019 CHI Conference on Human Factors in Computing Systems\\n(Glasgow, Scotland Uk) (CHI ‚Äô19). Association for Computing Machinery, New\\nYork, NY, USA, 1‚Äì11. https://doi.org/10.1145/3290605.3300376\\n\\n[33] Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. 2020. HiFi-GAN: Generative\\nAdversarial Networks for Efficient and High Fidelity Speech Synthesis. https:\\n//doi.org/10.48550/ARXIV.2010.05646\\n\\n[34] Felix Kreuk, Adam Polyak, Jade Copet, Eugene Kharitonov, Tu-Anh Nguyen,\\nMorgane Rivi√®re, Wei-Ning Hsu, Abdelrahman Mohamed, Emmanuel Dupoux,\\nand Yossi Adi. 2021. Textless Speech Emotion Conversion using Discrete and\\nDecomposed Representations. https://doi.org/10.48550/ARXIV.2111.07402\\n[35] Kushal Lakhotia, Evgeny Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak,\\nBenjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Adelrahman Mo-\\nhamed, and Emmanuel Dupoux. 2021. Generative Spoken Language Modeling\\nfrom Raw Audio. https://doi.org/10.48550/ARXIV.2102.01192\\n\\n[36] Boon Pang Lim. 2010. Computational differences between whispered and non-\\nwhispered speech, Ph.D. Thesis, University of Illinois Urbana-Champaign.\\n[37] H. Malaviya, J. Shah, M. Patel, J. Munshi, and H. A. Patil. 2020. Mspec-Net : Multi-\\nDomain Speech Conversion Network. In ICASSP 2020 - 2020 IEEE International\\nConference on Acoustics, Speech and Signal Processing (ICASSP). 7764‚Äì7768.\\n[38] Michael McAuliffe, Michaela Socolof, Sarah Mihuc, Michael Wagner, and Morgan\\nSonderegger. 2017. Montreal Forced Aligner: Trainable Text-Speech Alignment\\nUsing Kaldi. 498‚Äì502. https://doi.org/10.21437/Interspeech.2017-1386\\n\\n[39] Leland McInnes, John Healy, and James Melville. 2018. UMAP: Uniform Manifold\\nApproximation and Projection for Dimension Reduction. https://doi.org/10.\\n48550/ARXIV.1802.03426\\n\\n[40] Lisa Lucks Mendel, Sungmin Lee, Monique Pousson, Chhayakanta Patro, Skylar\\nMcSorley, Bonny Banerjee, Shamima Najnin, and Masoumeh Heidari Kapourchali.\\n2017. Corpus of deaf speech for acoustic and speech production research. The\\n\\nJournal of the Acoustical Society of America 142, (1) (2017), EL102. https://doi.\\norg/10.1121/1.4994288\\n\\n[41] Marco A. Oliveira. 2022. Machine Learning Approaches for Whisper to Normal\\nSpeech Conversion: A Survey. U.Porto Journal of Engineering 8, 2 (2022), 202‚Äì212.\\nhttps://doi.org/10.24840/2183-6493_008.002_0016\\n\\n[42] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015.\\nLibrispeech: An ASR corpus based on public domain audio books. In 2015 IEEE\\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP).\\n5206‚Äì5210. https://doi.org/10.1109/ICASSP.2015.7178964\\n\\n[43] Santiago Pascual, Antonio Bonafonte, Joan Serr√†, and Jose A. Gonzalez. 2018.\\nWhispered-to-voiced Alaryngeal Speech Conversion with Generative Adversarial\\nNetworks. https://doi.org/10.48550/ARXIV.1808.10687\\n\\n[44] Leonardo Pepino, Pablo Riera, and Luciana Ferrer. 2021.\\n\\nRecognition from Speech Using Wav2vec 2.0 Embeddings.\\narXiv:2104.03502 [cs.SD]\\n\\nEmotion\\n(April 2021).\\n\\n[45] Manfred ¬®Putzer and William J. Barry. 2016. Saarbruecken voice database. https:\\n\\n//stimmdb.coli.uni-saarland.de\\n\\n[46] Kaizhi Qian, Yang Zhang, Shiyu Chang, Xuesong Yang, and Mark Hasegawa-\\nJohnson. 2019. AutoVC: Zero-Shot Voice Style Transfer with Only Autoencoder\\nLoss. In Proceedings of the 36th International Conference on Machine Learning\\n(Proceedings of Machine Learning Research, Vol. 97), Kamalika Chaudhuri and\\nRuslan Salakhutdinov (Eds.). PMLR, 5210‚Äì5219. https://proceedings.mlr.press/\\nv97/qian19c.html\\n\\n[47] Jun Rekimoto. 2019. Homo Cyberneticus: The Era of Human-AI Integration.\\n\\nhttps://doi.org/10.48550/arXiv.1911.02637\\n\\n[48] Jun Rekimoto. 2022. DualVoice: Speech Interaction That Discriminates between\\nNormal and Whispered Voice Input. In Proceedings of the 35th Annual ACM\\nSymposium on User Interface Software and Technology (Bend, OR, USA) (UIST ‚Äô22).\\nAssociation for Computing Machinery, New York, NY, USA, Article 91, 10 pages.\\nhttps://doi.org/10.1145/3526113.3545685\\n\\n[49] Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan\\nLiu. 2020. FastSpeech 2: Fast and High-Quality End-to-End Text to Speech.\\nhttps://doi.org/10.48550/ARXIV.2006.04558\\n\\n[50] Gabriel Reyes, Dingtian Zhang, Sarthak Ghosh, Pratik Shah, Jason Wu, Aman\\nParnami, Bailey Bercik, Thad Starner, Gregory D. Abowd, and W. Keith Edwards.\\n2016. Whoosh: Non-Voice Acoustics for Low-Cost, Hands-Free, and Rapid Input\\non Smartwatches. In Proceedings of the 2016 ACM International Symposium on\\nWearable Computers (ISWS‚Äô16). ACM, 120‚Äì127. https://doi.org/10.1145/2971763.\\n2971765\\n\\n[51] Himanshu Sahni, Abdelkareem Bedri, Gabriel Reyes, Pavleen Thukral, Zehua\\nGuo, Thad Starner, and Maysam Ghovanloo. 2014. The Tongue and Ear Interface:\\nA Wearable System for Silent Speech Recognition. In Proceedings of the 2014 ACM\\nInternational Symposium on Wearable Computers (Seattle, Washington) (ISWC\\n‚Äô14). ACM, 47‚Äì54. https://doi.org/10.1145/2634317.2634322\\n\\n[52] seeed studio. 2019. ReSpeaker USB Mic Array. https://wiki.seeedstudio.com/\\n\\nReSpeaker-USB-Mic-Array/\\n\\n[53] Nirmesh Shah, Mihir Parmar, Neil Shah, and Hemant Patil. 2018. Novel MMSE\\nDiscoGAN for Cross-Domain Whisper-to-Speech Conversion. Machine Learning.\\nIn Speech and Language Processing (MLSLP) Workshop.\\n\\n[54] Ke Sun, Chun Yu, Weinan Shi, Lan Liu, and Yuanchun Shi. 2018. Lip-Interact: Im-\\nproving Mobile Device Interaction with Silent Speech Commands. In Proceedings\\nof the 31st Annual ACM Symposium on User Interface Software and Technology\\n(Berlin, Germany) (UIST ‚Äô18). Association for Computing Machinery, New York,\\nNY, USA, 581‚Äì593. https://doi.org/10.1145/3242587.3242599\\n\\n[55] Lifa Sun, Kun Li, Hao Wang, Shiyin Kang, and Helen Meng. 2016. Phonetic\\nposteriorgrams for many-to-one voice conversion without parallel data training.\\nIn 2016 IEEE International Conference on Multimedia and Expo (ICME). 1‚Äì6. https:\\n//doi.org/10.1109/ICME.2016.7552917\\n\\n[56] International Telecommunication Union. 2013. BS.1534 : Method for the sub-\\nhttps:\\n\\njective assessment of intermediate quality level of audio systems.\\n//www.itu.int/rec/R-REC-BS.1534/en\\n\\n[57] Benjamin van Niekerk, Marc-Andre Carbonneau, Julian Zaidi, Matthew Baas,\\nHugo Seute, and Herman Kamper. 2022. A Comparison of Discrete and Soft\\nSpeech Units for Improved Voice Conversion. In ICASSP 2022 - 2022 IEEE Inter-\\nnational Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE.\\nhttps://doi.org/10.1109/icassp43922.2022.9746484\\n\\n[58] Yuxuan Wang, R. J. Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J. Weiss,\\nNavdeep Jaitly, Zongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, Quoc V.\\nLe, Yannis Agiomyrgiannakis, Rob Clark, and Rif A. Saurous. 2017. Tacotron: A\\nFully End-to-End Text-To-Speech Synthesis Model. CoRR abs/1703.10135 (2017).\\narXiv:1703.10135 http://arxiv.org/abs/1703.10135\\n\\n[59] Cheng Yi, Jianzhong Wang, Ning Cheng, Shiyu Zhou, and Bo Xu. 2020. Applying\\nWav2vec2.0 to Speech Recognition in Various Low-resource Languages. (Dec.\\n2020). arXiv:2012.12121 [cs.CL]\\n\\n[60] zeta chicken. 2017. toWhisper. https://github.com/zeta-chicken/toWhisper\\n[61] Qiu-Shi Zhu, Long Zhou, Jie Zhang, Shu-Jie Liu, Yu-Chen Hu, and Li-Rong\\nDai. 2022. Robust Data2vec: Noise-robust Speech Representation Learning for\\n\\n\\x0cWESPER: Zero-shot and Realtime Whisper to Normal Voice Conversion for Whisper-based Speech Interactions\\n\\nCHI ‚Äô23, April 23‚Äì28, 2023, Hamburg, Germany\\n\\nASR by Combining Regression and Improved Contrastive Learning.\\n\\nhttps:\\n\\n//doi.org/10.48550/ARXIV.2210.15324\\n\\n\\x0c'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_filepath = \"papers/2303.01639.pdf\"\n",
    "text = extract_text(pdf_filepath)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62376"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7027\n",
      "4871\n"
     ]
    }
   ],
   "source": [
    "# Remove unnecessary parts\n",
    "text = remove_unnecessary_parts(text)\n",
    "\n",
    "# check the length of the text\n",
    "print(len(text.split()))\n",
    "\n",
    "# Preprocess text\n",
    "text = preprocess_text(text)\n",
    "\n",
    "# check the length of the text\n",
    "print(len(text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### input text WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions Jun Rekimoto The University Tokyo 731 Hongo Bunkyoku Tokyo Japan Sony Computer Science Laboratories Kyoto 131 Hontorocho Shimogyoku Kyotoshi Kyoto Japan rekimotoacmorg 3 2 0 2 r M 3 D S c 1 v 9 3 6 1 0 3 0 3 2 v X r Figure 1 WESPER realtime whispertonormal speech conversion mechanism consisting speechtounit STU en coder generates common speech units whispered normal utterances using selfsupervised pretraining unittospeech UTS decoder recovers speech speech units It achieves userindependent voice conversion real time ABSTRACT Recognizing whispered speech converting normal speech creates many possibilities speech interaction Because sound pressure whispered speech significantly lower normal speech used semisilent speech interaction public places without audible others Converting whispers normal speech also improves speech quality people speech hearing impairments However conventional speech con version techniques provide sufficient conversion quality require speakerdependent datasets consisting pairs whis pered normal speech utterances To address problems propose WESPER zeroshot realtime whispertonormal speech conversion mechanism based selfsupervised learning WESPER consists speechtounit STU encoder generates hidden speech units common whispered normal speech unittospeech UTS decoder reconstructs speech encoded speech units Unlike existing methods conversion userindependent require paired dataset whis pered normal speech The UTS decoder reconstruct speech target speaker ‚Äô voice speech units requires unlabeled target speaker ‚Äô speech data We confirmed quality speech converted whisper improved preserving natural prosody Additionally confirmed effectiveness proposed approach perform speech construction people speech hearing disabilities CCS CONCEPTS ‚Ä¢ Humancentered computing ‚Üí Soundbased input output Interface design prototyping Mobile devices ‚Ä¢ Computing method ologies ‚Üí Neural networks Permission make digital hard copies part work personal classroom use granted without fee provided copies made distributed profit commercial advantage copies bear notice full citation first page Copyrights components work owned others authors must honored Abstracting credit permitted To copy otherwise republish post servers redistribute lists requires prior specific permission andor fee Request permissions permissionsacmorg CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany ¬© 2023 Copyright held ownerauthors Publication rights licensed ACM ACM ISBN 97814503942152304 1500 httpsdoiorg10114535445483580706 KEYWORDS speech interaction whispered voice whispered voice conversion silent speech artificial intelligence neural networks selfsupervised learning ACM Reference Format Jun Rekimoto 2023 WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions In Proceedings 2023 CHI Conference Human Factors Computing Systems CHI ‚Äô 23 Energy PredictorPitch PredictorWhisper Hoarse Normal VoicesTransformer LayersMelSpectrogramReconstructed Normal VoiceCNN EncoderTransformer LayerTransformer LayerTransformer LayerTransformer LayerCommon speech unitsbetween whisper normal voices Modified FastSpeech2SpeechtoUnit STU EncoderUnittoSpeech UTS DecoderModified HuBERT Vocoder HiFiGANMelSpectrogram Decoder256one per 20msSpeaker independentincluding persons speech hearing disabilitiesCTC Speech RecogRecognized text CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany JRekimoto April 23‚Äì28 2023 Hamburg Germany ACM New York NY USA 13 pages httpsdoiorg10114535445483580706 1 INTRODUCTION Although voice interaction systems widely deployed typically easy use presence people Using voice commands public places may socially unaccept able risk confidential information leaked In addition speaking conference call uncomfortable people vicinity compromise confidentiality conversation To overcome problems various silent speech input tech niques developed 3 31 32 50 51 54 however methods require special sensors achieved high accu racy speech recognition remaining instead level recog nizing predefined commands Conversion unconditioned silent speech normal vocal utterances also achieved Additionally silent speech could employed capture utter ances people speech hearing impairments However owing abovementioned limitations existing methods meet requirements practical accessibility aids In contrast silent speech focus whispered speech Whis pers sufficiently low sound pressure ensure confidentiality whispering almost equivalent silent speech Whispering captured ordinary microphone require special sensor configuration People speech disorders still speak whisper hoarse voice although vocal organs may injured extracted thus voices might recognized To address issues propose WESPER realtime zeroshot whispertonormal voice conversion method based selfsupervised learning It designed perform speaker inde pendent conversion whispered speech normal speech There need peruser training paired datasets whispered normal utterances required The proposed architec ture consists speechtounit STU encoder pretrained normal whispered speeches unittospeech UTS decoder generates target voice speech units Figure 1 By pretraining unpaired whisper normal voices STU trained reduce difference normal whispered utterances output common speech unit The UTS decoder trained speech data specific speaker without accompanying text labels If person ‚Äô voice used conversion performed restore whispered voice person ‚Äô normal voice even another person ‚Äô voice Because encoder decoder operate nonautoregressive manner entire system operates realtime Therefore applied teleconferencing example conference participant speak whispered voice converted real time others listen speech played back normal voice Figure 2 shows melspectrogram examples whispertonormal voice conversion using proposed method More conversion results demonstrated accompanying video The contributions study summarized follows Figure 2 WESPER conversion result Left whispered speech Middle whispered speech converted WESPER Right Normal speech speaker tran scription ‚Ä¢ We propose realtime speakerindependent vocabularyfree whispertonormal speech conversion method trained unpaired whispers normal speech ‚Ä¢ The target voice learned voice samples specific speaker without text transcription ‚Ä¢ We experimentally confirmed improvement performance normal speakers dysarthric hearingimpaired speakers 2 RELATED WORK 21 Research Silent Whispered Speech Various studies silent speech developed methods rec ognize user ‚Äô silent utterances silent commands using various sensor configurations including lip reading EMG Electromyogra phy ultrasound 3 31 32 50 51 54 However systems require special dataset perform training increases need special sensors prevents technologies ing widely used Hence systems offer limited vocabulary less 100 commands available typically around 30 Al though silent speech recognition sometimes identified possible approach help people dysphonia methods interpret unrestricted free speech owing vocabulary limitation Whispered speech similar characteristics silent speech preserving social acceptability public spaces However widely adopted given ordinary mi crophones used SilentVoice 15 speech interaction technique uses gressive speech utterance made inhaling The amplitude ingressive speech low considered variation silent speech It requires microphonelike device placed close mouth training required users speak correctly ingressive speech A custom corpus ingressive speech text transcriptions also required Research whisper voice recognition 6 11 14 18 previ ously conducted The Alexa smart speaker supports whisper mode 23 In mode user whispers Alexa Alexa responds whisper 10 DualVoice 48 also proposed method per form endtoend recognition whispered voices based self supervised speech recognition system based wav2vec20 2 HuBERT 21 They also proposed interaction technique WhisperWESPER convertedWhisperNormalsecsecsecHz WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany model training data peruser dataset conversion normal voice whisper speech recognition silent speech ex3 31 32 50 51 54 special paireddataset dedicated sensors required Parotoron 4 paired whispernormal voice required DualVoice 48 SilentVoice 15 CycleGANVC 30 MSpeCNet 37 AGANW2SC 16 WESPER labeled unpaired whisper normal voice labeled silentvoice custom unpaired unlabeled whisper normal voice paired whispernormal voice paired whispernormal voice unpaired unlabeled whisper normal voice required required required required required required YES NO NO YES YES YES YES Table 1 Research silent whispered speech YES commands characters YES voice conversion YES YES NO NO NO YES voice conversion distinguish whispered normal utterances WESPER combined DualVoice selectively convert user ‚Äô whispered voice 22 Speech Conversion Normal speech conversion technologies developed convert one voiced utterance another 19 29 30 43 46 however conversion quality methods remains unsatisfactory applied whispertonormal voice conversion Recently several machine learningbased whispertonormal voice conversion techniques investigated 41 To com pare techniques important consider quality conversion required characteristics dataset used training A paired whispernormal voice dataset ie dataset containing whispered normal versions utter ance labeled whispered voice dataset ie dataset containing whispered utterances accompanied text labels require significant amount effort prepare Conversely unpaired unlabeled whisper voice dataset used ie dataset containing whisper utterances without corresponding normal versions labels effort required prepare dataset low Attentionguided generative adversarial network whisper normal speech conversion AGANW2SC GANbased whisper normal speech conversion 16 It converts whispered voice represented melspectrogram corresponding normal speech ‚Äô melspectrogram It based GAN attention map used conversion It requires paired whispernormal speech dataset MSpeCNet 37 autoencoderbased multi domain voice conversion supports whispertonormal conver sion It also requires paired whispernormal voice dataset Moreover whispered speech converted text using speech recognition generate normal speech using texttospeech methods 22 However approach requires labeled dataset whispered speech recognition prosodic information con tained whispered utterances lost intermediate text representation convey information Parrotron 4 speech conversion system designed im prove speech speakers dysplasia It based encoderdecoder model conforms texttospeech system Tacotron 58 however differs Tacotron input output data formatted melspectrograms It also quires paired source target speeches making construction required dataset relatively difficult Phonetic posteriorgrams PPGs 55 intermediate informa tion obtained automatic speech recognition PPGs used manytoone speaker conversion represent articulation spoken content speaker independently To knowledge effects PPGs whispered voice investigated Our method also uses intermediated speech units require textbased corpus case automatic speech recognition ASR PPGs In contrast proposed system requires samples un paired target source speech require accompa nying text transcriptions thus making datasets easy prepare independent target language The characteristics related technologies summarized Table 1 We also demonstrate whisper normal conversion exam ples including NMSEDiscoGAN 53 MSpeCNet 37 CycleGAN VC 30 AGANW2SC 16 WESPER httpswesperproj githubio 23 Self Supervised Representation Learning Speech Recently combining pretraining selfsupervised representa tion learning unlabeled speech data finetuning labeled speech data attracted attention These systems primarily CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany JRekimoto intended speech recognition applications also applied perform speaker language emotion recogni tion 44 59 In particular pretraining method hiddenunit BERT hu BERT 21 similar masked language model used bidirectional encoder representations transformers BERT 12 natural language processing It designed mask part input estimate corresponding expression features rest input With pretraining model learn acoustic properties input data characteristics speech For use ASR pretraining finetuning performed small amount audio data text transcriptions A projection layer connectionist temporal classification CTC layer 17 added generate text transcriptions audio waveforms As reported 2 59 selfsupervised ASR achieved speech recognition accuracy comparable conventional stateoftheart ASRs finetuning small amount labeled speech data Therefore architecture could suitable recogniz ing whispered voice recognition limited whispered speech corpora WESPER unique uses pretraining reduce dif ference latent vectors encoding whispered normal utterances 24 TextlessNLP Recently textfree speech processing speech conversion meth ods developed Through selfsupervised learning systems derive latent representations speech data accompanied text transcriptions TextlessNLP34 35 Au dioLM 5 use text transcriptions phoneme symbols speech processing systems use discrete units constructed selfsupervised learning Soft discrete unit another approach textless speech processing 57 Our proposed method also uses nondiscrete vectors latent speech representations obtained selfsupervised learning explicitly use text transcriptions phoneme symbols Our research unique demonstrate whispered normal speech represented similar speech units selfsupervised learning In addition method also designed work seamlessly speech generation system later stages 3 THE WESPER VOICE CONVERSION MODEL WESPER consists STU encoder UTS decoder The STU converts whispered normal speech common speech units The UTS converts common speech units melspectrograms reconstructed speech vocoder WESPER char acterized fact common speech units utterance similar although STU pretrained unpaired whispered normal speech trained paired set whispered normal utterances The details method used train model described Figure 3 Overview STU pretraining Unpaired whis pered normal speech used pretraining The trans former layer trained estimating discrete units masked input The projection layer transformer layers generates 256dimensional vectors one every 20 ms used common speech units 31 SpeechtoUnit STU Encoder The STU encoder takes audio waveform input outputs units It based HuBERT 21 selfsupervised neural network speech pretrains large amount unlabeled speech BERT 12like manner learns recover relevant parts partially masked speech features thereby acquires speech language model For purpose want generate speech units identical possible whispered normal speech To achieve pretrained STU mixture whispered normal speech utterances used normal English speech many speakers Librispeech 960h dataset 42 Librispeech speech data mechanically converted whispered voice us ing LPCbased audio conversion tool 60 Additionally used wTIMIT speech dataset normal whispered speech 36 speech length 58 h Figure 3 shows pretraining process STU detail Only unpaired whispered normal speech used pretraining The transformer layer trained estimating discrete units masked input Similar HuBERT discrete target units first generated kmeans clustering input speech data first stage kmeans clustering outputs transformer intermediate layer In experiment used 100 discrete units The projection layer transformer layers generates sequence 256dimensional vectors one per 20 ms used common speech units In STU 12 transformer layers placed front CNN feature extractor Figure 1 After pretraining comparing output layer confirmed 1 difference Transformer Layer 11Transformer Layer 12ProjectionLEN 768LEN 256Label Embedding256 KLEN KLENCrossEntropy LossCosinesimilarityTo Unit Speech CNN EncoderWhisper Normal voicesunpairedTransformer Layer 1MaskingAcoustic unit discoveryeg KMeans MFCC HuBERTintermediate layerCommon Speech UnitK Discrete unitsK100Predict hidden units masked locations25620ms WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany In addition finetune STU transcriptattached wTIMIT Librispeech datasets adding CTC layer 17 48 therefore STU could used ASR recognize whis pered normal speeches 32 UTS Decoder The UTS decoder takes speech units generated STU input reconstructs target normal speech It based nonautoregressive texttospeech TTS system FastSpeech2 49 While original FastSpeech2 includes embedding layer takes text phoneme tokens transforms sequence vector embeddings eliminate layer UTS takes speech units direct input The original FastSpeech2 also includes duration estimator estimates duration phoneme token length regulator adjusts number internal vectors according estimated duration These parts also eliminated purpose STU generates speech units constant rate When learning original FastSpeech2 necessary pro vide duration phoneme corpus ground truth external tool Montreal Forced Aligner 38 Because limitation FastSpeech2 ‚Äô learning languagedependent Conversely UTS require duration estimation UTS languageindependent The output UTS melspectrogram FastSpeech2 A vocoder HiFiGAN 33 converts actual speech waveform In regular TTS system target speech corresponding text labels needed training By contrast proposed UTS requires target speech text labels The target speech passed STU obtain sequence speech units associated speech waveform used train UTS Figure 6 In experiment UTS trained using speech data single speaker LJSpeech 27 data speakers taken narration data 4 SYSTEM CONFIGURATION We designed WESPER model using PyTorch framework The STU based HuBERT UTS based modified implementation FastSpeech2 PyTorch implementation 8 We used Librispeech wTIMIT include normal whispered speech pretraining With dual NVIDIA R6000 pretraining took 48 h UTS training took 26 h target voice The total processing time required perform conversion approximately 120th actual speech duration single NVIDIA R6000 110th Apple M1 Max CPU The actual quality conversion demonstrated attached video We designed two types interfaces The first operated pushtotalk style user first speaks whispered normal voice pressing button Thereafter ton released speech waveform recorded time sent speechconversion neural networks result immediately played back The detected nonaudio period input speech automatically converted speech segments without additional user input Figure 4 Comparison whisperednormal voice differ ences The comparison made using normal speech speech whispering transformations Above An STU pretrained normal whispered speeches produced fewer differences pretrained normal speech As transformer layer deepens difference tween two decreases Middle There difference melspectrogram decreased speech units Bottom UMAP 39 visualizations differences tween whisper normal voices melspectrogram speechunit ‚Äô space speech unit values whispered normal voices de creased increasing layer depth 2 difference decreased STU pretrained whispered normal speech utterances compared pretraining normal speech Figure 4 Figure 4 bottom also shows UMAP 39 vi sualization whispered normal speech melspectrogram space common speech unit space The corresponding periods two speeches connected lines As shown figure differences reduced common speech unit space except long distances Although speech feature values whispered normal voices different assume addressed self supervised pretraining utterances similar linguistic standpoints represented similar units We speculate learning method extract common pronunciations normal whispered utterances similar selfsupervised pretraining methods extract common pronunciations utterances different speakers ASR system CNN Encoder012341112Output transformer intermediate layersSTU pretrained normal voicesSTUpretrained whispered normal voices121086420Mean vector differenceWhisper Normal voicesTransformer Layer 1Transformer Layer 2Transformer Layer 3Transformer Layer 4Transformer Layer 11Transformer Layer 12DifferencereductionNormalWhisperdiffMelspectrogramSpeech UnitsMelspectrogramSpeech unit CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany JRekimoto Figure 5 Comparison FastSpeech2 49 UTS decoders FastSpeech2 needs predict duration phoneme whereas UTS accepts common speech unit duration This eliminates duration predictor length regulator LR figure Phoneme embedding also eliminated common speech unit consist discrete tokens Figure 6 STU training 1 UTS training 2 The UTS learns decode speech units target speech using wave data target voice frozen STU No text la beled dataset required Notably WESPER trained labeled corpus pretrained normal whispered voices Figure 7 shows current WESPER speech input configurations We tested headset b directional microphone four ar rayed MEMS Microelectromechanical systems microphones 52 c mobile phone microphone popguard avoid whis pering pop noise soundproofing material reduce ambient noise 5 EVALUATION The WESPER mechanism allows whispertonormal conversion independent input speaker Here evaluate conversion Figure 7 WESPER speech input device headset b ar ray directional microphone c cell phone microphone popprotection soundproofing material quality three aspects considering whispertonormal conver sion voice reconstruction people speech hearing disorders 51 Quality WhispertoNormal Conversion To evaluate quality converted speech recruited 50 genderbalanced participants online using Prolific crowdsourc ing system 25 18 fluent English Each participant listened four sets normal speech whispered speech WESPERconverted whispered speech 12 voices total webbased user interface rated utterances 5point Mean Opinion Score MOS questionnaires examples voices included attached video We used LJSpeechtrained WESPER voice target voice transcription sentence voices avoid differ ences impressions based sentence content The results presented Figure 8 Figure 8 shows sults MOS evaluation The WESPERconverted voice showed score normal whispered voices It con firmed WESPER conversion improved MOS original whispered speech ùëù 001 pairwise ttest Cohen ‚Äô effect size 054 Figure 8 b shows responses question ‚Äú Is voice hoarse normal ‚Äù clear improvement voices converted WESPER ùëù 001 Figure 8 c shows responses question ‚Äú Is voice using consistent artic ulation standard intonation prosody ‚Äù Here WESPER whisper showed almost equal results Notably WESPER affect naturalness prosody Considering speech con verted WESPER generated unittospeech module vocoder result indicates WESPER preserve natural prosody original speech Multiple Stimuli Hidden Reference Anchor MUSHRA eval uation In addition MOS test evaluated speech quality us ing Multiple Stimuli Hidden Reference Anchor MUSHRA MUSHRA method evaluating perceived quality audio defined ITUR Recommendation BS15343 56 MUSHRA uses anchor audio audios evaluator expected Energy PredictorPitch PredictorTransformer LayersMelSpectrogramb UnittoSpeech UTS DecoderMelSpectrogram DecoderEnergy PredictorPitch PredictorTransformer LayersMelSpectrogramMelSpectrogram DecoderPhoneme EmbeddingDuration PredictorLRCommon Speech Unita FastSpeech2PhonemePositional EncodingPositional EncodingSpeech Unit Unit Speechtarget voiceno text labelslossPredicted melspectrogramTarget melspectrogramSpeech unitMelspectrogramconversionpitch energyextractionPredictedPitch energylossfrozentrainingSpeech Unit Acoustic unit discovery systemeg KMeans MFCCHuBERTintermediate layerNormal voicesWhisper voicestrainingSelfsupervised learning masked token prediction1unpairedSTU trainingUTS training2abcpop shieldsoundproof material WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany Figure 8 Quality whispertonormal conversion Mean Opinion Scores MOS normal whispered WESPER converted whispered voices b Hoarsenormal voice rating c natural prosody rating se standard error ùëù 001 ttest ns significant model train finetune test WER CER BLEU Google trained Google wTIMITN wTIMITW WESPERwTIMITW HuBERT base wTIMITN Librispeech Librispeech wTIMITW librispeech wTIMITNW wTIMITW 1155 4470 2668 2106 3306 1375 466 2838 1270 817 1545 547 076 034 052 054 038 070 wTIMITN wTIMIT 36 normal voice Google Google Cloud SpeechtoText 24 wTIMITW wTIMIT whisper voice WESPER‚Ä¢ WESPER converted ‚Ä¢ HuBERT HuBERT base model pretrained Librispeech 42 wTIMITNW Table 2 Whispered voice recognition accuracy Google Cloud SpeechtoText 24 reference ASR The recognition rate whispered voice normal ASR high however result converting whispered voice normal voice using WESPER recognized recognition rate improved Notably WESPER trained labeled data pretrained unlabeled unpaired normal whispered voices assign ratings 0 100 audio comparing anchor audio reference For rating participants play voice many times like Because presence anchor hidden reference audio MUSHRA considered reliable MOS We recruited 50 genderbalanced Englishspeaking participants age 18 via Internet using Prolific crowdsourcing system 25 We used javascriptbased MUSHRA testing tool 28 online evaluation Figure 9 system available https githubcomrkmtmushraJSprolific We used whisper normal voice samples previous MOS test The participants ‚Äô responses collected Internet The results presented Figure 10 It revalidates results MOS evaluation ùëù 001 pairwise ttest effect size 057 According evaluations draw following con clusions ‚Ä¢ WESPER convert whispered voices normal voices ‚Ä¢ Speechdata converted WESPER better MOS whispered source voice Figure 9 An example MUSHRA evaluation web inter faces nsabc CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany JRekimoto Figure 10 Speech quality evaluation whispered WESPER converted voices MUSHRA MUltiple Stimuli Hidden Reference Anchor ùëù 001 ttest ‚Ä¢ WESPER preserved natural prosody source whispered voice 52 Speech Recognition Accuracy There two possible ways use WESPER speech recognizer including speech recognition WESPER recognition speech converted WESPER speech recognizers In former method WESPER model based HuBERT pre trained whispered normal speech finetuned using whispered corpus text inferred whispered voice In latter case whispered speech converted WESPER con trol speechenabled device If whispered speech converted normal speech existing speechenabled devices used immediately without need modify whispered speech Using existing speech recognizer google cloud speechto text 24 reference measured speech recognition accu racy normal speech whispered speech whispered speech converted WESPER The wTIMIT corpus used measurements wTIMIT TIMITcompliant transcription containing normal whispered speech labels This corpus evaluated recogni tion accuracy whispered speech converted WESPER using recognition accuracy normal whispered speech baseline The results summarized Table 2 terms word error rate WER character error rate CER well bilingual eval uation understudy BLEU As shown table recognition accuracy high whispered speech directly rec ognized WER4470 accuracy improved WER2668 conversion WESPER When tested wTIMITW HuBERTbase pretrained Librispeech wTIMITNW shows better results google cloud speechtotext HuBERT WER3306 Google WER4470 It speculated effect pretraining mixture whis pered normal speech without finetuning whispered voice may contribute accuracy ASR Figure 11 Speech quality evaluation people speech disorders ranked 5point MOS ùëù 001 ùëù 005 S1S5 speakers VFP Vocal Fold Polyps SD Spasmodic Dys phonia Figure 12 Speech quality evaluation people vocal disabilities ranked MUSHRA ùëù 001 S1S5 speakers VFP Vocal Fold Polyps SD Spasmodic Dysphonia Notably WESPER trained labeled corpus It pre trained whispered normal speeches improves speech recognition accuracy Therefore speech recognition via WESPER conversion require preparation corpus whispered speech specific unspecified speakers 53 Evaluations Speech Reconstruction People Speech Disorders An important goal WESPER reconstruction atypical speech people speech disorders hearing impairments This makes speech understandable people familiar individual speech patterns Dysphonia involuntary hoarse breathy strained sound ing voice low volume pitch There various causes including spasms polyps vocal tract causes If vocal cords removed throat cancer causes voice becomes extremely difficult produce For hearing impair ment even vocal cords affected control vocal cords becomes difficult resulting dysphonia Electrolarynx used mechanically vibrate throat vocalization ns WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany fluent German addition English reference sentence used German The results presented Figures 11 12 13 14 Figure 11 shows results terms MOS For SD VFP WESPER converted voices higher MOS scores ùëù 001 effect size066 MUSHRA scores ùëù 001 effect size079 Figure 13 shows responses question ‚Äú Is voice hoarse normal ‚Äù clear improvement WESPERconverted voices compared original VFP SD voices ùëù 001 Figure 14 shows answers question ‚Äú Does voice use consistent articulation standard intonation prosody ‚Äù Here WESPER converted original voices showed nearly equal scores although WESPERconverted voices showed slightly better scores It assumed WESPER affect prosody original speech According evaluations make following con clusions ‚Ä¢ WESPERconverted voices people VFP SD speech disabilities showed better quality This suggests WESPER improve quality speech people con ditions terms intelligibility people unfamiliar individual speech patterns ‚Ä¢ Similarly WESPER improve naturalness original VFP SD speech ‚Ä¢ WESPER could also preserve natural prosody source speech Notably test performed German sentences Although WESPER pretraining performed English speech German speech prosody source speech preserved MOSs improved Therefore result may demonstrate languageindependence ability WESPER model The attached video shows example whisper normal conversion Japanese Because wav2vec 20 base model STU also shown languageindependent pretraining perfor mance 9 languageindependent ability WESPER could feature worth evaluating future 54 Speech Reconstruction Evaluation People Hearing Impairment Finally evaluated effect speech reconstruction WES PER people hearing impairments People hearing loss hear speech others therefore tend difficulty speaking way easily understood general speakers However vocal organs mal speech different characteristics people dysphasia We used ‚Äú corpus deaf speech acoustic speech production research ‚Äù 40 Utterances five speakers one hearing speaker four hearing impaired used We recruited 50 evaluation participants using Prolific 25 evenly balanced terms gender fluent English speak ers age 18 We asked participants perform 5point ranking utterance terms MOS hoarsenormal natural prosody The results presented Figures 15 16 17 18 These results indicate MOS MUSHRA rating scores Figure 13 HoarseNormal assessments utterances peo ple speech disorders ùëù 001 Figure 14 Speech prosody consistency utterances peo ple speech disorders ns significant people vocal cord damage sound produced artificial pitch conversion deterministic large devia tion normal vocalization The communication deficit caused dysphonia serious problem elimination voice conversion technology great social value To investigate quality improvement WESPER voice conversion evaluated speech utterances people two types speech disorders described Vocal Fold Polyps refer VFPs VFPs among common benign lesions larynx affecting quality voice production Spasmodic Dysphonia refer SD This also called laryngeal dystonia SD another common neurological disorder affects voice speech It lifelong condition causes spasms muscles produce voice We used Saarbruecken Voice Database SVD corpus 1 45 sample utterances people VFP SD The SVD commonly used corpus voices people speech disorders It contains recordings vowel utterances recording Ger man reference sentence ‚Äú Guten Morgen wie geht es Ihnen ‚Äù ‚Äú Good morning ‚Äù This sentence used evaluation Fifty participants recruited evaluation using Prolific 25 The recruited participants evenly balanced terms gender age 18 The participants nsnsnsnsnsns CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany JRekimoto Figure 15 Speech quality evaluation people hear ing impairment Speech quality ranked 5point MOS S1 ‚Äô S4 ‚Äô speakers ùëù 001 ùëù 005 Figure 18 Speech natural prosody consistency people hearing impairment ùëù 005 The results experiments summarized Table 3 6 DISCUSSIONS Combination WESPER UserDependent Finetuning The sults evaluation experiments indicated degree improvement speech people hearing impairments less speech people dysphasia There fore assume former significant prosodic varia tions Although primary purpose study achieve speakerindependent speech conversion pretraining alone plan conduct additional experiments investigate whether conversion performance improved applying smallscale finetuning speaker Even case believe speech pairs necessary text transcription required A pair whispered hoarse speech utterances normal speech converted com mon speech unit STU used instead text transcription case ASR finetuning Audio Input Device Suitable Whispered Speech As demonstrated study whispered hoarse voices converted normal voices In practice selection appropriate audio input device would important We currently testing proposed approach normal headsets directional array microphone designed smart speakers obtained good results For wearable devices nonaudible murmur NAM mi crophone detects skin vibrations could used inaudible utterances 20 Combining noise reduction techniques 7 61 another important future direction Philips Dyson also developing masks provide pow ered respiratory ventilation protect air pollution infectious diseases 13 26 A microphone placed inside masks pick whispered voice would give effect almost equivalent silent speech HumanAI Integration This research concerns machinelearning techniques converting whispered speech unspecified speakers normal speech In practice however established users noticed similar whispered utterances easily converted normal speech whereas others difficult They tried speak relying machine learning user Figure 16 Speech quality evaluation people hear ing impairment Speech quality ranked MUSHRA S1 ‚Äô S4 ‚Äô speakers ùëù 005 Figure 17 HoarseNormal assessments utterances peo ple hearing impairment ùëù 001 ùëù 005 higher converted WESPER ùëù 005 effect size 045 MOS ùëù 005 effect size 021 MUSHRA degree improvement less speakers voice disorders In addition prosody ratings significantly lower speech hearing impaired compared speech hearing speakers Therefore results indicate people hearing loss may difficulty controlling prosody speaking nsnsnsns WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany Speaker Type Normal Whisper WESPERWhisper MOS MUSHRA Q1 416 9001 415 290 5133 327 397 6364 379 VFP SD S1 WESPERS1 S2 WESPERS2 S3 WESPERS3 S4 WESPERS4 S5 WESPERS4 Speakers speech disorders 1913 3572 3666 5753 4219 6274 2751 4445 4986 6553 3520 5330 272 342 308 396 320 412 224 288 354 394 293 364 All WESPERAll 154 378 220 416 200 436 120 336 268 390 193 389 Q2 399 331 343 300 324 340 376 326 360 236 220 354 364 312 329 Normal S1 ‚Äô WESPERS1 ‚Äô S2 ‚Äô WESPERS2 ‚Äô S3 ‚Äô WESPERS3 ‚Äô S4 ‚Äô WESPERS4 ‚Äô All WESPERAll Speakers hearing impaired 9175 4305 4747 2076 2005 2830 3507 3586 4123 3207 3621 368 230 261 161 193 203 239 238 306 208 250 Mean Opinion Score MUltiple Stimuli Hidden Reference Anchor ‚Äò Is voice hoarse normal ‚Äô 375 251 297 184 255 222 293 259 335 212 236 376 229 238 174 206 219 239 226 262 228 294 ‚Äò Is voice use consistent articulation standard intonation prosody ‚Äô MOS MUSHRA Q1 Q2 WESPER‚Ä¢ WESPER converted ‚Ä¢ VFP SD S1S5 S1 ‚Äô S4 ‚Äô Vocal Fold Polyps speakers Spasmodic Dysphonia speakers speaker IDs Table 3 Summary Voice Conversion Quality Evaluation MUSHRA 100point score others5point score side This interaction suggests machine learning unilaterally extend human capabilities synergistic effects achieved learning human side well This seen actual example humanAI integration 47 vocalization 7 CONCLUSION In study proposed WESPER mechanism real time conversion whispered speech normal speech We con firmed common speech unit obtained selfsupervised learning even acoustic features whispered mal speech different Speech reproduction learned speech data arbitrary target speakers without using text labels There need train user need parallel data whispered normal speech From speech units WESPER reconstruct utterances target speaker requires unlabeled speech data target speakers We con firmed quality converted speech improved prosody speech preserved Additionally reported evaluation results reconstructing speech utterances people speech disorders hearing impairments\n"
     ]
    }
   ],
   "source": [
    "# If the text is too long, reduce it with GPT-3.5\n",
    "if len(text.split()) > 8000:\n",
    "    text = reduce_length_with_gpt35(text)\n",
    "\n",
    "else:\n",
    "    print(\"### input text\", text)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### input text WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions Jun Rekimoto The University Tokyo 731 Hongo Bunkyoku Tokyo Japan Sony Computer Science Laboratories Kyoto 131 Hontorocho Shimogyoku Kyotoshi Kyoto Japan rekimotoacmorg 3 2 0 2 r M 3 D S c 1 v 9 3 6 1 0 3 0 3 2 v X r Figure 1 WESPER realtime whispertonormal speech conversion mechanism consisting speechtounit STU en coder generates common speech units whispered normal utterances using selfsupervised pretraining unittospeech UTS decoder recovers speech speech units It achieves userindependent voice conversion real time ABSTRACT Recognizing whispered speech converting normal speech creates many possibilities speech interaction Because sound pressure whispered speech significantly lower normal speech used semisilent speech interaction public places without audible others Converting whispers normal speech also improves speech quality people speech hearing impairments However conventional speech con version techniques provide sufficient conversion quality require speakerdependent datasets consisting pairs whis pered normal speech utterances To address problems propose WESPER zeroshot realtime whispertonormal speech conversion mechanism based selfsupervised learning WESPER consists speechtounit STU encoder generates hidden speech units common whispered normal speech unittospeech UTS decoder reconstructs speech encoded speech units Unlike existing methods conversion userindependent require paired dataset whis pered normal speech The UTS decoder reconstruct speech target speaker ‚Äô voice speech units requires unlabeled target speaker ‚Äô speech data We confirmed quality speech converted whisper improved preserving natural prosody Additionally confirmed effectiveness proposed approach perform speech construction people speech hearing disabilities CCS CONCEPTS ‚Ä¢ Humancentered computing ‚Üí Soundbased input output Interface design prototyping Mobile devices ‚Ä¢ Computing method ologies ‚Üí Neural networks Permission make digital hard copies part work personal classroom use granted without fee provided copies made distributed profit commercial advantage copies bear notice full citation first page Copyrights components work owned others authors must honored Abstracting credit permitted To copy otherwise republish post servers redistribute lists requires prior specific permission andor fee Request permissions permissionsacmorg CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany ¬© 2023 Copyright held ownerauthors Publication rights licensed ACM ACM ISBN 97814503942152304 1500 httpsdoiorg10114535445483580706 KEYWORDS speech interaction whispered voice whispered voice conversion silent speech artificial intelligence neural networks selfsupervised learning ACM Reference Format Jun Rekimoto 2023 WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions In Proceedings 2023 CHI Conference Human Factors Computing Systems CHI ‚Äô 23 Energy PredictorPitch PredictorWhisper Hoarse Normal VoicesTransformer LayersMelSpectrogramReconstructed Normal VoiceCNN EncoderTransformer LayerTransformer LayerTransformer LayerTransformer LayerCommon speech unitsbetween whisper normal voices Modified FastSpeech2SpeechtoUnit STU EncoderUnittoSpeech UTS DecoderModified HuBERT Vocoder HiFiGANMelSpectrogram Decoder256one per 20msSpeaker independentincluding persons speech hearing disabilitiesCTC Speech RecogRecognized text CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany JRekimoto April 23‚Äì28 2023 Hamburg Germany ACM New York NY USA 13 pages httpsdoiorg10114535445483580706 1 INTRODUCTION Although voice interaction systems widely deployed typically easy use presence people Using voice commands public places may socially unaccept able risk confidential information leaked In addition speaking conference call uncomfortable people vicinity compromise confidentiality conversation To overcome problems various silent speech input tech niques developed 3 31 32 50 51 54 however methods require special sensors achieved high accu racy speech recognition remaining instead level recog nizing predefined commands Conversion unconditioned silent speech normal vocal utterances also achieved Additionally silent speech could employed capture utter ances people speech hearing impairments However owing abovementioned limitations existing methods meet requirements practical accessibility aids In contrast silent speech focus whispered speech Whis pers sufficiently low sound pressure ensure confidentiality whispering almost equivalent silent speech Whispering captured ordinary microphone require special sensor configuration People speech disorders still speak whisper hoarse voice although vocal organs may injured extracted thus voices might recognized To address issues propose WESPER realtime zeroshot whispertonormal voice conversion method based selfsupervised learning It designed perform speaker inde pendent conversion whispered speech normal speech There need peruser training paired datasets whispered normal utterances required The proposed architec ture consists speechtounit STU encoder pretrained normal whispered speeches unittospeech UTS decoder generates target voice speech units Figure 1 By pretraining unpaired whisper normal voices STU trained reduce difference normal whispered utterances output common speech unit The UTS decoder trained speech data specific speaker without accompanying text labels If person ‚Äô voice used conversion performed restore whispered voice person ‚Äô normal voice even another person ‚Äô voice Because encoder decoder operate nonautoregressive manner entire system operates realtime Therefore applied teleconferencing example conference participant speak whispered voice converted real time others listen speech played back normal voice Figure 2 shows melspectrogram examples whispertonormal voice conversion using proposed method More conversion results demonstrated accompanying video The contributions study summarized follows Figure 2 WESPER conversion result Left whispered speech Middle whispered speech converted WESPER Right Normal speech speaker tran scription ‚Ä¢ We propose realtime speakerindependent vocabularyfree whispertonormal speech conversion method trained unpaired whispers normal speech ‚Ä¢ The target voice learned voice samples specific speaker without text transcription ‚Ä¢ We experimentally confirmed improvement performance normal speakers dysarthric hearingimpaired speakers 2 RELATED WORK 21 Research Silent Whispered Speech Various studies silent speech developed methods rec ognize user ‚Äô silent utterances silent commands using various sensor configurations including lip reading EMG Electromyogra phy ultrasound 3 31 32 50 51 54 However systems require special dataset perform training increases need special sensors prevents technologies ing widely used Hence systems offer limited vocabulary less 100 commands available typically around 30 Al though silent speech recognition sometimes identified possible approach help people dysphonia methods interpret unrestricted free speech owing vocabulary limitation Whispered speech similar characteristics silent speech preserving social acceptability public spaces However widely adopted given ordinary mi crophones used SilentVoice 15 speech interaction technique uses gressive speech utterance made inhaling The amplitude ingressive speech low considered variation silent speech It requires microphonelike device placed close mouth training required users speak correctly ingressive speech A custom corpus ingressive speech text transcriptions also required Research whisper voice recognition 6 11 14 18 previ ously conducted The Alexa smart speaker supports whisper mode 23 In mode user whispers Alexa Alexa responds whisper 10 DualVoice 48 also proposed method per form endtoend recognition whispered voices based self supervised speech recognition system based wav2vec20 2 HuBERT 21 They also proposed interaction technique WhisperWESPER convertedWhisperNormalsecsecsecHz WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany model training data peruser dataset conversion normal voice whisper speech recognition silent speech ex3 31 32 50 51 54 special paireddataset dedicated sensors required Parotoron 4 paired whispernormal voice required DualVoice 48 SilentVoice 15 CycleGANVC 30 MSpeCNet 37 AGANW2SC 16 WESPER labeled unpaired whisper normal voice labeled silentvoice custom unpaired unlabeled whisper normal voice paired whispernormal voice paired whispernormal voice unpaired unlabeled whisper normal voice required required required required required required YES NO NO YES YES YES YES Table 1 Research silent whispered speech YES commands characters YES voice conversion YES YES NO NO NO YES voice conversion distinguish whispered normal utterances WESPER combined DualVoice selectively convert user ‚Äô whispered voice 22 Speech Conversion Normal speech conversion technologies developed convert one voiced utterance another 19 29 30 43 46 however conversion quality methods remains unsatisfactory applied whispertonormal voice conversion Recently several machine learningbased whispertonormal voice conversion techniques investigated 41 To com pare techniques important consider quality conversion required characteristics dataset used training A paired whispernormal voice dataset ie dataset containing whispered normal versions utter ance labeled whispered voice dataset ie dataset containing whispered utterances accompanied text labels require significant amount effort prepare Conversely unpaired unlabeled whisper voice dataset used ie dataset containing whisper utterances without corresponding normal versions labels effort required prepare dataset low Attentionguided generative adversarial network whisper normal speech conversion AGANW2SC GANbased whisper normal speech conversion 16 It converts whispered voice represented melspectrogram corresponding normal speech ‚Äô melspectrogram It based GAN attention map used conversion It requires paired whispernormal speech dataset MSpeCNet 37 autoencoderbased multi domain voice conversion supports whispertonormal conver sion It also requires paired whispernormal voice dataset Moreover whispered speech converted text using speech recognition generate normal speech using texttospeech methods 22 However approach requires labeled dataset whispered speech recognition prosodic information con tained whispered utterances lost intermediate text representation convey information Parrotron 4 speech conversion system designed im prove speech speakers dysplasia It based encoderdecoder model conforms texttospeech system Tacotron 58 however differs Tacotron input output data formatted melspectrograms It also quires paired source target speeches making construction required dataset relatively difficult Phonetic posteriorgrams PPGs 55 intermediate informa tion obtained automatic speech recognition PPGs used manytoone speaker conversion represent articulation spoken content speaker independently To knowledge effects PPGs whispered voice investigated Our method also uses intermediated speech units require textbased corpus case automatic speech recognition ASR PPGs In contrast proposed system requires samples un paired target source speech require accompa nying text transcriptions thus making datasets easy prepare independent target language The characteristics related technologies summarized Table 1 We also demonstrate whisper normal conversion exam ples including NMSEDiscoGAN 53 MSpeCNet 37 CycleGAN VC 30 AGANW2SC 16 WESPER httpswesperproj githubio 23 Self Supervised Representation Learning Speech Recently combining pretraining selfsupervised representa tion learning unlabeled speech data finetuning labeled speech data attracted attention These systems primarily CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany JRekimoto intended speech recognition applications also applied perform speaker language emotion recogni tion 44 59 In particular pretraining method hiddenunit BERT hu BERT 21 similar masked language model used bidirectional encoder representations transformers BERT 12 natural language processing It designed mask part input estimate corresponding expression features rest input With pretraining model learn acoustic properties input data characteristics speech For use ASR pretraining finetuning performed small amount audio data text transcriptions A projection layer connectionist temporal classification CTC layer 17 added generate text transcriptions audio waveforms As reported 2 59 selfsupervised ASR achieved speech recognition accuracy comparable conventional stateoftheart ASRs finetuning small amount labeled speech data Therefore architecture could suitable recogniz ing whispered voice recognition limited whispered speech corpora WESPER unique uses pretraining reduce dif ference latent vectors encoding whispered normal utterances 24 TextlessNLP Recently textfree speech processing speech conversion meth ods developed Through selfsupervised learning systems derive latent representations speech data accompanied text transcriptions TextlessNLP34 35 Au dioLM 5 use text transcriptions phoneme symbols speech processing systems use discrete units constructed selfsupervised learning Soft discrete unit another approach textless speech processing 57 Our proposed method also uses nondiscrete vectors latent speech representations obtained selfsupervised learning explicitly use text transcriptions phoneme symbols Our research unique demonstrate whispered normal speech represented similar speech units selfsupervised learning In addition method also designed work seamlessly speech generation system later stages 3 THE WESPER VOICE CONVERSION MODEL WESPER consists STU encoder UTS decoder The STU converts whispered normal speech common speech units The UTS converts common speech units melspectrograms reconstructed speech vocoder WESPER char acterized fact common speech units utterance similar although STU pretrained unpaired whispered normal speech trained paired set whispered normal utterances The details method used train model described Figure 3 Overview STU pretraining Unpaired whis pered normal speech used pretraining The trans former layer trained estimating discrete units masked input The projection layer transformer layers generates 256dimensional vectors one every 20 ms used common speech units 31 SpeechtoUnit STU Encoder The STU encoder takes audio waveform input outputs units It based HuBERT 21 selfsupervised neural network speech pretrains large amount unlabeled speech BERT 12like manner learns recover relevant parts partially masked speech features thereby acquires speech language model For purpose want generate speech units identical possible whispered normal speech To achieve pretrained STU mixture whispered normal speech utterances used normal English speech many speakers Librispeech 960h dataset 42 Librispeech speech data mechanically converted whispered voice us ing LPCbased audio conversion tool 60 Additionally used wTIMIT speech dataset normal whispered speech 36 speech length 58 h Figure 3 shows pretraining process STU detail Only unpaired whispered normal speech used pretraining The transformer layer trained estimating discrete units masked input Similar HuBERT discrete target units first generated kmeans clustering input speech data first stage kmeans clustering outputs transformer intermediate layer In experiment used 100 discrete units The projection layer transformer layers generates sequence 256dimensional vectors one per 20 ms used common speech units In STU 12 transformer layers placed front CNN feature extractor Figure 1 After pretraining comparing output layer confirmed 1 difference Transformer Layer 11Transformer Layer 12ProjectionLEN 768LEN 256Label Embedding256 KLEN KLENCrossEntropy LossCosinesimilarityTo Unit Speech CNN EncoderWhisper Normal voicesunpairedTransformer Layer 1MaskingAcoustic unit discoveryeg KMeans MFCC HuBERTintermediate layerCommon Speech UnitK Discrete unitsK100Predict hidden units masked locations25620ms WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany In addition finetune STU transcriptattached wTIMIT Librispeech datasets adding CTC layer 17 48 therefore STU could used ASR recognize whis pered normal speeches 32 UTS Decoder The UTS decoder takes speech units generated STU input reconstructs target normal speech It based nonautoregressive texttospeech TTS system FastSpeech2 49 While original FastSpeech2 includes embedding layer takes text phoneme tokens transforms sequence vector embeddings eliminate layer UTS takes speech units direct input The original FastSpeech2 also includes duration estimator estimates duration phoneme token length regulator adjusts number internal vectors according estimated duration These parts also eliminated purpose STU generates speech units constant rate When learning original FastSpeech2 necessary pro vide duration phoneme corpus ground truth external tool Montreal Forced Aligner 38 Because limitation FastSpeech2 ‚Äô learning languagedependent Conversely UTS require duration estimation UTS languageindependent The output UTS melspectrogram FastSpeech2 A vocoder HiFiGAN 33 converts actual speech waveform In regular TTS system target speech corresponding text labels needed training By contrast proposed UTS requires target speech text labels The target speech passed STU obtain sequence speech units associated speech waveform used train UTS Figure 6 In experiment UTS trained using speech data single speaker LJSpeech 27 data speakers taken narration data 4 SYSTEM CONFIGURATION We designed WESPER model using PyTorch framework The STU based HuBERT UTS based modified implementation FastSpeech2 PyTorch implementation 8 We used Librispeech wTIMIT include normal whispered speech pretraining With dual NVIDIA R6000 pretraining took 48 h UTS training took 26 h target voice The total processing time required perform conversion approximately 120th actual speech duration single NVIDIA R6000 110th Apple M1 Max CPU The actual quality conversion demonstrated attached video We designed two types interfaces The first operated pushtotalk style user first speaks whispered normal voice pressing button Thereafter ton released speech waveform recorded time sent speechconversion neural networks result immediately played back The detected nonaudio period input speech automatically converted speech segments without additional user input Figure 4 Comparison whisperednormal voice differ ences The comparison made using normal speech speech whispering transformations Above An STU pretrained normal whispered speeches produced fewer differences pretrained normal speech As transformer layer deepens difference tween two decreases Middle There difference melspectrogram decreased speech units Bottom UMAP 39 visualizations differences tween whisper normal voices melspectrogram speechunit ‚Äô space speech unit values whispered normal voices de creased increasing layer depth 2 difference decreased STU pretrained whispered normal speech utterances compared pretraining normal speech Figure 4 Figure 4 bottom also shows UMAP 39 vi sualization whispered normal speech melspectrogram space common speech unit space The corresponding periods two speeches connected lines As shown figure differences reduced common speech unit space except long distances Although speech feature values whispered normal voices different assume addressed self supervised pretraining utterances similar linguistic standpoints represented similar units We speculate learning method extract common pronunciations normal whispered utterances similar selfsupervised pretraining methods extract common pronunciations utterances different speakers ASR system CNN Encoder012341112Output transformer intermediate layersSTU pretrained normal voicesSTUpretrained whispered normal voices121086420Mean vector differenceWhisper Normal voicesTransformer Layer 1Transformer Layer 2Transformer Layer 3Transformer Layer 4Transformer Layer 11Transformer Layer 12DifferencereductionNormalWhisperdiffMelspectrogramSpeech UnitsMelspectrogramSpeech unit CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany JRekimoto Figure 5 Comparison FastSpeech2 49 UTS decoders FastSpeech2 needs predict duration phoneme whereas UTS accepts common speech unit duration This eliminates duration predictor length regulator LR figure Phoneme embedding also eliminated common speech unit consist discrete tokens Figure 6 STU training 1 UTS training 2 The UTS learns decode speech units target speech using wave data target voice frozen STU No text la beled dataset required Notably WESPER trained labeled corpus pretrained normal whispered voices Figure 7 shows current WESPER speech input configurations We tested headset b directional microphone four ar rayed MEMS Microelectromechanical systems microphones 52 c mobile phone microphone popguard avoid whis pering pop noise soundproofing material reduce ambient noise 5 EVALUATION The WESPER mechanism allows whispertonormal conversion independent input speaker Here evaluate conversion Figure 7 WESPER speech input device headset b ar ray directional microphone c cell phone microphone popprotection soundproofing material quality three aspects considering whispertonormal conver sion voice reconstruction people speech hearing disorders 51 Quality WhispertoNormal Conversion To evaluate quality converted speech recruited 50 genderbalanced participants online using Prolific crowdsourc ing system 25 18 fluent English Each participant listened four sets normal speech whispered speech WESPERconverted whispered speech 12 voices total webbased user interface rated utterances 5point Mean Opinion Score MOS questionnaires examples voices included attached video We used LJSpeechtrained WESPER voice target voice transcription sentence voices avoid differ ences impressions based sentence content The results presented Figure 8 Figure 8 shows sults MOS evaluation The WESPERconverted voice showed score normal whispered voices It con firmed WESPER conversion improved MOS original whispered speech ùëù 001 pairwise ttest Cohen ‚Äô effect size 054 Figure 8 b shows responses question ‚Äú Is voice hoarse normal ‚Äù clear improvement voices converted WESPER ùëù 001 Figure 8 c shows responses question ‚Äú Is voice using consistent artic ulation standard intonation prosody ‚Äù Here WESPER whisper showed almost equal results Notably WESPER affect naturalness prosody Considering speech con verted WESPER generated unittospeech module vocoder result indicates WESPER preserve natural prosody original speech Multiple Stimuli Hidden Reference Anchor MUSHRA eval uation In addition MOS test evaluated speech quality us ing Multiple Stimuli Hidden Reference Anchor MUSHRA MUSHRA method evaluating perceived quality audio defined ITUR Recommendation BS15343 56 MUSHRA uses anchor audio audios evaluator expected Energy PredictorPitch PredictorTransformer LayersMelSpectrogramb UnittoSpeech UTS DecoderMelSpectrogram DecoderEnergy PredictorPitch PredictorTransformer LayersMelSpectrogramMelSpectrogram DecoderPhoneme EmbeddingDuration PredictorLRCommon Speech Unita FastSpeech2PhonemePositional EncodingPositional EncodingSpeech Unit Unit Speechtarget voiceno text labelslossPredicted melspectrogramTarget melspectrogramSpeech unitMelspectrogramconversionpitch energyextractionPredictedPitch energylossfrozentrainingSpeech Unit Acoustic unit discovery systemeg KMeans MFCCHuBERTintermediate layerNormal voicesWhisper voicestrainingSelfsupervised learning masked token prediction1unpairedSTU trainingUTS training2abcpop shieldsoundproof material WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany Figure 8 Quality whispertonormal conversion Mean Opinion Scores MOS normal whispered WESPER converted whispered voices b Hoarsenormal voice rating c natural prosody rating se standard error ùëù 001 ttest ns significant model train finetune test WER CER BLEU Google trained Google wTIMITN wTIMITW WESPERwTIMITW HuBERT base wTIMITN Librispeech Librispeech wTIMITW librispeech wTIMITNW wTIMITW 1155 4470 2668 2106 3306 1375 466 2838 1270 817 1545 547 076 034 052 054 038 070 wTIMITN wTIMIT 36 normal voice Google Google Cloud SpeechtoText 24 wTIMITW wTIMIT whisper voice WESPER‚Ä¢ WESPER converted ‚Ä¢ HuBERT HuBERT base model pretrained Librispeech 42 wTIMITNW Table 2 Whispered voice recognition accuracy Google Cloud SpeechtoText 24 reference ASR The recognition rate whispered voice normal ASR high however result converting whispered voice normal voice using WESPER recognized recognition rate improved Notably WESPER trained labeled data pretrained unlabeled unpaired normal whispered voices assign ratings 0 100 audio comparing anchor audio reference For rating participants play voice many times like Because presence anchor hidden reference audio MUSHRA considered reliable MOS We recruited 50 genderbalanced Englishspeaking participants age 18 via Internet using Prolific crowdsourcing system 25 We used javascriptbased MUSHRA testing tool 28 online evaluation Figure 9 system available https githubcomrkmtmushraJSprolific We used whisper normal voice samples previous MOS test The participants ‚Äô responses collected Internet The results presented Figure 10 It revalidates results MOS evaluation ùëù 001 pairwise ttest effect size 057 According evaluations draw following con clusions ‚Ä¢ WESPER convert whispered voices normal voices ‚Ä¢ Speechdata converted WESPER better MOS whispered source voice Figure 9 An example MUSHRA evaluation web inter faces nsabc CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany JRekimoto Figure 10 Speech quality evaluation whispered WESPER converted voices MUSHRA MUltiple Stimuli Hidden Reference Anchor ùëù 001 ttest ‚Ä¢ WESPER preserved natural prosody source whispered voice 52 Speech Recognition Accuracy There two possible ways use WESPER speech recognizer including speech recognition WESPER recognition speech converted WESPER speech recognizers In former method WESPER model based HuBERT pre trained whispered normal speech finetuned using whispered corpus text inferred whispered voice In latter case whispered speech converted WESPER con trol speechenabled device If whispered speech converted normal speech existing speechenabled devices used immediately without need modify whispered speech Using existing speech recognizer google cloud speechto text 24 reference measured speech recognition accu racy normal speech whispered speech whispered speech converted WESPER The wTIMIT corpus used measurements wTIMIT TIMITcompliant transcription containing normal whispered speech labels This corpus evaluated recogni tion accuracy whispered speech converted WESPER using recognition accuracy normal whispered speech baseline The results summarized Table 2 terms word error rate WER character error rate CER well bilingual eval uation understudy BLEU As shown table recognition accuracy high whispered speech directly rec ognized WER4470 accuracy improved WER2668 conversion WESPER When tested wTIMITW HuBERTbase pretrained Librispeech wTIMITNW shows better results google cloud speechtotext HuBERT WER3306 Google WER4470 It speculated effect pretraining mixture whis pered normal speech without finetuning whispered voice may contribute accuracy ASR Figure 11 Speech quality evaluation people speech disorders ranked 5point MOS ùëù 001 ùëù 005 S1S5 speakers VFP Vocal Fold Polyps SD Spasmodic Dys phonia Figure 12 Speech quality evaluation people vocal disabilities ranked MUSHRA ùëù 001 S1S5 speakers VFP Vocal Fold Polyps SD Spasmodic Dysphonia Notably WESPER trained labeled corpus It pre trained whispered normal speeches improves speech recognition accuracy Therefore speech recognition via WESPER conversion require preparation corpus whispered speech specific unspecified speakers 53 Evaluations Speech Reconstruction People Speech Disorders An important goal WESPER reconstruction atypical speech people speech disorders hearing impairments This makes speech understandable people familiar individual speech patterns Dysphonia involuntary hoarse breathy strained sound ing voice low volume pitch There various causes including spasms polyps vocal tract causes If vocal cords removed throat cancer causes voice becomes extremely difficult produce For hearing impair ment even vocal cords affected control vocal cords becomes difficult resulting dysphonia Electrolarynx used mechanically vibrate throat vocalization ns WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany fluent German addition English reference sentence used German The results presented Figures 11 12 13 14 Figure 11 shows results terms MOS For SD VFP WESPER converted voices higher MOS scores ùëù 001 effect size066 MUSHRA scores ùëù 001 effect size079 Figure 13 shows responses question ‚Äú Is voice hoarse normal ‚Äù clear improvement WESPERconverted voices compared original VFP SD voices ùëù 001 Figure 14 shows answers question ‚Äú Does voice use consistent articulation standard intonation prosody ‚Äù Here WESPER converted original voices showed nearly equal scores although WESPERconverted voices showed slightly better scores It assumed WESPER affect prosody original speech According evaluations make following con clusions ‚Ä¢ WESPERconverted voices people VFP SD speech disabilities showed better quality This suggests WESPER improve quality speech people con ditions terms intelligibility people unfamiliar individual speech patterns ‚Ä¢ Similarly WESPER improve naturalness original VFP SD speech ‚Ä¢ WESPER could also preserve natural prosody source speech Notably test performed German sentences Although WESPER pretraining performed English speech German speech prosody source speech preserved MOSs improved Therefore result may demonstrate languageindependence ability WESPER model The attached video shows example whisper normal conversion Japanese Because wav2vec 20 base model STU also shown languageindependent pretraining perfor mance 9 languageindependent ability WESPER could feature worth evaluating future 54 Speech Reconstruction Evaluation People Hearing Impairment Finally evaluated effect speech reconstruction WES PER people hearing impairments People hearing loss hear speech others therefore tend difficulty speaking way easily understood general speakers However vocal organs mal speech different characteristics people dysphasia We used ‚Äú corpus deaf speech acoustic speech production research ‚Äù 40 Utterances five speakers one hearing speaker four hearing impaired used We recruited 50 evaluation participants using Prolific 25 evenly balanced terms gender fluent English speak ers age 18 We asked participants perform 5point ranking utterance terms MOS hoarsenormal natural prosody The results presented Figures 15 16 17 18 These results indicate MOS MUSHRA rating scores Figure 13 HoarseNormal assessments utterances peo ple speech disorders ùëù 001 Figure 14 Speech prosody consistency utterances peo ple speech disorders ns significant people vocal cord damage sound produced artificial pitch conversion deterministic large devia tion normal vocalization The communication deficit caused dysphonia serious problem elimination voice conversion technology great social value To investigate quality improvement WESPER voice conversion evaluated speech utterances people two types speech disorders described Vocal Fold Polyps refer VFPs VFPs among common benign lesions larynx affecting quality voice production Spasmodic Dysphonia refer SD This also called laryngeal dystonia SD another common neurological disorder affects voice speech It lifelong condition causes spasms muscles produce voice We used Saarbruecken Voice Database SVD corpus 1 45 sample utterances people VFP SD The SVD commonly used corpus voices people speech disorders It contains recordings vowel utterances recording Ger man reference sentence ‚Äú Guten Morgen wie geht es Ihnen ‚Äù ‚Äú Good morning ‚Äù This sentence used evaluation Fifty participants recruited evaluation using Prolific 25 The recruited participants evenly balanced terms gender age 18 The participants nsnsnsnsnsns CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany JRekimoto Figure 15 Speech quality evaluation people hear ing impairment Speech quality ranked 5point MOS S1 ‚Äô S4 ‚Äô speakers ùëù 001 ùëù 005 Figure 18 Speech natural prosody consistency people hearing impairment ùëù 005 The results experiments summarized Table 3 6 DISCUSSIONS Combination WESPER UserDependent Finetuning The sults evaluation experiments indicated degree improvement speech people hearing impairments less speech people dysphasia There fore assume former significant prosodic varia tions Although primary purpose study achieve speakerindependent speech conversion pretraining alone plan conduct additional experiments investigate whether conversion performance improved applying smallscale finetuning speaker Even case believe speech pairs necessary text transcription required A pair whispered hoarse speech utterances normal speech converted com mon speech unit STU used instead text transcription case ASR finetuning Audio Input Device Suitable Whispered Speech As demonstrated study whispered hoarse voices converted normal voices In practice selection appropriate audio input device would important We currently testing proposed approach normal headsets directional array microphone designed smart speakers obtained good results For wearable devices nonaudible murmur NAM mi crophone detects skin vibrations could used inaudible utterances 20 Combining noise reduction techniques 7 61 another important future direction Philips Dyson also developing masks provide pow ered respiratory ventilation protect air pollution infectious diseases 13 26 A microphone placed inside masks pick whispered voice would give effect almost equivalent silent speech HumanAI Integration This research concerns machinelearning techniques converting whispered speech unspecified speakers normal speech In practice however established users noticed similar whispered utterances easily converted normal speech whereas others difficult They tried speak relying machine learning user Figure 16 Speech quality evaluation people hear ing impairment Speech quality ranked MUSHRA S1 ‚Äô S4 ‚Äô speakers ùëù 005 Figure 17 HoarseNormal assessments utterances peo ple hearing impairment ùëù 001 ùëù 005 higher converted WESPER ùëù 005 effect size 045 MOS ùëù 005 effect size 021 MUSHRA degree improvement less speakers voice disorders In addition prosody ratings significantly lower speech hearing impaired compared speech hearing speakers Therefore results indicate people hearing loss may difficulty controlling prosody speaking nsnsnsns WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany Speaker Type Normal Whisper WESPERWhisper MOS MUSHRA Q1 416 9001 415 290 5133 327 397 6364 379 VFP SD S1 WESPERS1 S2 WESPERS2 S3 WESPERS3 S4 WESPERS4 S5 WESPERS4 Speakers speech disorders 1913 3572 3666 5753 4219 6274 2751 4445 4986 6553 3520 5330 272 342 308 396 320 412 224 288 354 394 293 364 All WESPERAll 154 378 220 416 200 436 120 336 268 390 193 389 Q2 399 331 343 300 324 340 376 326 360 236 220 354 364 312 329 Normal S1 ‚Äô WESPERS1 ‚Äô S2 ‚Äô WESPERS2 ‚Äô S3 ‚Äô WESPERS3 ‚Äô S4 ‚Äô WESPERS4 ‚Äô All WESPERAll Speakers hearing impaired 9175 4305 4747 2076 2005 2830 3507 3586 4123 3207 3621 368 230 261 161 193 203 239 238 306 208 250 Mean Opinion Score MUltiple Stimuli Hidden Reference Anchor ‚Äò Is voice hoarse normal ‚Äô 375 251 297 184 255 222 293 259 335 212 236 376 229 238 174 206 219 239 226 262 228 294 ‚Äò Is voice use consistent articulation standard intonation prosody ‚Äô MOS MUSHRA Q1 Q2 WESPER‚Ä¢ WESPER converted ‚Ä¢ VFP SD S1S5 S1 ‚Äô S4 ‚Äô Vocal Fold Polyps speakers Spasmodic Dysphonia speakers speaker IDs Table 3 Summary Voice Conversion Quality Evaluation MUSHRA 100point score others5point score side This interaction suggests machine learning unilaterally extend human capabilities synergistic effects achieved learning human side well This seen actual example humanAI integration 47 vocalization 7 CONCLUSION In study proposed WESPER mechanism real time conversion whispered speech normal speech We con firmed common speech unit obtained selfsupervised learning even acoustic features whispered mal speech different Speech reproduction learned speech data arbitrary target speakers without using text labels There need train user need parallel data whispered normal speech From speech units WESPER reconstruct utterances target speaker requires unlabeled speech data target speakers We con firmed quality converted speech improved prosody speech preserved Additionally reported evaluation results reconstructing speech utterances people speech disorders hearing impairments\n",
      "#### GPT Ë´ñÊñáÂêç:WESPER: „Çº„É≠„Ç∑„Éß„ÉÉ„Éà„É™„Ç¢„É´„Çø„Ç§„É†„Åß„ÅÆÂõÅ„Åç„Åã„ÇâÈÄöÂ∏∏„ÅÆÈü≥Â£∞„Å∏„ÅÆÂ§âÊèõ„Å®ÂõÅ„Åç„Å´Âü∫„Å•„ÅèÈü≥Â£∞ÂØæË©±\n",
      "„Ç≠„Éº„ÉØ„Éº„Éâ:Èü≥Â£∞ÂØæË©±„ÄÅÂõÅ„ÅçÈü≥Â£∞„ÄÅÂõÅ„ÅçÈü≥Â£∞Â§âÊèõ„ÄÅÁÑ°Èü≥Èü≥Â£∞„ÄÅ‰∫∫Â∑•Áü•ËÉΩ„ÄÅ„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÄÅËá™Â∑±ÊïôÂ∏´„ÅÇ„ÇäÂ≠¶Áøí\n",
      "Ë™≤È°å:ÂõÅ„ÅçÈü≥Â£∞„ÇíÈÄöÂ∏∏„ÅÆÈü≥Â£∞„Å´Â§âÊèõ„Åô„Çã„Åì„Å®„Åß„ÄÅÂÖ¨ÂÖ±„ÅÆÂ†¥ÊâÄ„Åß‰ªñ‰∫∫„Å´ËÅû„Åì„Åà„Åö„Å´Èü≥Â£∞ÂØæË©±„ÇíË°å„ÅÜ„Åì„Å®„ÅåÂèØËÉΩ„Å®„Å™„Çã„ÄÇ„Åó„Åã„Åó„ÄÅÊó¢Â≠ò„ÅÆÈü≥Â£∞Â§âÊèõÊäÄË°ì„ÅØÂ§âÊèõÂìÅË≥™„ÅåÂçÅÂàÜ„Åß„Å™„Åè„ÄÅÂõÅ„Åç„Å®ÈÄöÂ∏∏„ÅÆÈü≥Â£∞„ÅÆ„Éö„Ç¢„ÇíÂê´„ÇÄË©±ËÄÖ‰æùÂ≠ò„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅåÂøÖË¶Å„Åß„ÅÇ„Çã„ÄÇ\n",
      "ÊâãÊ≥ï:Ëá™Â∑±ÊïôÂ∏´„ÅÇ„ÇäÂ≠¶Áøí„Å´Âü∫„Å•„Åè„Çº„É≠„Ç∑„Éß„ÉÉ„Éà„É™„Ç¢„É´„Çø„Ç§„É†„ÅÆÂõÅ„Åç„Åã„ÇâÈÄöÂ∏∏„ÅÆÈü≥Â£∞„Å∏„ÅÆÂ§âÊèõÊ©üÊßã„ÄÅWESPER„ÇíÊèêÊ°à„ÄÇWESPER„ÅØ„ÄÅÂõÅ„Åç„Å®ÈÄöÂ∏∏„ÅÆÁô∫Ë©±„ÅÆÂÖ±ÈÄö„ÅÆÈü≥Â£∞Âçò‰Ωç„ÇíÁîüÊàê„Åô„ÇãÈü≥Â£∞„Åã„ÇâÂçò‰Ωç(STU)„Ç®„É≥„Ç≥„Éº„ÉÄ„Å®„ÄÅÈü≥Â£∞Âçò‰Ωç„Åã„ÇâÈü≥Â£∞„ÇíÂæ©ÂÖÉ„Åô„ÇãÂçò‰Ωç„Åã„ÇâÈü≥Â£∞(UTS)„Éá„Ç≥„Éº„ÉÄ„Åã„ÇâÊßãÊàê„Åï„Çå„Çã„ÄÇ\n",
      "ÁµêÊûú:WESPER„ÅØ„ÄÅ„É¶„Éº„Ç∂„ÉºÁã¨Á´ã„ÅÆÈü≥Â£∞Â§âÊèõ„Çí„É™„Ç¢„É´„Çø„Ç§„É†„ÅßÂÆüÁèæ„Åó„ÄÅÈü≥Â£∞ÂìÅË≥™„ÇíÂêë‰∏ä„Åï„Åõ„Çã„Åì„Å®„ÅåÁ¢∫Ë™ç„Åï„Çå„Åü„ÄÇ„Åæ„Åü„ÄÅÈü≥Â£∞„ÇÑËÅ¥Ë¶öÈöúÂÆ≥„ÇíÊåÅ„Å§‰∫∫„ÄÖ„ÅÆÈü≥Â£∞ÂÜçÊßãÁØâ„Å´ÂØæ„Åô„ÇãÊèêÊ°àÊâãÊ≥ï„ÅÆÊúâÂäπÊÄß„ÇÇÁ¢∫Ë™ç„Åï„Çå„Åü„ÄÇ\n",
      "**** Ë´ñÊñáÂêç:WESPER: „Çº„É≠„Ç∑„Éß„ÉÉ„Éà„É™„Ç¢„É´„Çø„Ç§„É†„Åß„ÅÆÂõÅ„Åç„Åã„ÇâÈÄöÂ∏∏„ÅÆÈü≥Â£∞„Å∏„ÅÆÂ§âÊèõ„Å®ÂõÅ„Åç„Å´Âü∫„Å•„ÅèÈü≥Â£∞ÂØæË©±\n",
      "**** „Ç≠„Éº„ÉØ„Éº„Éâ:Èü≥Â£∞ÂØæË©±„ÄÅÂõÅ„ÅçÈü≥Â£∞„ÄÅÂõÅ„ÅçÈü≥Â£∞Â§âÊèõ„ÄÅÁÑ°Èü≥Èü≥Â£∞„ÄÅ‰∫∫Â∑•Áü•ËÉΩ„ÄÅ„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÄÅËá™Â∑±ÊïôÂ∏´„ÅÇ„ÇäÂ≠¶Áøí\n",
      "**** Ë™≤È°å:ÂõÅ„ÅçÈü≥Â£∞„ÇíÈÄöÂ∏∏„ÅÆÈü≥Â£∞„Å´Â§âÊèõ„Åô„Çã„Åì„Å®„Åß„ÄÅÂÖ¨ÂÖ±„ÅÆÂ†¥ÊâÄ„Åß‰ªñ‰∫∫„Å´ËÅû„Åì„Åà„Åö„Å´Èü≥Â£∞ÂØæË©±„ÇíË°å„ÅÜ„Åì„Å®„ÅåÂèØËÉΩ„Å®„Å™„Çã„ÄÇ„Åó„Åã„Åó„ÄÅÊó¢Â≠ò„ÅÆÈü≥Â£∞Â§âÊèõÊäÄË°ì„ÅØÂ§âÊèõÂìÅË≥™„ÅåÂçÅÂàÜ„Åß„Å™„Åè„ÄÅÂõÅ„Åç„Å®ÈÄöÂ∏∏„ÅÆÈü≥Â£∞„ÅÆ„Éö„Ç¢„ÇíÂê´„ÇÄË©±ËÄÖ‰æùÂ≠ò„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅåÂøÖË¶Å„Åß„ÅÇ„Çã„ÄÇ\n",
      "**** ÊâãÊ≥ï:Ëá™Â∑±ÊïôÂ∏´„ÅÇ„ÇäÂ≠¶Áøí„Å´Âü∫„Å•„Åè„Çº„É≠„Ç∑„Éß„ÉÉ„Éà„É™„Ç¢„É´„Çø„Ç§„É†„ÅÆÂõÅ„Åç„Åã„ÇâÈÄöÂ∏∏„ÅÆÈü≥Â£∞„Å∏„ÅÆÂ§âÊèõÊ©üÊßã„ÄÅWESPER„ÇíÊèêÊ°à„ÄÇWESPER„ÅØ„ÄÅÂõÅ„Åç„Å®ÈÄöÂ∏∏„ÅÆÁô∫Ë©±„ÅÆÂÖ±ÈÄö„ÅÆÈü≥Â£∞Âçò‰Ωç„ÇíÁîüÊàê„Åô„ÇãÈü≥Â£∞„Åã„ÇâÂçò‰Ωç(STU)„Ç®„É≥„Ç≥„Éº„ÉÄ„Å®„ÄÅÈü≥Â£∞Âçò‰Ωç„Åã„ÇâÈü≥Â£∞„ÇíÂæ©ÂÖÉ„Åô„ÇãÂçò‰Ωç„Åã„ÇâÈü≥Â£∞(UTS)„Éá„Ç≥„Éº„ÉÄ„Åã„ÇâÊßãÊàê„Åï„Çå„Çã„ÄÇ\n",
      "**** ÁµêÊûú:WESPER„ÅØ„ÄÅ„É¶„Éº„Ç∂„ÉºÁã¨Á´ã„ÅÆÈü≥Â£∞Â§âÊèõ„Çí„É™„Ç¢„É´„Çø„Ç§„É†„ÅßÂÆüÁèæ„Åó„ÄÅÈü≥Â£∞ÂìÅË≥™„ÇíÂêë‰∏ä„Åï„Åõ„Çã„Åì„Å®„ÅåÁ¢∫Ë™ç„Åï„Çå„Åü„ÄÇ„Åæ„Åü„ÄÅÈü≥Â£∞„ÇÑËÅ¥Ë¶öÈöúÂÆ≥„ÇíÊåÅ„Å§‰∫∫„ÄÖ„ÅÆÈü≥Â£∞ÂÜçÊßãÁØâ„Å´ÂØæ„Åô„ÇãÊèêÊ°àÊâãÊ≥ï„ÅÆÊúâÂäπÊÄß„ÇÇÁ¢∫Ë™ç„Åï„Çå„Åü„ÄÇ\n",
      "Dict by ChatGPT {'title_jp': 'WESPER: „Çº„É≠„Ç∑„Éß„ÉÉ„Éà„É™„Ç¢„É´„Çø„Ç§„É†„Åß„ÅÆÂõÅ„Åç„Åã„ÇâÈÄöÂ∏∏„ÅÆÈü≥Â£∞„Å∏„ÅÆÂ§âÊèõ„Å®ÂõÅ„Åç„Å´Âü∫„Å•„ÅèÈü≥Â£∞ÂØæË©±', 'keywords': 'Èü≥Â£∞ÂØæË©±„ÄÅÂõÅ„ÅçÈü≥Â£∞„ÄÅÂõÅ„ÅçÈü≥Â£∞Â§âÊèõ„ÄÅÁÑ°Èü≥Èü≥Â£∞„ÄÅ‰∫∫Â∑•Áü•ËÉΩ„ÄÅ„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÄÅËá™Â∑±ÊïôÂ∏´„ÅÇ„ÇäÂ≠¶Áøí', 'problem': 'ÂõÅ„ÅçÈü≥Â£∞„ÇíÈÄöÂ∏∏„ÅÆÈü≥Â£∞„Å´Â§âÊèõ„Åô„Çã„Åì„Å®„Åß„ÄÅÂÖ¨ÂÖ±„ÅÆÂ†¥ÊâÄ„Åß‰ªñ‰∫∫„Å´ËÅû„Åì„Åà„Åö„Å´Èü≥Â£∞ÂØæË©±„ÇíË°å„ÅÜ„Åì„Å®„ÅåÂèØËÉΩ„Å®„Å™„Çã„ÄÇ„Åó„Åã„Åó„ÄÅÊó¢Â≠ò„ÅÆÈü≥Â£∞Â§âÊèõÊäÄË°ì„ÅØÂ§âÊèõÂìÅË≥™„ÅåÂçÅÂàÜ„Åß„Å™„Åè„ÄÅÂõÅ„Åç„Å®ÈÄöÂ∏∏„ÅÆÈü≥Â£∞„ÅÆ„Éö„Ç¢„ÇíÂê´„ÇÄË©±ËÄÖ‰æùÂ≠ò„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅåÂøÖË¶Å„Åß„ÅÇ„Çã„ÄÇ', 'method': 'Ëá™Â∑±ÊïôÂ∏´„ÅÇ„ÇäÂ≠¶Áøí„Å´Âü∫„Å•„Åè„Çº„É≠„Ç∑„Éß„ÉÉ„Éà„É™„Ç¢„É´„Çø„Ç§„É†„ÅÆÂõÅ„Åç„Åã„ÇâÈÄöÂ∏∏„ÅÆÈü≥Â£∞„Å∏„ÅÆÂ§âÊèõÊ©üÊßã„ÄÅWESPER„ÇíÊèêÊ°à„ÄÇWESPER„ÅØ„ÄÅÂõÅ„Åç„Å®ÈÄöÂ∏∏„ÅÆÁô∫Ë©±„ÅÆÂÖ±ÈÄö„ÅÆÈü≥Â£∞Âçò‰Ωç„ÇíÁîüÊàê„Åô„ÇãÈü≥Â£∞„Åã„ÇâÂçò‰Ωç(STU)„Ç®„É≥„Ç≥„Éº„ÉÄ„Å®„ÄÅÈü≥Â£∞Âçò‰Ωç„Åã„ÇâÈü≥Â£∞„ÇíÂæ©ÂÖÉ„Åô„ÇãÂçò‰Ωç„Åã„ÇâÈü≥Â£∞(UTS)„Éá„Ç≥„Éº„ÉÄ„Åã„ÇâÊßãÊàê„Åï„Çå„Çã„ÄÇ', 'result': 'WESPER„ÅØ„ÄÅ„É¶„Éº„Ç∂„ÉºÁã¨Á´ã„ÅÆÈü≥Â£∞Â§âÊèõ„Çí„É™„Ç¢„É´„Çø„Ç§„É†„ÅßÂÆüÁèæ„Åó„ÄÅÈü≥Â£∞ÂìÅË≥™„ÇíÂêë‰∏ä„Åï„Åõ„Çã„Åì„Å®„ÅåÁ¢∫Ë™ç„Åï„Çå„Åü„ÄÇ„Åæ„Åü„ÄÅÈü≥Â£∞„ÇÑËÅ¥Ë¶öÈöúÂÆ≥„ÇíÊåÅ„Å§‰∫∫„ÄÖ„ÅÆÈü≥Â£∞ÂÜçÊßãÁØâ„Å´ÂØæ„Åô„ÇãÊèêÊ°àÊâãÊ≥ï„ÅÆÊúâÂäπÊÄß„ÇÇÁ¢∫Ë™ç„Åï„Çå„Åü„ÄÇ'}\n",
      "{'title_jp': 'WESPER: „Çº„É≠„Ç∑„Éß„ÉÉ„Éà„É™„Ç¢„É´„Çø„Ç§„É†„Åß„ÅÆÂõÅ„Åç„Åã„ÇâÈÄöÂ∏∏„ÅÆÈü≥Â£∞„Å∏„ÅÆÂ§âÊèõ„Å®ÂõÅ„Åç„Å´Âü∫„Å•„ÅèÈü≥Â£∞ÂØæË©±', 'keywords': 'Èü≥Â£∞ÂØæË©±„ÄÅÂõÅ„ÅçÈü≥Â£∞„ÄÅÂõÅ„ÅçÈü≥Â£∞Â§âÊèõ„ÄÅÁÑ°Èü≥Èü≥Â£∞„ÄÅ‰∫∫Â∑•Áü•ËÉΩ„ÄÅ„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÄÅËá™Â∑±ÊïôÂ∏´„ÅÇ„ÇäÂ≠¶Áøí', 'problem': 'ÂõÅ„ÅçÈü≥Â£∞„ÇíÈÄöÂ∏∏„ÅÆÈü≥Â£∞„Å´Â§âÊèõ„Åô„Çã„Åì„Å®„Åß„ÄÅÂÖ¨ÂÖ±„ÅÆÂ†¥ÊâÄ„Åß‰ªñ‰∫∫„Å´ËÅû„Åì„Åà„Åö„Å´Èü≥Â£∞ÂØæË©±„ÇíË°å„ÅÜ„Åì„Å®„ÅåÂèØËÉΩ„Å®„Å™„Çã„ÄÇ„Åó„Åã„Åó„ÄÅÊó¢Â≠ò„ÅÆÈü≥Â£∞Â§âÊèõÊäÄË°ì„ÅØÂ§âÊèõÂìÅË≥™„ÅåÂçÅÂàÜ„Åß„Å™„Åè„ÄÅÂõÅ„Åç„Å®ÈÄöÂ∏∏„ÅÆÈü≥Â£∞„ÅÆ„Éö„Ç¢„ÇíÂê´„ÇÄË©±ËÄÖ‰æùÂ≠ò„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅåÂøÖË¶Å„Åß„ÅÇ„Çã„ÄÇ', 'method': 'Ëá™Â∑±ÊïôÂ∏´„ÅÇ„ÇäÂ≠¶Áøí„Å´Âü∫„Å•„Åè„Çº„É≠„Ç∑„Éß„ÉÉ„Éà„É™„Ç¢„É´„Çø„Ç§„É†„ÅÆÂõÅ„Åç„Åã„ÇâÈÄöÂ∏∏„ÅÆÈü≥Â£∞„Å∏„ÅÆÂ§âÊèõÊ©üÊßã„ÄÅWESPER„ÇíÊèêÊ°à„ÄÇWESPER„ÅØ„ÄÅÂõÅ„Åç„Å®ÈÄöÂ∏∏„ÅÆÁô∫Ë©±„ÅÆÂÖ±ÈÄö„ÅÆÈü≥Â£∞Âçò‰Ωç„ÇíÁîüÊàê„Åô„ÇãÈü≥Â£∞„Åã„ÇâÂçò‰Ωç(STU)„Ç®„É≥„Ç≥„Éº„ÉÄ„Å®„ÄÅÈü≥Â£∞Âçò‰Ωç„Åã„ÇâÈü≥Â£∞„ÇíÂæ©ÂÖÉ„Åô„ÇãÂçò‰Ωç„Åã„ÇâÈü≥Â£∞(UTS)„Éá„Ç≥„Éº„ÉÄ„Åã„ÇâÊßãÊàê„Åï„Çå„Çã„ÄÇ', 'result': 'WESPER„ÅØ„ÄÅ„É¶„Éº„Ç∂„ÉºÁã¨Á´ã„ÅÆÈü≥Â£∞Â§âÊèõ„Çí„É™„Ç¢„É´„Çø„Ç§„É†„ÅßÂÆüÁèæ„Åó„ÄÅÈü≥Â£∞ÂìÅË≥™„ÇíÂêë‰∏ä„Åï„Åõ„Çã„Åì„Å®„ÅåÁ¢∫Ë™ç„Åï„Çå„Åü„ÄÇ„Åæ„Åü„ÄÅÈü≥Â£∞„ÇÑËÅ¥Ë¶öÈöúÂÆ≥„ÇíÊåÅ„Å§‰∫∫„ÄÖ„ÅÆÈü≥Â£∞ÂÜçÊßãÁØâ„Å´ÂØæ„Åô„ÇãÊèêÊ°àÊâãÊ≥ï„ÅÆÊúâÂäπÊÄß„ÇÇÁ¢∫Ë™ç„Åï„Çå„Åü„ÄÇ'}\n"
     ]
    }
   ],
   "source": [
    "# Generate summary with GPT-4\n",
    "summary = get_summary(text)\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac@gmail.com/My Drive/2023/summarize_arxv/playground.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac%40gmail.com/My%20Drive/2023/summarize_arxv/playground.ipynb#W5sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mdict\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac%40gmail.com/My%20Drive/2023/summarize_arxv/playground.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# test extract_elements\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac%40gmail.com/My%20Drive/2023/summarize_arxv/playground.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m extract_elements(summary)\n",
      "\u001b[1;32m/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac@gmail.com/My Drive/2023/summarize_arxv/playground.ipynb Cell 8\u001b[0m in \u001b[0;36mextract_elements\u001b[0;34m(summary)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac%40gmail.com/My%20Drive/2023/summarize_arxv/playground.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_elements\u001b[39m(summary):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac%40gmail.com/My%20Drive/2023/summarize_arxv/playground.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mdict\u001b[39m \u001b[39m=\u001b[39m {}    \n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac%40gmail.com/My%20Drive/2023/summarize_arxv/playground.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m summary\u001b[39m.\u001b[39;49msplit(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac%40gmail.com/My%20Drive/2023/summarize_arxv/playground.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m****\u001b[39m\u001b[39m\"\u001b[39m, b)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac%40gmail.com/My%20Drive/2023/summarize_arxv/playground.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         \u001b[39mif\u001b[39;00m b\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mË´ñÊñáÂêç\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "def extract_elements(summary):\n",
    "    dict = {}    \n",
    "    for b in summary.split('\\n'):\n",
    "        print(\"****\", b)\n",
    "        if b.startswith(\"Ë´ñÊñáÂêç\"):\n",
    "            dict['title_jp'] = b[4:].lstrip()\n",
    "        if b.startswith(\"„Ç≠„Éº„ÉØ„Éº„Éâ\"):\n",
    "            dict['keywords'] = b[6:].lstrip()\n",
    "        if b.startswith(\"Ë™≤È°å\"):\n",
    "            dict['problem'] = b[3:].lstrip()\n",
    "        if b.startswith(\"ÊâãÊ≥ï\"):\n",
    "            dict['method'] = b[3:].lstrip()\n",
    "        if b.startswith(\"ÁµêÊûú\"):\n",
    "            dict['result'] = b[3:].lstrip()\n",
    "    print(\"Dict by ChatGPT\", dict)\n",
    "    return dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac@gmail.com/My Drive/2023/summarize_arxv/playground.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac%40gmail.com/My%20Drive/2023/summarize_arxv/playground.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# test extract_elements\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac%40gmail.com/My%20Drive/2023/summarize_arxv/playground.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m extract_elements(summary)\n",
      "\u001b[1;32m/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac@gmail.com/My Drive/2023/summarize_arxv/playground.ipynb Cell 9\u001b[0m in \u001b[0;36mextract_elements\u001b[0;34m(summary)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac%40gmail.com/My%20Drive/2023/summarize_arxv/playground.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_elements\u001b[39m(summary):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac%40gmail.com/My%20Drive/2023/summarize_arxv/playground.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mdict\u001b[39m \u001b[39m=\u001b[39m {}    \n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac%40gmail.com/My%20Drive/2023/summarize_arxv/playground.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m summary\u001b[39m.\u001b[39;49msplit(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac%40gmail.com/My%20Drive/2023/summarize_arxv/playground.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m****\u001b[39m\u001b[39m\"\u001b[39m, b)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac%40gmail.com/My%20Drive/2023/summarize_arxv/playground.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         \u001b[39mif\u001b[39;00m b\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mË´ñÊñáÂêç\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "# test extract_elements\n",
    "\n",
    "extract_elements(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make 10 pairs of questions and answers\n",
    "\n",
    "schema = {\n",
    "  \"type\": \"object\",\n",
    "  \"pair1\": {\n",
    "    \"question\": {\"type\": \"string\"},\n",
    "    \"answer\": {\"type\": \"string\"},\n",
    "  },\n",
    "  \"pair2\": {\n",
    "    \"question\": {\"type\": \"string\"},\n",
    "    \"answer\": {\"type\": \"string\"},\n",
    "  },\n",
    "  \"pair3\": {\n",
    "    \"question\": {\"type\": \"string\"},\n",
    "    \"answer\": {\"type\": \"string\"},\n",
    "  },\n",
    "  \"pair4\": {\n",
    "    \"question\": {\"type\": \"string\"},\n",
    "    \"answer\": {\"type\": \"string\"},\n",
    "  },\n",
    "  \"pair5\": {\n",
    "    \"question\": {\"type\": \"string\"},\n",
    "    \"answer\": {\"type\": \"string\"},\n",
    "  },\n",
    "  \"pair6\": {\n",
    "    \"question\": {\"type\": \"string\"},\n",
    "    \"answer\": {\"type\": \"string\"},\n",
    "  },\n",
    "  \"pair7\": {\n",
    "    \"question\": {\"type\": \"string\"},\n",
    "    \"answer\": {\"type\": \"string\"},\n",
    "  },\n",
    "  \"pair8\": {\n",
    "    \"question\": {\"type\": \"string\"},\n",
    "    \"answer\": {\"type\": \"string\"},\n",
    "  },\n",
    "  \"pair9\": {\n",
    "    \"question\": {\"type\": \"string\"},\n",
    "    \"answer\": {\"type\": \"string\"},\n",
    "  },\n",
    "  \"pair10\": {\n",
    "    \"question\": {\"type\": \"string\"},\n",
    "    \"answer\": {\"type\": \"string\"},\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"title\": \"WESPER: Zeroshot Realtime Whisper-to-Normal Voice Conversion for Whisper-based Speech Interactions\",\n",
      "\"author\": [\"Jun Rekimoto\"],\n",
      "\"jornal/conference\": \"CHI ‚Äô 23\",\n",
      "\"year\": \"2023\",\n",
      "\"abstract\": \"Recognizing whispered speech and converting it to normal speech creates many possibilities for speech interaction. However, conventional speech conversion techniques do not provide sufficient conversion quality and require speaker-dependent datasets. To address these problems, we propose WESPER, a zeroshot realtime whisper-to-normal speech conversion mechanism based on self-supervised learning. WESPER consists of a speech-to-unit (STU) encoder that generates hidden speech units common to whispered and normal speech, and a unit-to-speech (UTS) decoder that reconstructs speech from the encoded speech units. Unlike existing methods, our conversion is user-independent and does not require a paired dataset of whispered and normal speech.\",\n",
      "\"keywords\": \"speech interaction, whispered voice, whispered voice conversion, silent speech, artificial intelligence, neural networks, self-supervised learning\",\n",
      "\"problem\": \"ÂæìÊù•„ÅÆÈü≥Â£∞Â§âÊèõÊäÄË°ì„ÅØ„ÄÅÂ§âÊèõÂìÅË≥™„ÅåÂçÅÂàÜ„Åß„Å™„Åè„ÄÅË©±ËÄÖ‰æùÂ≠ò„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅåÂøÖË¶Å„Åß„ÅÇ„Å£„Åü„ÄÇ\",\n",
      "\"method\": \"WESPER„ÅØ„ÄÅËá™Â∑±ÊïôÂ∏´‰ªò„ÅçÂ≠¶Áøí„Å´Âü∫„Å•„Åè„Çº„É≠„Ç∑„Éß„ÉÉ„Éà„ÅÆ„É™„Ç¢„É´„Çø„Ç§„É†„ÅÆ„Åï„Åï„ÇÑ„Åç„Åã„ÇâÈÄöÂ∏∏„ÅÆÈü≥Â£∞„Å∏„ÅÆÂ§âÊèõ„É°„Ç´„Éã„Ç∫„É†„ÇíÊèêÊ°à„Åô„Çã„ÄÇ\",\n",
      "\"interaction\": \"WESPER„ÅØ„ÄÅ„Åï„Åï„ÇÑ„Åç„Å®ÈÄöÂ∏∏„ÅÆÈü≥Â£∞„Å´ÂÖ±ÈÄö„ÅÆÈö†„Çå„ÅüÈü≥Â£∞„É¶„Éã„ÉÉ„Éà„ÇíÁîüÊàê„Åô„ÇãÈü≥Â£∞„Åã„Çâ„É¶„Éã„ÉÉ„ÉàÔºàSTUÔºâ„Ç®„É≥„Ç≥„Éº„ÉÄ„Å®„ÄÅ„Ç®„É≥„Ç≥„Éº„Éâ„Åï„Çå„ÅüÈü≥Â£∞„É¶„Éã„ÉÉ„Éà„Åã„ÇâÈü≥Â£∞„ÇíÂÜçÊßãÁØâ„Åô„Çã„É¶„Éã„ÉÉ„Éà„Åã„ÇâÈü≥Â£∞ÔºàUTSÔºâ„Éá„Ç≥„Éº„ÉÄ„ÅßÊßãÊàê„Åï„Çå„Å¶„ÅÑ„Çã„ÄÇ\",\n",
      "\"technical_contribution\": \"Êó¢Â≠ò„ÅÆÊñπÊ≥ï„Å®„ÅØÁï∞„Å™„Çä„ÄÅÁßÅ„Åü„Å°„ÅÆÂ§âÊèõ„ÅØ„É¶„Éº„Ç∂„Éº„Å´‰æùÂ≠ò„Åõ„Åö„ÄÅ„Åï„Åï„ÇÑ„Åç„Å®ÈÄöÂ∏∏„ÅÆÈü≥Â£∞„ÅÆ„Éö„Ç¢„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÇíÂøÖË¶Å„Å®„Åó„Åæ„Åõ„Çì„ÄÇ\",\n",
      "\"result\": \"Èü≥Â£∞„ÅÆÂ§âÊèõÂìÅË≥™„ÅåÊîπÂñÑ„Åï„Çå„ÄÅÈü≥Â£∞„ÅÆÊäëÊèö„Åå‰øùÊåÅ„Åï„Çå„Çã„Åì„Å®„ÇíÁ¢∫Ë™ç„Åó„Åæ„Åó„Åü„ÄÇ\",\n",
      "\"github\": \"https://wesperproj.github.io\",\n",
      "\"doi\": \"https://doi.org/10.1145/3544548.3580706\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Analyze the following paper and provide the required information. meta„Éá„Éº„Çø„Å´Èñ¢„Åó„Å¶„ÅØÂÖÉ„ÅÆË®ÄË™û„Çí‰øùÊåÅ„Åõ„ÇàÔºéÂÜÖÂÆπ„Å´Èñ¢„Åô„ÇãÈ†ÖÁõÆ„ÅØÊó•Êú¨Ë™û„ÅßÔºå„Åù„Çå„Åû„Çå„ÅÆÈ†ÖÁõÆ„ÅØÊúÄÂ§ß„Åß„ÇÇ180ÊñáÂ≠ó‰ª•ÂÜÖ„Å´Ë¶ÅÁ¥Ñ„Åõ„Çà.\"\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "  model=\"gpt-4-0613\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\":f\"{prompt}\"},\n",
    "    {\"role\": \"user\", \"content\": \"Analyze the following paper and provide the required information.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"The paper is: [{text}]\"}\n",
    "  ],\n",
    "  functions=[{\"name\": \"analyze_paper\", \"parameters\": schema}],\n",
    "  function_call={\"name\": \"analyze_paper\"},\n",
    "  temperature=0,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.function_call.arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"title\": \"WESPER: Zeroshot Realtime Whisper-to-Normal Voice Conversion for Whisper-based Speech Interactions\",\n",
      "\"author\": [\"Jun Rekimoto\"],\n",
      "\"jornal/conference\": \"CHI ‚Äô 23\",\n",
      "\"year\": \"2023\",\n",
      "\"abstract\": \"Recognizing whispered speech and converting it to normal speech creates many possibilities for speech interaction. Because the sound pressure of whispered speech is significantly lower than normal speech, it can be used for semi-silent speech interaction in public places without being audible to others. Converting whispers to normal speech also improves speech quality for people with speech and hearing impairments. However, conventional speech conversion techniques do not provide sufficient conversion quality and require speaker-dependent datasets consisting of pairs of whispered and normal speech utterances. To address these problems, we propose WESPER, a zero-shot, real-time whisper-to-normal speech conversion mechanism based on self-supervised learning. WESPER consists of a speech-to-unit (STU) encoder that generates hidden speech units common to whispered and normal speech, and a unit-to-speech (UTS) decoder that reconstructs speech from the encoded speech units. Unlike existing methods, our conversion is user-independent and does not require a paired dataset of whispered and normal speech.\",\n",
      "\"keywords\": \"speech interaction, whispered voice, whispered voice conversion, silent speech, artificial intelligence, neural networks, self-supervised learning\",\n",
      "\"problem\": \"Recognizing whispered speech and converting it to normal speech is a challenge due to the significantly lower sound pressure of whispered speech. Conventional speech conversion techniques do not provide sufficient conversion quality and require speaker-dependent datasets consisting of pairs of whispered and normal speech utterances.\",\n",
      "\"method\": \"The proposed solution, WESPER, is a zero-shot, real-time whisper-to-normal speech conversion mechanism based on self-supervised learning. It consists of a speech-to-unit (STU) encoder that generates hidden speech units common to whispered and normal speech, and a unit-to-speech (UTS) decoder that reconstructs speech from the encoded speech units.\",\n",
      "\"interaction\": \"The system operates in real-time and can be applied to teleconferencing, for example, where a conference participant can speak in a whispered voice that is converted in real time for others to listen to as speech played back in a normal voice.\",\n",
      "\"technical_contribution\": \"The technical contribution of this paper is the development of WESPER, a zero-shot, real-time whisper-to-normal speech conversion mechanism based on self-supervised learning. Unlike existing methods, the conversion is user-independent and does not require a paired dataset of whispered and normal speech.\",\n",
      "\"result\": \"The quality of speech converted from whisper improved while preserving natural prosody. The effectiveness of the proposed approach was confirmed through its performance in speech reconstruction for people with speech and hearing disabilities.\",\n",
      "\"github\": \"https://wesperproj.github.io\",\n",
      "\"doi\": \"https://doi.org/10.1145/3544548.3580706\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "json_summary = completion.choices[0].message.function_call.arguments\n",
    "\n",
    "print(json_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(json_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "  json_result = json.loads(completion.choices[0].message.function_call.arguments)\n",
    "\n",
    "except:\n",
    "  print(\"error\")\n",
    "  print(completion.choices[0].message.function_call.arguments)\n",
    "\n",
    "\n",
    "json_result[\"text\"] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'WESPER: Zeroshot Realtime Whisper-to-Normal Voice Conversion for Whisper-based Speech Interactions',\n",
       " 'author': ['Jun Rekimoto'],\n",
       " 'jornal/conference': 'CHI ‚Äô 23',\n",
       " 'year': '2023',\n",
       " 'abstract': 'Recognizing whispered speech and converting it to normal speech creates many possibilities for speech interaction. Because the sound pressure of whispered speech is significantly lower than normal speech, it can be used for semi-silent speech interaction in public places without being audible to others. Converting whispers to normal speech also improves speech quality for people with speech and hearing impairments. However, conventional speech conversion techniques do not provide sufficient conversion quality and require speaker-dependent datasets consisting of pairs of whispered and normal speech utterances. To address these problems, we propose WESPER, a zero-shot, real-time whisper-to-normal speech conversion mechanism based on self-supervised learning. WESPER consists of a speech-to-unit (STU) encoder that generates hidden speech units common to whispered and normal speech, and a unit-to-speech (UTS) decoder that reconstructs speech from the encoded speech units. Unlike existing methods, our conversion is user-independent and does not require a paired dataset of whispered and normal speech.',\n",
       " 'keywords': 'speech interaction, whispered voice, whispered voice conversion, silent speech, artificial intelligence, neural networks, self-supervised learning',\n",
       " 'problem': 'Recognizing whispered speech and converting it to normal speech is a challenge due to the significantly lower sound pressure of whispered speech. Conventional speech conversion techniques do not provide sufficient conversion quality and require speaker-dependent datasets consisting of pairs of whispered and normal speech utterances.',\n",
       " 'method': 'The proposed solution, WESPER, is a zero-shot, real-time whisper-to-normal speech conversion mechanism based on self-supervised learning. It consists of a speech-to-unit (STU) encoder that generates hidden speech units common to whispered and normal speech, and a unit-to-speech (UTS) decoder that reconstructs speech from the encoded speech units.',\n",
       " 'interaction': 'The system operates in real-time and can be applied to teleconferencing, for example, where a conference participant can speak in a whispered voice that is converted in real time for others to listen to as speech played back in a normal voice.',\n",
       " 'technical_contribution': 'The technical contribution of this paper is the development of WESPER, a zero-shot, real-time whisper-to-normal speech conversion mechanism based on self-supervised learning. Unlike existing methods, the conversion is user-independent and does not require a paired dataset of whispered and normal speech.',\n",
       " 'result': 'The quality of speech converted from whisper improved while preserving natural prosody. The effectiveness of the proposed approach was confirmed through its performance in speech reconstruction for people with speech and hearing disabilities.',\n",
       " 'github': 'https://wesperproj.github.io',\n",
       " 'doi': 'https://doi.org/10.1145/3544548.3580706',\n",
       " 'text': 'WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions Jun Rekimoto The University Tokyo 731 Hongo Bunkyoku Tokyo Japan Sony Computer Science Laboratories Kyoto 131 Hontorocho Shimogyoku Kyotoshi Kyoto Japan rekimotoacmorg 3 2 0 2 r M 3 D S c 1 v 9 3 6 1 0 3 0 3 2 v X r Figure 1 WESPER realtime whispertonormal speech conversion mechanism consisting speechtounit STU en coder generates common speech units whispered normal utterances using selfsupervised pretraining unittospeech UTS decoder recovers speech speech units It achieves userindependent voice conversion real time ABSTRACT Recognizing whispered speech converting normal speech creates many possibilities speech interaction Because sound pressure whispered speech significantly lower normal speech used semisilent speech interaction public places without audible others Converting whispers normal speech also improves speech quality people speech hearing impairments However conventional speech con version techniques provide sufficient conversion quality require speakerdependent datasets consisting pairs whis pered normal speech utterances To address problems propose WESPER zeroshot realtime whispertonormal speech conversion mechanism based selfsupervised learning WESPER consists speechtounit STU encoder generates hidden speech units common whispered normal speech unittospeech UTS decoder reconstructs speech encoded speech units Unlike existing methods conversion userindependent require paired dataset whis pered normal speech The UTS decoder reconstruct speech target speaker ‚Äô voice speech units requires unlabeled target speaker ‚Äô speech data We confirmed quality speech converted whisper improved preserving natural prosody Additionally confirmed effectiveness proposed approach perform speech construction people speech hearing disabilities CCS CONCEPTS ‚Ä¢ Humancentered computing ‚Üí Soundbased input output Interface design prototyping Mobile devices ‚Ä¢ Computing method ologies ‚Üí Neural networks Permission make digital hard copies part work personal classroom use granted without fee provided copies made distributed profit commercial advantage copies bear notice full citation first page Copyrights components work owned others authors must honored Abstracting credit permitted To copy otherwise republish post servers redistribute lists requires prior specific permission andor fee Request permissions permissionsacmorg CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany ¬© 2023 Copyright held ownerauthors Publication rights licensed ACM ACM ISBN 97814503942152304 1500 httpsdoiorg10114535445483580706 KEYWORDS speech interaction whispered voice whispered voice conversion silent speech artificial intelligence neural networks selfsupervised learning ACM Reference Format Jun Rekimoto 2023 WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions In Proceedings 2023 CHI Conference Human Factors Computing Systems CHI ‚Äô 23 Energy PredictorPitch PredictorWhisper Hoarse Normal VoicesTransformer LayersMelSpectrogramReconstructed Normal VoiceCNN EncoderTransformer LayerTransformer LayerTransformer LayerTransformer LayerCommon speech unitsbetween whisper normal voices Modified FastSpeech2SpeechtoUnit STU EncoderUnittoSpeech UTS DecoderModified HuBERT Vocoder HiFiGANMelSpectrogram Decoder256one per 20msSpeaker independentincluding persons speech hearing disabilitiesCTC Speech RecogRecognized text CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany JRekimoto April 23‚Äì28 2023 Hamburg Germany ACM New York NY USA 13 pages httpsdoiorg10114535445483580706 1 INTRODUCTION Although voice interaction systems widely deployed typically easy use presence people Using voice commands public places may socially unaccept able risk confidential information leaked In addition speaking conference call uncomfortable people vicinity compromise confidentiality conversation To overcome problems various silent speech input tech niques developed 3 31 32 50 51 54 however methods require special sensors achieved high accu racy speech recognition remaining instead level recog nizing predefined commands Conversion unconditioned silent speech normal vocal utterances also achieved Additionally silent speech could employed capture utter ances people speech hearing impairments However owing abovementioned limitations existing methods meet requirements practical accessibility aids In contrast silent speech focus whispered speech Whis pers sufficiently low sound pressure ensure confidentiality whispering almost equivalent silent speech Whispering captured ordinary microphone require special sensor configuration People speech disorders still speak whisper hoarse voice although vocal organs may injured extracted thus voices might recognized To address issues propose WESPER realtime zeroshot whispertonormal voice conversion method based selfsupervised learning It designed perform speaker inde pendent conversion whispered speech normal speech There need peruser training paired datasets whispered normal utterances required The proposed architec ture consists speechtounit STU encoder pretrained normal whispered speeches unittospeech UTS decoder generates target voice speech units Figure 1 By pretraining unpaired whisper normal voices STU trained reduce difference normal whispered utterances output common speech unit The UTS decoder trained speech data specific speaker without accompanying text labels If person ‚Äô voice used conversion performed restore whispered voice person ‚Äô normal voice even another person ‚Äô voice Because encoder decoder operate nonautoregressive manner entire system operates realtime Therefore applied teleconferencing example conference participant speak whispered voice converted real time others listen speech played back normal voice Figure 2 shows melspectrogram examples whispertonormal voice conversion using proposed method More conversion results demonstrated accompanying video The contributions study summarized follows Figure 2 WESPER conversion result Left whispered speech Middle whispered speech converted WESPER Right Normal speech speaker tran scription ‚Ä¢ We propose realtime speakerindependent vocabularyfree whispertonormal speech conversion method trained unpaired whispers normal speech ‚Ä¢ The target voice learned voice samples specific speaker without text transcription ‚Ä¢ We experimentally confirmed improvement performance normal speakers dysarthric hearingimpaired speakers 2 RELATED WORK 21 Research Silent Whispered Speech Various studies silent speech developed methods rec ognize user ‚Äô silent utterances silent commands using various sensor configurations including lip reading EMG Electromyogra phy ultrasound 3 31 32 50 51 54 However systems require special dataset perform training increases need special sensors prevents technologies ing widely used Hence systems offer limited vocabulary less 100 commands available typically around 30 Al though silent speech recognition sometimes identified possible approach help people dysphonia methods interpret unrestricted free speech owing vocabulary limitation Whispered speech similar characteristics silent speech preserving social acceptability public spaces However widely adopted given ordinary mi crophones used SilentVoice 15 speech interaction technique uses gressive speech utterance made inhaling The amplitude ingressive speech low considered variation silent speech It requires microphonelike device placed close mouth training required users speak correctly ingressive speech A custom corpus ingressive speech text transcriptions also required Research whisper voice recognition 6 11 14 18 previ ously conducted The Alexa smart speaker supports whisper mode 23 In mode user whispers Alexa Alexa responds whisper 10 DualVoice 48 also proposed method per form endtoend recognition whispered voices based self supervised speech recognition system based wav2vec20 2 HuBERT 21 They also proposed interaction technique WhisperWESPER convertedWhisperNormalsecsecsecHz WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany model training data peruser dataset conversion normal voice whisper speech recognition silent speech ex3 31 32 50 51 54 special paireddataset dedicated sensors required Parotoron 4 paired whispernormal voice required DualVoice 48 SilentVoice 15 CycleGANVC 30 MSpeCNet 37 AGANW2SC 16 WESPER labeled unpaired whisper normal voice labeled silentvoice custom unpaired unlabeled whisper normal voice paired whispernormal voice paired whispernormal voice unpaired unlabeled whisper normal voice required required required required required required YES NO NO YES YES YES YES Table 1 Research silent whispered speech YES commands characters YES voice conversion YES YES NO NO NO YES voice conversion distinguish whispered normal utterances WESPER combined DualVoice selectively convert user ‚Äô whispered voice 22 Speech Conversion Normal speech conversion technologies developed convert one voiced utterance another 19 29 30 43 46 however conversion quality methods remains unsatisfactory applied whispertonormal voice conversion Recently several machine learningbased whispertonormal voice conversion techniques investigated 41 To com pare techniques important consider quality conversion required characteristics dataset used training A paired whispernormal voice dataset ie dataset containing whispered normal versions utter ance labeled whispered voice dataset ie dataset containing whispered utterances accompanied text labels require significant amount effort prepare Conversely unpaired unlabeled whisper voice dataset used ie dataset containing whisper utterances without corresponding normal versions labels effort required prepare dataset low Attentionguided generative adversarial network whisper normal speech conversion AGANW2SC GANbased whisper normal speech conversion 16 It converts whispered voice represented melspectrogram corresponding normal speech ‚Äô melspectrogram It based GAN attention map used conversion It requires paired whispernormal speech dataset MSpeCNet 37 autoencoderbased multi domain voice conversion supports whispertonormal conver sion It also requires paired whispernormal voice dataset Moreover whispered speech converted text using speech recognition generate normal speech using texttospeech methods 22 However approach requires labeled dataset whispered speech recognition prosodic information con tained whispered utterances lost intermediate text representation convey information Parrotron 4 speech conversion system designed im prove speech speakers dysplasia It based encoderdecoder model conforms texttospeech system Tacotron 58 however differs Tacotron input output data formatted melspectrograms It also quires paired source target speeches making construction required dataset relatively difficult Phonetic posteriorgrams PPGs 55 intermediate informa tion obtained automatic speech recognition PPGs used manytoone speaker conversion represent articulation spoken content speaker independently To knowledge effects PPGs whispered voice investigated Our method also uses intermediated speech units require textbased corpus case automatic speech recognition ASR PPGs In contrast proposed system requires samples un paired target source speech require accompa nying text transcriptions thus making datasets easy prepare independent target language The characteristics related technologies summarized Table 1 We also demonstrate whisper normal conversion exam ples including NMSEDiscoGAN 53 MSpeCNet 37 CycleGAN VC 30 AGANW2SC 16 WESPER httpswesperproj githubio 23 Self Supervised Representation Learning Speech Recently combining pretraining selfsupervised representa tion learning unlabeled speech data finetuning labeled speech data attracted attention These systems primarily CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany JRekimoto intended speech recognition applications also applied perform speaker language emotion recogni tion 44 59 In particular pretraining method hiddenunit BERT hu BERT 21 similar masked language model used bidirectional encoder representations transformers BERT 12 natural language processing It designed mask part input estimate corresponding expression features rest input With pretraining model learn acoustic properties input data characteristics speech For use ASR pretraining finetuning performed small amount audio data text transcriptions A projection layer connectionist temporal classification CTC layer 17 added generate text transcriptions audio waveforms As reported 2 59 selfsupervised ASR achieved speech recognition accuracy comparable conventional stateoftheart ASRs finetuning small amount labeled speech data Therefore architecture could suitable recogniz ing whispered voice recognition limited whispered speech corpora WESPER unique uses pretraining reduce dif ference latent vectors encoding whispered normal utterances 24 TextlessNLP Recently textfree speech processing speech conversion meth ods developed Through selfsupervised learning systems derive latent representations speech data accompanied text transcriptions TextlessNLP34 35 Au dioLM 5 use text transcriptions phoneme symbols speech processing systems use discrete units constructed selfsupervised learning Soft discrete unit another approach textless speech processing 57 Our proposed method also uses nondiscrete vectors latent speech representations obtained selfsupervised learning explicitly use text transcriptions phoneme symbols Our research unique demonstrate whispered normal speech represented similar speech units selfsupervised learning In addition method also designed work seamlessly speech generation system later stages 3 THE WESPER VOICE CONVERSION MODEL WESPER consists STU encoder UTS decoder The STU converts whispered normal speech common speech units The UTS converts common speech units melspectrograms reconstructed speech vocoder WESPER char acterized fact common speech units utterance similar although STU pretrained unpaired whispered normal speech trained paired set whispered normal utterances The details method used train model described Figure 3 Overview STU pretraining Unpaired whis pered normal speech used pretraining The trans former layer trained estimating discrete units masked input The projection layer transformer layers generates 256dimensional vectors one every 20 ms used common speech units 31 SpeechtoUnit STU Encoder The STU encoder takes audio waveform input outputs units It based HuBERT 21 selfsupervised neural network speech pretrains large amount unlabeled speech BERT 12like manner learns recover relevant parts partially masked speech features thereby acquires speech language model For purpose want generate speech units identical possible whispered normal speech To achieve pretrained STU mixture whispered normal speech utterances used normal English speech many speakers Librispeech 960h dataset 42 Librispeech speech data mechanically converted whispered voice us ing LPCbased audio conversion tool 60 Additionally used wTIMIT speech dataset normal whispered speech 36 speech length 58 h Figure 3 shows pretraining process STU detail Only unpaired whispered normal speech used pretraining The transformer layer trained estimating discrete units masked input Similar HuBERT discrete target units first generated kmeans clustering input speech data first stage kmeans clustering outputs transformer intermediate layer In experiment used 100 discrete units The projection layer transformer layers generates sequence 256dimensional vectors one per 20 ms used common speech units In STU 12 transformer layers placed front CNN feature extractor Figure 1 After pretraining comparing output layer confirmed 1 difference Transformer Layer 11Transformer Layer 12ProjectionLEN 768LEN 256Label Embedding256 KLEN KLENCrossEntropy LossCosinesimilarityTo Unit Speech CNN EncoderWhisper Normal voicesunpairedTransformer Layer 1MaskingAcoustic unit discoveryeg KMeans MFCC HuBERTintermediate layerCommon Speech UnitK Discrete unitsK100Predict hidden units masked locations25620ms WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany In addition finetune STU transcriptattached wTIMIT Librispeech datasets adding CTC layer 17 48 therefore STU could used ASR recognize whis pered normal speeches 32 UTS Decoder The UTS decoder takes speech units generated STU input reconstructs target normal speech It based nonautoregressive texttospeech TTS system FastSpeech2 49 While original FastSpeech2 includes embedding layer takes text phoneme tokens transforms sequence vector embeddings eliminate layer UTS takes speech units direct input The original FastSpeech2 also includes duration estimator estimates duration phoneme token length regulator adjusts number internal vectors according estimated duration These parts also eliminated purpose STU generates speech units constant rate When learning original FastSpeech2 necessary pro vide duration phoneme corpus ground truth external tool Montreal Forced Aligner 38 Because limitation FastSpeech2 ‚Äô learning languagedependent Conversely UTS require duration estimation UTS languageindependent The output UTS melspectrogram FastSpeech2 A vocoder HiFiGAN 33 converts actual speech waveform In regular TTS system target speech corresponding text labels needed training By contrast proposed UTS requires target speech text labels The target speech passed STU obtain sequence speech units associated speech waveform used train UTS Figure 6 In experiment UTS trained using speech data single speaker LJSpeech 27 data speakers taken narration data 4 SYSTEM CONFIGURATION We designed WESPER model using PyTorch framework The STU based HuBERT UTS based modified implementation FastSpeech2 PyTorch implementation 8 We used Librispeech wTIMIT include normal whispered speech pretraining With dual NVIDIA R6000 pretraining took 48 h UTS training took 26 h target voice The total processing time required perform conversion approximately 120th actual speech duration single NVIDIA R6000 110th Apple M1 Max CPU The actual quality conversion demonstrated attached video We designed two types interfaces The first operated pushtotalk style user first speaks whispered normal voice pressing button Thereafter ton released speech waveform recorded time sent speechconversion neural networks result immediately played back The detected nonaudio period input speech automatically converted speech segments without additional user input Figure 4 Comparison whisperednormal voice differ ences The comparison made using normal speech speech whispering transformations Above An STU pretrained normal whispered speeches produced fewer differences pretrained normal speech As transformer layer deepens difference tween two decreases Middle There difference melspectrogram decreased speech units Bottom UMAP 39 visualizations differences tween whisper normal voices melspectrogram speechunit ‚Äô space speech unit values whispered normal voices de creased increasing layer depth 2 difference decreased STU pretrained whispered normal speech utterances compared pretraining normal speech Figure 4 Figure 4 bottom also shows UMAP 39 vi sualization whispered normal speech melspectrogram space common speech unit space The corresponding periods two speeches connected lines As shown figure differences reduced common speech unit space except long distances Although speech feature values whispered normal voices different assume addressed self supervised pretraining utterances similar linguistic standpoints represented similar units We speculate learning method extract common pronunciations normal whispered utterances similar selfsupervised pretraining methods extract common pronunciations utterances different speakers ASR system CNN Encoder012341112Output transformer intermediate layersSTU pretrained normal voicesSTUpretrained whispered normal voices121086420Mean vector differenceWhisper Normal voicesTransformer Layer 1Transformer Layer 2Transformer Layer 3Transformer Layer 4Transformer Layer 11Transformer Layer 12DifferencereductionNormalWhisperdiffMelspectrogramSpeech UnitsMelspectrogramSpeech unit CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany JRekimoto Figure 5 Comparison FastSpeech2 49 UTS decoders FastSpeech2 needs predict duration phoneme whereas UTS accepts common speech unit duration This eliminates duration predictor length regulator LR figure Phoneme embedding also eliminated common speech unit consist discrete tokens Figure 6 STU training 1 UTS training 2 The UTS learns decode speech units target speech using wave data target voice frozen STU No text la beled dataset required Notably WESPER trained labeled corpus pretrained normal whispered voices Figure 7 shows current WESPER speech input configurations We tested headset b directional microphone four ar rayed MEMS Microelectromechanical systems microphones 52 c mobile phone microphone popguard avoid whis pering pop noise soundproofing material reduce ambient noise 5 EVALUATION The WESPER mechanism allows whispertonormal conversion independent input speaker Here evaluate conversion Figure 7 WESPER speech input device headset b ar ray directional microphone c cell phone microphone popprotection soundproofing material quality three aspects considering whispertonormal conver sion voice reconstruction people speech hearing disorders 51 Quality WhispertoNormal Conversion To evaluate quality converted speech recruited 50 genderbalanced participants online using Prolific crowdsourc ing system 25 18 fluent English Each participant listened four sets normal speech whispered speech WESPERconverted whispered speech 12 voices total webbased user interface rated utterances 5point Mean Opinion Score MOS questionnaires examples voices included attached video We used LJSpeechtrained WESPER voice target voice transcription sentence voices avoid differ ences impressions based sentence content The results presented Figure 8 Figure 8 shows sults MOS evaluation The WESPERconverted voice showed score normal whispered voices It con firmed WESPER conversion improved MOS original whispered speech ùëù 001 pairwise ttest Cohen ‚Äô effect size 054 Figure 8 b shows responses question ‚Äú Is voice hoarse normal ‚Äù clear improvement voices converted WESPER ùëù 001 Figure 8 c shows responses question ‚Äú Is voice using consistent artic ulation standard intonation prosody ‚Äù Here WESPER whisper showed almost equal results Notably WESPER affect naturalness prosody Considering speech con verted WESPER generated unittospeech module vocoder result indicates WESPER preserve natural prosody original speech Multiple Stimuli Hidden Reference Anchor MUSHRA eval uation In addition MOS test evaluated speech quality us ing Multiple Stimuli Hidden Reference Anchor MUSHRA MUSHRA method evaluating perceived quality audio defined ITUR Recommendation BS15343 56 MUSHRA uses anchor audio audios evaluator expected Energy PredictorPitch PredictorTransformer LayersMelSpectrogramb UnittoSpeech UTS DecoderMelSpectrogram DecoderEnergy PredictorPitch PredictorTransformer LayersMelSpectrogramMelSpectrogram DecoderPhoneme EmbeddingDuration PredictorLRCommon Speech Unita FastSpeech2PhonemePositional EncodingPositional EncodingSpeech Unit Unit Speechtarget voiceno text labelslossPredicted melspectrogramTarget melspectrogramSpeech unitMelspectrogramconversionpitch energyextractionPredictedPitch energylossfrozentrainingSpeech Unit Acoustic unit discovery systemeg KMeans MFCCHuBERTintermediate layerNormal voicesWhisper voicestrainingSelfsupervised learning masked token prediction1unpairedSTU trainingUTS training2abcpop shieldsoundproof material WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany Figure 8 Quality whispertonormal conversion Mean Opinion Scores MOS normal whispered WESPER converted whispered voices b Hoarsenormal voice rating c natural prosody rating se standard error ùëù 001 ttest ns significant model train finetune test WER CER BLEU Google trained Google wTIMITN wTIMITW WESPERwTIMITW HuBERT base wTIMITN Librispeech Librispeech wTIMITW librispeech wTIMITNW wTIMITW 1155 4470 2668 2106 3306 1375 466 2838 1270 817 1545 547 076 034 052 054 038 070 wTIMITN wTIMIT 36 normal voice Google Google Cloud SpeechtoText 24 wTIMITW wTIMIT whisper voice WESPER‚Ä¢ WESPER converted ‚Ä¢ HuBERT HuBERT base model pretrained Librispeech 42 wTIMITNW Table 2 Whispered voice recognition accuracy Google Cloud SpeechtoText 24 reference ASR The recognition rate whispered voice normal ASR high however result converting whispered voice normal voice using WESPER recognized recognition rate improved Notably WESPER trained labeled data pretrained unlabeled unpaired normal whispered voices assign ratings 0 100 audio comparing anchor audio reference For rating participants play voice many times like Because presence anchor hidden reference audio MUSHRA considered reliable MOS We recruited 50 genderbalanced Englishspeaking participants age 18 via Internet using Prolific crowdsourcing system 25 We used javascriptbased MUSHRA testing tool 28 online evaluation Figure 9 system available https githubcomrkmtmushraJSprolific We used whisper normal voice samples previous MOS test The participants ‚Äô responses collected Internet The results presented Figure 10 It revalidates results MOS evaluation ùëù 001 pairwise ttest effect size 057 According evaluations draw following con clusions ‚Ä¢ WESPER convert whispered voices normal voices ‚Ä¢ Speechdata converted WESPER better MOS whispered source voice Figure 9 An example MUSHRA evaluation web inter faces nsabc CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany JRekimoto Figure 10 Speech quality evaluation whispered WESPER converted voices MUSHRA MUltiple Stimuli Hidden Reference Anchor ùëù 001 ttest ‚Ä¢ WESPER preserved natural prosody source whispered voice 52 Speech Recognition Accuracy There two possible ways use WESPER speech recognizer including speech recognition WESPER recognition speech converted WESPER speech recognizers In former method WESPER model based HuBERT pre trained whispered normal speech finetuned using whispered corpus text inferred whispered voice In latter case whispered speech converted WESPER con trol speechenabled device If whispered speech converted normal speech existing speechenabled devices used immediately without need modify whispered speech Using existing speech recognizer google cloud speechto text 24 reference measured speech recognition accu racy normal speech whispered speech whispered speech converted WESPER The wTIMIT corpus used measurements wTIMIT TIMITcompliant transcription containing normal whispered speech labels This corpus evaluated recogni tion accuracy whispered speech converted WESPER using recognition accuracy normal whispered speech baseline The results summarized Table 2 terms word error rate WER character error rate CER well bilingual eval uation understudy BLEU As shown table recognition accuracy high whispered speech directly rec ognized WER4470 accuracy improved WER2668 conversion WESPER When tested wTIMITW HuBERTbase pretrained Librispeech wTIMITNW shows better results google cloud speechtotext HuBERT WER3306 Google WER4470 It speculated effect pretraining mixture whis pered normal speech without finetuning whispered voice may contribute accuracy ASR Figure 11 Speech quality evaluation people speech disorders ranked 5point MOS ùëù 001 ùëù 005 S1S5 speakers VFP Vocal Fold Polyps SD Spasmodic Dys phonia Figure 12 Speech quality evaluation people vocal disabilities ranked MUSHRA ùëù 001 S1S5 speakers VFP Vocal Fold Polyps SD Spasmodic Dysphonia Notably WESPER trained labeled corpus It pre trained whispered normal speeches improves speech recognition accuracy Therefore speech recognition via WESPER conversion require preparation corpus whispered speech specific unspecified speakers 53 Evaluations Speech Reconstruction People Speech Disorders An important goal WESPER reconstruction atypical speech people speech disorders hearing impairments This makes speech understandable people familiar individual speech patterns Dysphonia involuntary hoarse breathy strained sound ing voice low volume pitch There various causes including spasms polyps vocal tract causes If vocal cords removed throat cancer causes voice becomes extremely difficult produce For hearing impair ment even vocal cords affected control vocal cords becomes difficult resulting dysphonia Electrolarynx used mechanically vibrate throat vocalization ns WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany fluent German addition English reference sentence used German The results presented Figures 11 12 13 14 Figure 11 shows results terms MOS For SD VFP WESPER converted voices higher MOS scores ùëù 001 effect size066 MUSHRA scores ùëù 001 effect size079 Figure 13 shows responses question ‚Äú Is voice hoarse normal ‚Äù clear improvement WESPERconverted voices compared original VFP SD voices ùëù 001 Figure 14 shows answers question ‚Äú Does voice use consistent articulation standard intonation prosody ‚Äù Here WESPER converted original voices showed nearly equal scores although WESPERconverted voices showed slightly better scores It assumed WESPER affect prosody original speech According evaluations make following con clusions ‚Ä¢ WESPERconverted voices people VFP SD speech disabilities showed better quality This suggests WESPER improve quality speech people con ditions terms intelligibility people unfamiliar individual speech patterns ‚Ä¢ Similarly WESPER improve naturalness original VFP SD speech ‚Ä¢ WESPER could also preserve natural prosody source speech Notably test performed German sentences Although WESPER pretraining performed English speech German speech prosody source speech preserved MOSs improved Therefore result may demonstrate languageindependence ability WESPER model The attached video shows example whisper normal conversion Japanese Because wav2vec 20 base model STU also shown languageindependent pretraining perfor mance 9 languageindependent ability WESPER could feature worth evaluating future 54 Speech Reconstruction Evaluation People Hearing Impairment Finally evaluated effect speech reconstruction WES PER people hearing impairments People hearing loss hear speech others therefore tend difficulty speaking way easily understood general speakers However vocal organs mal speech different characteristics people dysphasia We used ‚Äú corpus deaf speech acoustic speech production research ‚Äù 40 Utterances five speakers one hearing speaker four hearing impaired used We recruited 50 evaluation participants using Prolific 25 evenly balanced terms gender fluent English speak ers age 18 We asked participants perform 5point ranking utterance terms MOS hoarsenormal natural prosody The results presented Figures 15 16 17 18 These results indicate MOS MUSHRA rating scores Figure 13 HoarseNormal assessments utterances peo ple speech disorders ùëù 001 Figure 14 Speech prosody consistency utterances peo ple speech disorders ns significant people vocal cord damage sound produced artificial pitch conversion deterministic large devia tion normal vocalization The communication deficit caused dysphonia serious problem elimination voice conversion technology great social value To investigate quality improvement WESPER voice conversion evaluated speech utterances people two types speech disorders described Vocal Fold Polyps refer VFPs VFPs among common benign lesions larynx affecting quality voice production Spasmodic Dysphonia refer SD This also called laryngeal dystonia SD another common neurological disorder affects voice speech It lifelong condition causes spasms muscles produce voice We used Saarbruecken Voice Database SVD corpus 1 45 sample utterances people VFP SD The SVD commonly used corpus voices people speech disorders It contains recordings vowel utterances recording Ger man reference sentence ‚Äú Guten Morgen wie geht es Ihnen ‚Äù ‚Äú Good morning ‚Äù This sentence used evaluation Fifty participants recruited evaluation using Prolific 25 The recruited participants evenly balanced terms gender age 18 The participants nsnsnsnsnsns CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany JRekimoto Figure 15 Speech quality evaluation people hear ing impairment Speech quality ranked 5point MOS S1 ‚Äô S4 ‚Äô speakers ùëù 001 ùëù 005 Figure 18 Speech natural prosody consistency people hearing impairment ùëù 005 The results experiments summarized Table 3 6 DISCUSSIONS Combination WESPER UserDependent Finetuning The sults evaluation experiments indicated degree improvement speech people hearing impairments less speech people dysphasia There fore assume former significant prosodic varia tions Although primary purpose study achieve speakerindependent speech conversion pretraining alone plan conduct additional experiments investigate whether conversion performance improved applying smallscale finetuning speaker Even case believe speech pairs necessary text transcription required A pair whispered hoarse speech utterances normal speech converted com mon speech unit STU used instead text transcription case ASR finetuning Audio Input Device Suitable Whispered Speech As demonstrated study whispered hoarse voices converted normal voices In practice selection appropriate audio input device would important We currently testing proposed approach normal headsets directional array microphone designed smart speakers obtained good results For wearable devices nonaudible murmur NAM mi crophone detects skin vibrations could used inaudible utterances 20 Combining noise reduction techniques 7 61 another important future direction Philips Dyson also developing masks provide pow ered respiratory ventilation protect air pollution infectious diseases 13 26 A microphone placed inside masks pick whispered voice would give effect almost equivalent silent speech HumanAI Integration This research concerns machinelearning techniques converting whispered speech unspecified speakers normal speech In practice however established users noticed similar whispered utterances easily converted normal speech whereas others difficult They tried speak relying machine learning user Figure 16 Speech quality evaluation people hear ing impairment Speech quality ranked MUSHRA S1 ‚Äô S4 ‚Äô speakers ùëù 005 Figure 17 HoarseNormal assessments utterances peo ple hearing impairment ùëù 001 ùëù 005 higher converted WESPER ùëù 005 effect size 045 MOS ùëù 005 effect size 021 MUSHRA degree improvement less speakers voice disorders In addition prosody ratings significantly lower speech hearing impaired compared speech hearing speakers Therefore results indicate people hearing loss may difficulty controlling prosody speaking nsnsnsns WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany Speaker Type Normal Whisper WESPERWhisper MOS MUSHRA Q1 416 9001 415 290 5133 327 397 6364 379 VFP SD S1 WESPERS1 S2 WESPERS2 S3 WESPERS3 S4 WESPERS4 S5 WESPERS4 Speakers speech disorders 1913 3572 3666 5753 4219 6274 2751 4445 4986 6553 3520 5330 272 342 308 396 320 412 224 288 354 394 293 364 All WESPERAll 154 378 220 416 200 436 120 336 268 390 193 389 Q2 399 331 343 300 324 340 376 326 360 236 220 354 364 312 329 Normal S1 ‚Äô WESPERS1 ‚Äô S2 ‚Äô WESPERS2 ‚Äô S3 ‚Äô WESPERS3 ‚Äô S4 ‚Äô WESPERS4 ‚Äô All WESPERAll Speakers hearing impaired 9175 4305 4747 2076 2005 2830 3507 3586 4123 3207 3621 368 230 261 161 193 203 239 238 306 208 250 Mean Opinion Score MUltiple Stimuli Hidden Reference Anchor ‚Äò Is voice hoarse normal ‚Äô 375 251 297 184 255 222 293 259 335 212 236 376 229 238 174 206 219 239 226 262 228 294 ‚Äò Is voice use consistent articulation standard intonation prosody ‚Äô MOS MUSHRA Q1 Q2 WESPER‚Ä¢ WESPER converted ‚Ä¢ VFP SD S1S5 S1 ‚Äô S4 ‚Äô Vocal Fold Polyps speakers Spasmodic Dysphonia speakers speaker IDs Table 3 Summary Voice Conversion Quality Evaluation MUSHRA 100point score others5point score side This interaction suggests machine learning unilaterally extend human capabilities synergistic effects achieved learning human side well This seen actual example humanAI integration 47 vocalization 7 CONCLUSION In study proposed WESPER mechanism real time conversion whispered speech normal speech We con firmed common speech unit obtained selfsupervised learning even acoustic features whispered mal speech different Speech reproduction learned speech data arbitrary target speakers without using text labels There need train user need parallel data whispered normal speech From speech units WESPER reconstruct utterances target speaker requires unlabeled speech data target speakers We con firmed quality converted speech improved prosody speech preserved Additionally reported evaluation results reconstructing speech utterances people speech disorders hearing impairments'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save json_result\n",
    "import json\n",
    "\n",
    "# FILEPATH: /Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac@gmail.com/My Drive/2023/summarize_arxv/playground.ipynb\n",
    "# save json_result\n",
    "\n",
    "with open(\"json_result.json\", \"w\") as f:\n",
    "    json.dump(json_result, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'WESPER: Zeroshot Realtime Whisper-to-Normal Voice Conversion for Whisper-based Speech Interactions', 'author': ['Jun Rekimoto'], 'jornal/conference': 'CHI ‚Äô 23', 'year': '2023', 'abstract': 'Recognizing whispered speech and converting it to normal speech creates many possibilities for speech interaction. Because the sound pressure of whispered speech is significantly lower than normal speech, it can be used for semi-silent speech interaction in public places without being audible to others. Converting whispers to normal speech also improves speech quality for people with speech and hearing impairments. However, conventional speech conversion techniques do not provide sufficient conversion quality and require speaker-dependent datasets consisting of pairs of whispered and normal speech utterances. To address these problems, we propose WESPER, a zero-shot, real-time whisper-to-normal speech conversion mechanism based on self-supervised learning. WESPER consists of a speech-to-unit (STU) encoder that generates hidden speech units common to whispered and normal speech, and a unit-to-speech (UTS) decoder that reconstructs speech from the encoded speech units. Unlike existing methods, our conversion is user-independent and does not require a paired dataset of whispered and normal speech.', 'keywords': 'speech interaction, whispered voice, whispered voice conversion, silent speech, artificial intelligence, neural networks, self-supervised learning', 'problem': 'Recognizing whispered speech and converting it to normal speech is a challenge due to the significantly lower sound pressure of whispered speech. Conventional speech conversion techniques do not provide sufficient conversion quality and require speaker-dependent datasets consisting of pairs of whispered and normal speech utterances.', 'method': 'The proposed solution, WESPER, is a zero-shot, real-time whisper-to-normal speech conversion mechanism based on self-supervised learning. It consists of a speech-to-unit (STU) encoder that generates hidden speech units common to whispered and normal speech, and a unit-to-speech (UTS) decoder that reconstructs speech from the encoded speech units.', 'interaction': 'The system operates in real-time and can be applied to teleconferencing, for example, where a conference participant can speak in a whispered voice that is converted in real time for others to listen to as speech played back in a normal voice.', 'technical_contribution': 'The technical contribution of this paper is the development of WESPER, a zero-shot, real-time whisper-to-normal speech conversion mechanism based on self-supervised learning. Unlike existing methods, the conversion is user-independent and does not require a paired dataset of whispered and normal speech.', 'result': 'The quality of speech converted from whisper improved while preserving natural prosody. The effectiveness of the proposed approach was confirmed through its performance in speech reconstruction for people with speech and hearing disabilities.', 'github': 'https://wesperproj.github.io', 'doi': 'https://doi.org/10.1145/3544548.3580706', 'text': 'WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions Jun Rekimoto The University Tokyo 731 Hongo Bunkyoku Tokyo Japan Sony Computer Science Laboratories Kyoto 131 Hontorocho Shimogyoku Kyotoshi Kyoto Japan rekimotoacmorg 3 2 0 2 r M 3 D S c 1 v 9 3 6 1 0 3 0 3 2 v X r Figure 1 WESPER realtime whispertonormal speech conversion mechanism consisting speechtounit STU en coder generates common speech units whispered normal utterances using selfsupervised pretraining unittospeech UTS decoder recovers speech speech units It achieves userindependent voice conversion real time ABSTRACT Recognizing whispered speech converting normal speech creates many possibilities speech interaction Because sound pressure whispered speech significantly lower normal speech used semisilent speech interaction public places without audible others Converting whispers normal speech also improves speech quality people speech hearing impairments However conventional speech con version techniques provide sufficient conversion quality require speakerdependent datasets consisting pairs whis pered normal speech utterances To address problems propose WESPER zeroshot realtime whispertonormal speech conversion mechanism based selfsupervised learning WESPER consists speechtounit STU encoder generates hidden speech units common whispered normal speech unittospeech UTS decoder reconstructs speech encoded speech units Unlike existing methods conversion userindependent require paired dataset whis pered normal speech The UTS decoder reconstruct speech target speaker ‚Äô voice speech units requires unlabeled target speaker ‚Äô speech data We confirmed quality speech converted whisper improved preserving natural prosody Additionally confirmed effectiveness proposed approach perform speech construction people speech hearing disabilities CCS CONCEPTS ‚Ä¢ Humancentered computing ‚Üí Soundbased input output Interface design prototyping Mobile devices ‚Ä¢ Computing method ologies ‚Üí Neural networks Permission make digital hard copies part work personal classroom use granted without fee provided copies made distributed profit commercial advantage copies bear notice full citation first page Copyrights components work owned others authors must honored Abstracting credit permitted To copy otherwise republish post servers redistribute lists requires prior specific permission andor fee Request permissions permissionsacmorg CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany ¬© 2023 Copyright held ownerauthors Publication rights licensed ACM ACM ISBN 97814503942152304 1500 httpsdoiorg10114535445483580706 KEYWORDS speech interaction whispered voice whispered voice conversion silent speech artificial intelligence neural networks selfsupervised learning ACM Reference Format Jun Rekimoto 2023 WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions In Proceedings 2023 CHI Conference Human Factors Computing Systems CHI ‚Äô 23 Energy PredictorPitch PredictorWhisper Hoarse Normal VoicesTransformer LayersMelSpectrogramReconstructed Normal VoiceCNN EncoderTransformer LayerTransformer LayerTransformer LayerTransformer LayerCommon speech unitsbetween whisper normal voices Modified FastSpeech2SpeechtoUnit STU EncoderUnittoSpeech UTS DecoderModified HuBERT Vocoder HiFiGANMelSpectrogram Decoder256one per 20msSpeaker independentincluding persons speech hearing disabilitiesCTC Speech RecogRecognized text CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany JRekimoto April 23‚Äì28 2023 Hamburg Germany ACM New York NY USA 13 pages httpsdoiorg10114535445483580706 1 INTRODUCTION Although voice interaction systems widely deployed typically easy use presence people Using voice commands public places may socially unaccept able risk confidential information leaked In addition speaking conference call uncomfortable people vicinity compromise confidentiality conversation To overcome problems various silent speech input tech niques developed 3 31 32 50 51 54 however methods require special sensors achieved high accu racy speech recognition remaining instead level recog nizing predefined commands Conversion unconditioned silent speech normal vocal utterances also achieved Additionally silent speech could employed capture utter ances people speech hearing impairments However owing abovementioned limitations existing methods meet requirements practical accessibility aids In contrast silent speech focus whispered speech Whis pers sufficiently low sound pressure ensure confidentiality whispering almost equivalent silent speech Whispering captured ordinary microphone require special sensor configuration People speech disorders still speak whisper hoarse voice although vocal organs may injured extracted thus voices might recognized To address issues propose WESPER realtime zeroshot whispertonormal voice conversion method based selfsupervised learning It designed perform speaker inde pendent conversion whispered speech normal speech There need peruser training paired datasets whispered normal utterances required The proposed architec ture consists speechtounit STU encoder pretrained normal whispered speeches unittospeech UTS decoder generates target voice speech units Figure 1 By pretraining unpaired whisper normal voices STU trained reduce difference normal whispered utterances output common speech unit The UTS decoder trained speech data specific speaker without accompanying text labels If person ‚Äô voice used conversion performed restore whispered voice person ‚Äô normal voice even another person ‚Äô voice Because encoder decoder operate nonautoregressive manner entire system operates realtime Therefore applied teleconferencing example conference participant speak whispered voice converted real time others listen speech played back normal voice Figure 2 shows melspectrogram examples whispertonormal voice conversion using proposed method More conversion results demonstrated accompanying video The contributions study summarized follows Figure 2 WESPER conversion result Left whispered speech Middle whispered speech converted WESPER Right Normal speech speaker tran scription ‚Ä¢ We propose realtime speakerindependent vocabularyfree whispertonormal speech conversion method trained unpaired whispers normal speech ‚Ä¢ The target voice learned voice samples specific speaker without text transcription ‚Ä¢ We experimentally confirmed improvement performance normal speakers dysarthric hearingimpaired speakers 2 RELATED WORK 21 Research Silent Whispered Speech Various studies silent speech developed methods rec ognize user ‚Äô silent utterances silent commands using various sensor configurations including lip reading EMG Electromyogra phy ultrasound 3 31 32 50 51 54 However systems require special dataset perform training increases need special sensors prevents technologies ing widely used Hence systems offer limited vocabulary less 100 commands available typically around 30 Al though silent speech recognition sometimes identified possible approach help people dysphonia methods interpret unrestricted free speech owing vocabulary limitation Whispered speech similar characteristics silent speech preserving social acceptability public spaces However widely adopted given ordinary mi crophones used SilentVoice 15 speech interaction technique uses gressive speech utterance made inhaling The amplitude ingressive speech low considered variation silent speech It requires microphonelike device placed close mouth training required users speak correctly ingressive speech A custom corpus ingressive speech text transcriptions also required Research whisper voice recognition 6 11 14 18 previ ously conducted The Alexa smart speaker supports whisper mode 23 In mode user whispers Alexa Alexa responds whisper 10 DualVoice 48 also proposed method per form endtoend recognition whispered voices based self supervised speech recognition system based wav2vec20 2 HuBERT 21 They also proposed interaction technique WhisperWESPER convertedWhisperNormalsecsecsecHz WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany model training data peruser dataset conversion normal voice whisper speech recognition silent speech ex3 31 32 50 51 54 special paireddataset dedicated sensors required Parotoron 4 paired whispernormal voice required DualVoice 48 SilentVoice 15 CycleGANVC 30 MSpeCNet 37 AGANW2SC 16 WESPER labeled unpaired whisper normal voice labeled silentvoice custom unpaired unlabeled whisper normal voice paired whispernormal voice paired whispernormal voice unpaired unlabeled whisper normal voice required required required required required required YES NO NO YES YES YES YES Table 1 Research silent whispered speech YES commands characters YES voice conversion YES YES NO NO NO YES voice conversion distinguish whispered normal utterances WESPER combined DualVoice selectively convert user ‚Äô whispered voice 22 Speech Conversion Normal speech conversion technologies developed convert one voiced utterance another 19 29 30 43 46 however conversion quality methods remains unsatisfactory applied whispertonormal voice conversion Recently several machine learningbased whispertonormal voice conversion techniques investigated 41 To com pare techniques important consider quality conversion required characteristics dataset used training A paired whispernormal voice dataset ie dataset containing whispered normal versions utter ance labeled whispered voice dataset ie dataset containing whispered utterances accompanied text labels require significant amount effort prepare Conversely unpaired unlabeled whisper voice dataset used ie dataset containing whisper utterances without corresponding normal versions labels effort required prepare dataset low Attentionguided generative adversarial network whisper normal speech conversion AGANW2SC GANbased whisper normal speech conversion 16 It converts whispered voice represented melspectrogram corresponding normal speech ‚Äô melspectrogram It based GAN attention map used conversion It requires paired whispernormal speech dataset MSpeCNet 37 autoencoderbased multi domain voice conversion supports whispertonormal conver sion It also requires paired whispernormal voice dataset Moreover whispered speech converted text using speech recognition generate normal speech using texttospeech methods 22 However approach requires labeled dataset whispered speech recognition prosodic information con tained whispered utterances lost intermediate text representation convey information Parrotron 4 speech conversion system designed im prove speech speakers dysplasia It based encoderdecoder model conforms texttospeech system Tacotron 58 however differs Tacotron input output data formatted melspectrograms It also quires paired source target speeches making construction required dataset relatively difficult Phonetic posteriorgrams PPGs 55 intermediate informa tion obtained automatic speech recognition PPGs used manytoone speaker conversion represent articulation spoken content speaker independently To knowledge effects PPGs whispered voice investigated Our method also uses intermediated speech units require textbased corpus case automatic speech recognition ASR PPGs In contrast proposed system requires samples un paired target source speech require accompa nying text transcriptions thus making datasets easy prepare independent target language The characteristics related technologies summarized Table 1 We also demonstrate whisper normal conversion exam ples including NMSEDiscoGAN 53 MSpeCNet 37 CycleGAN VC 30 AGANW2SC 16 WESPER httpswesperproj githubio 23 Self Supervised Representation Learning Speech Recently combining pretraining selfsupervised representa tion learning unlabeled speech data finetuning labeled speech data attracted attention These systems primarily CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany JRekimoto intended speech recognition applications also applied perform speaker language emotion recogni tion 44 59 In particular pretraining method hiddenunit BERT hu BERT 21 similar masked language model used bidirectional encoder representations transformers BERT 12 natural language processing It designed mask part input estimate corresponding expression features rest input With pretraining model learn acoustic properties input data characteristics speech For use ASR pretraining finetuning performed small amount audio data text transcriptions A projection layer connectionist temporal classification CTC layer 17 added generate text transcriptions audio waveforms As reported 2 59 selfsupervised ASR achieved speech recognition accuracy comparable conventional stateoftheart ASRs finetuning small amount labeled speech data Therefore architecture could suitable recogniz ing whispered voice recognition limited whispered speech corpora WESPER unique uses pretraining reduce dif ference latent vectors encoding whispered normal utterances 24 TextlessNLP Recently textfree speech processing speech conversion meth ods developed Through selfsupervised learning systems derive latent representations speech data accompanied text transcriptions TextlessNLP34 35 Au dioLM 5 use text transcriptions phoneme symbols speech processing systems use discrete units constructed selfsupervised learning Soft discrete unit another approach textless speech processing 57 Our proposed method also uses nondiscrete vectors latent speech representations obtained selfsupervised learning explicitly use text transcriptions phoneme symbols Our research unique demonstrate whispered normal speech represented similar speech units selfsupervised learning In addition method also designed work seamlessly speech generation system later stages 3 THE WESPER VOICE CONVERSION MODEL WESPER consists STU encoder UTS decoder The STU converts whispered normal speech common speech units The UTS converts common speech units melspectrograms reconstructed speech vocoder WESPER char acterized fact common speech units utterance similar although STU pretrained unpaired whispered normal speech trained paired set whispered normal utterances The details method used train model described Figure 3 Overview STU pretraining Unpaired whis pered normal speech used pretraining The trans former layer trained estimating discrete units masked input The projection layer transformer layers generates 256dimensional vectors one every 20 ms used common speech units 31 SpeechtoUnit STU Encoder The STU encoder takes audio waveform input outputs units It based HuBERT 21 selfsupervised neural network speech pretrains large amount unlabeled speech BERT 12like manner learns recover relevant parts partially masked speech features thereby acquires speech language model For purpose want generate speech units identical possible whispered normal speech To achieve pretrained STU mixture whispered normal speech utterances used normal English speech many speakers Librispeech 960h dataset 42 Librispeech speech data mechanically converted whispered voice us ing LPCbased audio conversion tool 60 Additionally used wTIMIT speech dataset normal whispered speech 36 speech length 58 h Figure 3 shows pretraining process STU detail Only unpaired whispered normal speech used pretraining The transformer layer trained estimating discrete units masked input Similar HuBERT discrete target units first generated kmeans clustering input speech data first stage kmeans clustering outputs transformer intermediate layer In experiment used 100 discrete units The projection layer transformer layers generates sequence 256dimensional vectors one per 20 ms used common speech units In STU 12 transformer layers placed front CNN feature extractor Figure 1 After pretraining comparing output layer confirmed 1 difference Transformer Layer 11Transformer Layer 12ProjectionLEN 768LEN 256Label Embedding256 KLEN KLENCrossEntropy LossCosinesimilarityTo Unit Speech CNN EncoderWhisper Normal voicesunpairedTransformer Layer 1MaskingAcoustic unit discoveryeg KMeans MFCC HuBERTintermediate layerCommon Speech UnitK Discrete unitsK100Predict hidden units masked locations25620ms WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany In addition finetune STU transcriptattached wTIMIT Librispeech datasets adding CTC layer 17 48 therefore STU could used ASR recognize whis pered normal speeches 32 UTS Decoder The UTS decoder takes speech units generated STU input reconstructs target normal speech It based nonautoregressive texttospeech TTS system FastSpeech2 49 While original FastSpeech2 includes embedding layer takes text phoneme tokens transforms sequence vector embeddings eliminate layer UTS takes speech units direct input The original FastSpeech2 also includes duration estimator estimates duration phoneme token length regulator adjusts number internal vectors according estimated duration These parts also eliminated purpose STU generates speech units constant rate When learning original FastSpeech2 necessary pro vide duration phoneme corpus ground truth external tool Montreal Forced Aligner 38 Because limitation FastSpeech2 ‚Äô learning languagedependent Conversely UTS require duration estimation UTS languageindependent The output UTS melspectrogram FastSpeech2 A vocoder HiFiGAN 33 converts actual speech waveform In regular TTS system target speech corresponding text labels needed training By contrast proposed UTS requires target speech text labels The target speech passed STU obtain sequence speech units associated speech waveform used train UTS Figure 6 In experiment UTS trained using speech data single speaker LJSpeech 27 data speakers taken narration data 4 SYSTEM CONFIGURATION We designed WESPER model using PyTorch framework The STU based HuBERT UTS based modified implementation FastSpeech2 PyTorch implementation 8 We used Librispeech wTIMIT include normal whispered speech pretraining With dual NVIDIA R6000 pretraining took 48 h UTS training took 26 h target voice The total processing time required perform conversion approximately 120th actual speech duration single NVIDIA R6000 110th Apple M1 Max CPU The actual quality conversion demonstrated attached video We designed two types interfaces The first operated pushtotalk style user first speaks whispered normal voice pressing button Thereafter ton released speech waveform recorded time sent speechconversion neural networks result immediately played back The detected nonaudio period input speech automatically converted speech segments without additional user input Figure 4 Comparison whisperednormal voice differ ences The comparison made using normal speech speech whispering transformations Above An STU pretrained normal whispered speeches produced fewer differences pretrained normal speech As transformer layer deepens difference tween two decreases Middle There difference melspectrogram decreased speech units Bottom UMAP 39 visualizations differences tween whisper normal voices melspectrogram speechunit ‚Äô space speech unit values whispered normal voices de creased increasing layer depth 2 difference decreased STU pretrained whispered normal speech utterances compared pretraining normal speech Figure 4 Figure 4 bottom also shows UMAP 39 vi sualization whispered normal speech melspectrogram space common speech unit space The corresponding periods two speeches connected lines As shown figure differences reduced common speech unit space except long distances Although speech feature values whispered normal voices different assume addressed self supervised pretraining utterances similar linguistic standpoints represented similar units We speculate learning method extract common pronunciations normal whispered utterances similar selfsupervised pretraining methods extract common pronunciations utterances different speakers ASR system CNN Encoder012341112Output transformer intermediate layersSTU pretrained normal voicesSTUpretrained whispered normal voices121086420Mean vector differenceWhisper Normal voicesTransformer Layer 1Transformer Layer 2Transformer Layer 3Transformer Layer 4Transformer Layer 11Transformer Layer 12DifferencereductionNormalWhisperdiffMelspectrogramSpeech UnitsMelspectrogramSpeech unit CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany JRekimoto Figure 5 Comparison FastSpeech2 49 UTS decoders FastSpeech2 needs predict duration phoneme whereas UTS accepts common speech unit duration This eliminates duration predictor length regulator LR figure Phoneme embedding also eliminated common speech unit consist discrete tokens Figure 6 STU training 1 UTS training 2 The UTS learns decode speech units target speech using wave data target voice frozen STU No text la beled dataset required Notably WESPER trained labeled corpus pretrained normal whispered voices Figure 7 shows current WESPER speech input configurations We tested headset b directional microphone four ar rayed MEMS Microelectromechanical systems microphones 52 c mobile phone microphone popguard avoid whis pering pop noise soundproofing material reduce ambient noise 5 EVALUATION The WESPER mechanism allows whispertonormal conversion independent input speaker Here evaluate conversion Figure 7 WESPER speech input device headset b ar ray directional microphone c cell phone microphone popprotection soundproofing material quality three aspects considering whispertonormal conver sion voice reconstruction people speech hearing disorders 51 Quality WhispertoNormal Conversion To evaluate quality converted speech recruited 50 genderbalanced participants online using Prolific crowdsourc ing system 25 18 fluent English Each participant listened four sets normal speech whispered speech WESPERconverted whispered speech 12 voices total webbased user interface rated utterances 5point Mean Opinion Score MOS questionnaires examples voices included attached video We used LJSpeechtrained WESPER voice target voice transcription sentence voices avoid differ ences impressions based sentence content The results presented Figure 8 Figure 8 shows sults MOS evaluation The WESPERconverted voice showed score normal whispered voices It con firmed WESPER conversion improved MOS original whispered speech ùëù 001 pairwise ttest Cohen ‚Äô effect size 054 Figure 8 b shows responses question ‚Äú Is voice hoarse normal ‚Äù clear improvement voices converted WESPER ùëù 001 Figure 8 c shows responses question ‚Äú Is voice using consistent artic ulation standard intonation prosody ‚Äù Here WESPER whisper showed almost equal results Notably WESPER affect naturalness prosody Considering speech con verted WESPER generated unittospeech module vocoder result indicates WESPER preserve natural prosody original speech Multiple Stimuli Hidden Reference Anchor MUSHRA eval uation In addition MOS test evaluated speech quality us ing Multiple Stimuli Hidden Reference Anchor MUSHRA MUSHRA method evaluating perceived quality audio defined ITUR Recommendation BS15343 56 MUSHRA uses anchor audio audios evaluator expected Energy PredictorPitch PredictorTransformer LayersMelSpectrogramb UnittoSpeech UTS DecoderMelSpectrogram DecoderEnergy PredictorPitch PredictorTransformer LayersMelSpectrogramMelSpectrogram DecoderPhoneme EmbeddingDuration PredictorLRCommon Speech Unita FastSpeech2PhonemePositional EncodingPositional EncodingSpeech Unit Unit Speechtarget voiceno text labelslossPredicted melspectrogramTarget melspectrogramSpeech unitMelspectrogramconversionpitch energyextractionPredictedPitch energylossfrozentrainingSpeech Unit Acoustic unit discovery systemeg KMeans MFCCHuBERTintermediate layerNormal voicesWhisper voicestrainingSelfsupervised learning masked token prediction1unpairedSTU trainingUTS training2abcpop shieldsoundproof material WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany Figure 8 Quality whispertonormal conversion Mean Opinion Scores MOS normal whispered WESPER converted whispered voices b Hoarsenormal voice rating c natural prosody rating se standard error ùëù 001 ttest ns significant model train finetune test WER CER BLEU Google trained Google wTIMITN wTIMITW WESPERwTIMITW HuBERT base wTIMITN Librispeech Librispeech wTIMITW librispeech wTIMITNW wTIMITW 1155 4470 2668 2106 3306 1375 466 2838 1270 817 1545 547 076 034 052 054 038 070 wTIMITN wTIMIT 36 normal voice Google Google Cloud SpeechtoText 24 wTIMITW wTIMIT whisper voice WESPER‚Ä¢ WESPER converted ‚Ä¢ HuBERT HuBERT base model pretrained Librispeech 42 wTIMITNW Table 2 Whispered voice recognition accuracy Google Cloud SpeechtoText 24 reference ASR The recognition rate whispered voice normal ASR high however result converting whispered voice normal voice using WESPER recognized recognition rate improved Notably WESPER trained labeled data pretrained unlabeled unpaired normal whispered voices assign ratings 0 100 audio comparing anchor audio reference For rating participants play voice many times like Because presence anchor hidden reference audio MUSHRA considered reliable MOS We recruited 50 genderbalanced Englishspeaking participants age 18 via Internet using Prolific crowdsourcing system 25 We used javascriptbased MUSHRA testing tool 28 online evaluation Figure 9 system available https githubcomrkmtmushraJSprolific We used whisper normal voice samples previous MOS test The participants ‚Äô responses collected Internet The results presented Figure 10 It revalidates results MOS evaluation ùëù 001 pairwise ttest effect size 057 According evaluations draw following con clusions ‚Ä¢ WESPER convert whispered voices normal voices ‚Ä¢ Speechdata converted WESPER better MOS whispered source voice Figure 9 An example MUSHRA evaluation web inter faces nsabc CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany JRekimoto Figure 10 Speech quality evaluation whispered WESPER converted voices MUSHRA MUltiple Stimuli Hidden Reference Anchor ùëù 001 ttest ‚Ä¢ WESPER preserved natural prosody source whispered voice 52 Speech Recognition Accuracy There two possible ways use WESPER speech recognizer including speech recognition WESPER recognition speech converted WESPER speech recognizers In former method WESPER model based HuBERT pre trained whispered normal speech finetuned using whispered corpus text inferred whispered voice In latter case whispered speech converted WESPER con trol speechenabled device If whispered speech converted normal speech existing speechenabled devices used immediately without need modify whispered speech Using existing speech recognizer google cloud speechto text 24 reference measured speech recognition accu racy normal speech whispered speech whispered speech converted WESPER The wTIMIT corpus used measurements wTIMIT TIMITcompliant transcription containing normal whispered speech labels This corpus evaluated recogni tion accuracy whispered speech converted WESPER using recognition accuracy normal whispered speech baseline The results summarized Table 2 terms word error rate WER character error rate CER well bilingual eval uation understudy BLEU As shown table recognition accuracy high whispered speech directly rec ognized WER4470 accuracy improved WER2668 conversion WESPER When tested wTIMITW HuBERTbase pretrained Librispeech wTIMITNW shows better results google cloud speechtotext HuBERT WER3306 Google WER4470 It speculated effect pretraining mixture whis pered normal speech without finetuning whispered voice may contribute accuracy ASR Figure 11 Speech quality evaluation people speech disorders ranked 5point MOS ùëù 001 ùëù 005 S1S5 speakers VFP Vocal Fold Polyps SD Spasmodic Dys phonia Figure 12 Speech quality evaluation people vocal disabilities ranked MUSHRA ùëù 001 S1S5 speakers VFP Vocal Fold Polyps SD Spasmodic Dysphonia Notably WESPER trained labeled corpus It pre trained whispered normal speeches improves speech recognition accuracy Therefore speech recognition via WESPER conversion require preparation corpus whispered speech specific unspecified speakers 53 Evaluations Speech Reconstruction People Speech Disorders An important goal WESPER reconstruction atypical speech people speech disorders hearing impairments This makes speech understandable people familiar individual speech patterns Dysphonia involuntary hoarse breathy strained sound ing voice low volume pitch There various causes including spasms polyps vocal tract causes If vocal cords removed throat cancer causes voice becomes extremely difficult produce For hearing impair ment even vocal cords affected control vocal cords becomes difficult resulting dysphonia Electrolarynx used mechanically vibrate throat vocalization ns WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany fluent German addition English reference sentence used German The results presented Figures 11 12 13 14 Figure 11 shows results terms MOS For SD VFP WESPER converted voices higher MOS scores ùëù 001 effect size066 MUSHRA scores ùëù 001 effect size079 Figure 13 shows responses question ‚Äú Is voice hoarse normal ‚Äù clear improvement WESPERconverted voices compared original VFP SD voices ùëù 001 Figure 14 shows answers question ‚Äú Does voice use consistent articulation standard intonation prosody ‚Äù Here WESPER converted original voices showed nearly equal scores although WESPERconverted voices showed slightly better scores It assumed WESPER affect prosody original speech According evaluations make following con clusions ‚Ä¢ WESPERconverted voices people VFP SD speech disabilities showed better quality This suggests WESPER improve quality speech people con ditions terms intelligibility people unfamiliar individual speech patterns ‚Ä¢ Similarly WESPER improve naturalness original VFP SD speech ‚Ä¢ WESPER could also preserve natural prosody source speech Notably test performed German sentences Although WESPER pretraining performed English speech German speech prosody source speech preserved MOSs improved Therefore result may demonstrate languageindependence ability WESPER model The attached video shows example whisper normal conversion Japanese Because wav2vec 20 base model STU also shown languageindependent pretraining perfor mance 9 languageindependent ability WESPER could feature worth evaluating future 54 Speech Reconstruction Evaluation People Hearing Impairment Finally evaluated effect speech reconstruction WES PER people hearing impairments People hearing loss hear speech others therefore tend difficulty speaking way easily understood general speakers However vocal organs mal speech different characteristics people dysphasia We used ‚Äú corpus deaf speech acoustic speech production research ‚Äù 40 Utterances five speakers one hearing speaker four hearing impaired used We recruited 50 evaluation participants using Prolific 25 evenly balanced terms gender fluent English speak ers age 18 We asked participants perform 5point ranking utterance terms MOS hoarsenormal natural prosody The results presented Figures 15 16 17 18 These results indicate MOS MUSHRA rating scores Figure 13 HoarseNormal assessments utterances peo ple speech disorders ùëù 001 Figure 14 Speech prosody consistency utterances peo ple speech disorders ns significant people vocal cord damage sound produced artificial pitch conversion deterministic large devia tion normal vocalization The communication deficit caused dysphonia serious problem elimination voice conversion technology great social value To investigate quality improvement WESPER voice conversion evaluated speech utterances people two types speech disorders described Vocal Fold Polyps refer VFPs VFPs among common benign lesions larynx affecting quality voice production Spasmodic Dysphonia refer SD This also called laryngeal dystonia SD another common neurological disorder affects voice speech It lifelong condition causes spasms muscles produce voice We used Saarbruecken Voice Database SVD corpus 1 45 sample utterances people VFP SD The SVD commonly used corpus voices people speech disorders It contains recordings vowel utterances recording Ger man reference sentence ‚Äú Guten Morgen wie geht es Ihnen ‚Äù ‚Äú Good morning ‚Äù This sentence used evaluation Fifty participants recruited evaluation using Prolific 25 The recruited participants evenly balanced terms gender age 18 The participants nsnsnsnsnsns CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany JRekimoto Figure 15 Speech quality evaluation people hear ing impairment Speech quality ranked 5point MOS S1 ‚Äô S4 ‚Äô speakers ùëù 001 ùëù 005 Figure 18 Speech natural prosody consistency people hearing impairment ùëù 005 The results experiments summarized Table 3 6 DISCUSSIONS Combination WESPER UserDependent Finetuning The sults evaluation experiments indicated degree improvement speech people hearing impairments less speech people dysphasia There fore assume former significant prosodic varia tions Although primary purpose study achieve speakerindependent speech conversion pretraining alone plan conduct additional experiments investigate whether conversion performance improved applying smallscale finetuning speaker Even case believe speech pairs necessary text transcription required A pair whispered hoarse speech utterances normal speech converted com mon speech unit STU used instead text transcription case ASR finetuning Audio Input Device Suitable Whispered Speech As demonstrated study whispered hoarse voices converted normal voices In practice selection appropriate audio input device would important We currently testing proposed approach normal headsets directional array microphone designed smart speakers obtained good results For wearable devices nonaudible murmur NAM mi crophone detects skin vibrations could used inaudible utterances 20 Combining noise reduction techniques 7 61 another important future direction Philips Dyson also developing masks provide pow ered respiratory ventilation protect air pollution infectious diseases 13 26 A microphone placed inside masks pick whispered voice would give effect almost equivalent silent speech HumanAI Integration This research concerns machinelearning techniques converting whispered speech unspecified speakers normal speech In practice however established users noticed similar whispered utterances easily converted normal speech whereas others difficult They tried speak relying machine learning user Figure 16 Speech quality evaluation people hear ing impairment Speech quality ranked MUSHRA S1 ‚Äô S4 ‚Äô speakers ùëù 005 Figure 17 HoarseNormal assessments utterances peo ple hearing impairment ùëù 001 ùëù 005 higher converted WESPER ùëù 005 effect size 045 MOS ùëù 005 effect size 021 MUSHRA degree improvement less speakers voice disorders In addition prosody ratings significantly lower speech hearing impaired compared speech hearing speakers Therefore results indicate people hearing loss may difficulty controlling prosody speaking nsnsnsns WESPER Zeroshot Realtime Whisper Normal Voice Conversion Whisperbased Speech Interactions CHI ‚Äô 23 April 23‚Äì28 2023 Hamburg Germany Speaker Type Normal Whisper WESPERWhisper MOS MUSHRA Q1 416 9001 415 290 5133 327 397 6364 379 VFP SD S1 WESPERS1 S2 WESPERS2 S3 WESPERS3 S4 WESPERS4 S5 WESPERS4 Speakers speech disorders 1913 3572 3666 5753 4219 6274 2751 4445 4986 6553 3520 5330 272 342 308 396 320 412 224 288 354 394 293 364 All WESPERAll 154 378 220 416 200 436 120 336 268 390 193 389 Q2 399 331 343 300 324 340 376 326 360 236 220 354 364 312 329 Normal S1 ‚Äô WESPERS1 ‚Äô S2 ‚Äô WESPERS2 ‚Äô S3 ‚Äô WESPERS3 ‚Äô S4 ‚Äô WESPERS4 ‚Äô All WESPERAll Speakers hearing impaired 9175 4305 4747 2076 2005 2830 3507 3586 4123 3207 3621 368 230 261 161 193 203 239 238 306 208 250 Mean Opinion Score MUltiple Stimuli Hidden Reference Anchor ‚Äò Is voice hoarse normal ‚Äô 375 251 297 184 255 222 293 259 335 212 236 376 229 238 174 206 219 239 226 262 228 294 ‚Äò Is voice use consistent articulation standard intonation prosody ‚Äô MOS MUSHRA Q1 Q2 WESPER‚Ä¢ WESPER converted ‚Ä¢ VFP SD S1S5 S1 ‚Äô S4 ‚Äô Vocal Fold Polyps speakers Spasmodic Dysphonia speakers speaker IDs Table 3 Summary Voice Conversion Quality Evaluation MUSHRA 100point score others5point score side This interaction suggests machine learning unilaterally extend human capabilities synergistic effects achieved learning human side well This seen actual example humanAI integration 47 vocalization 7 CONCLUSION In study proposed WESPER mechanism real time conversion whispered speech normal speech We con firmed common speech unit obtained selfsupervised learning even acoustic features whispered mal speech different Speech reproduction learned speech data arbitrary target speakers without using text labels There need train user need parallel data whispered normal speech From speech units WESPER reconstruct utterances target speaker requires unlabeled speech data target speakers We con firmed quality converted speech improved prosody speech preserved Additionally reported evaluation results reconstructing speech utterances people speech disorders hearing impairments'}\n"
     ]
    }
   ],
   "source": [
    "# load the json file and translate it to japanese\n",
    "\n",
    "import json\n",
    "\n",
    "with open('json_result.json', 'r') as f:\n",
    "    json_result = json.load(f)\n",
    "\n",
    "print(json_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'update'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac@gmail.com/My Drive/2023/summarize_arxv/playground.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac%40gmail.com/My%20Drive/2023/summarize_arxv/playground.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# add text to json_summary\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac%40gmail.com/My%20Drive/2023/summarize_arxv/playground.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m json_summary\u001b[39m.\u001b[39;49mupdate({\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m: text})\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac%40gmail.com/My%20Drive/2023/summarize_arxv/playground.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m#show\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/naoki/Library/CloudStorage/GoogleDrive-naoki.kimura.ac%40gmail.com/My%20Drive/2023/summarize_arxv/playground.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m json_summary\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'update'"
     ]
    }
   ],
   "source": [
    "# add text to json_summary\n",
    "\n",
    "json_summary.update({'text': text})\n",
    "\n",
    "#show\n",
    "json_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exam_designer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
